{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7299bd4c-6d80-44d5-8fac-004b8d2836ea",
   "metadata": {},
   "source": [
    "# Athena.ipynb\n",
    "\n",
    "## Problem 1: Get top 10 subreddits by count of comments\n",
    "\n",
    "The response should contain two columns: `subreddit` and `comment_count`.\n",
    "Save the results in a file called `prob1_results.csv`. This needs to be commited to the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98d2998-f462-4560-8f40-68a1c5dd14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries  \n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8dece3-4aca-4d77-b31d-0e0a47b06e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36fe76f6-ae13-48ae-bd1f-03d68dbad98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_NAME = \"schema_name\"\n",
    "\n",
    "# fill in your net id below.\n",
    "netid = \"ek976\"\n",
    "S3_STAGING_PREFIX = \"data/results\"\n",
    "S3_BUCKET_NAME = f\"athena-{netid}\"\n",
    "S3_STAGING_DIR = f\"s3://{S3_BUCKET_NAME}/{S3_STAGING_PREFIX}/\"\n",
    "S3_OUTPUT_DIRECTORY = \"data\"\n",
    "AWS_REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206edc15-7d1f-49a5-bfe3-f7c94501bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load_query_results(\n",
    "    client: boto3.client, query_response: Dict\n",
    ") -> pd.DataFrame:\n",
    "    logger.info(\"download_and_load_query_results, enter\")\n",
    "    while True:\n",
    "        try:\n",
    "            # This function only loads the first 1000 rows\n",
    "            client.get_query_results(\n",
    "                QueryExecutionId=query_response[\"QueryExecutionId\"]\n",
    "            )\n",
    "            break\n",
    "        except Exception as err:\n",
    "            if \"not yet finished\" in str(err):\n",
    "                time.sleep(0.001)\n",
    "            else:\n",
    "                raise err\n",
    "    logger.info(f\"Time to complete query: {time.time() - start_time}s\")\n",
    "    temp_file_location: str = \"athena_query_results.csv\"\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=AWS_REGION,\n",
    "    )\n",
    "    s3_path = os.path.join(S3_STAGING_PREFIX, f\"{query_response['QueryExecutionId']}.csv\")\n",
    "    logger.info(f\"downloading file from S3_BUCKET_NAME={S3_BUCKET_NAME}, s3_path={s3_path}, to local file {temp_file_location}\")\n",
    "    s3_client.download_file(\n",
    "        S3_BUCKET_NAME,\n",
    "        s3_path,\n",
    "        temp_file_location,\n",
    "    )\n",
    "    df = pd.read_csv(temp_file_location)\n",
    "    logger.info(f\"results dataframe shape is {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c38d168-917a-4e29-a9ca-840ca87997ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_client = boto3.client(\n",
    "    \"athena\",\n",
    "    region_name=AWS_REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df43b384-31ec-47a3-8522-f3a72595d01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-11 03:00:53,473] p537 {<timed exec>:11} INFO - {'QueryExecutionId': 'd691202c-c9be-43a7-bc60-e6108b85a745', 'ResponseMetadata': {'RequestId': 'aa188a28-d128-43d2-b651-604ef6233729', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Nov 2024 03:00:53 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': 'aa188a28-d128-43d2-b651-604ef6233729'}, 'RetryAttempts': 0}}\n",
      "[2024-11-11 03:00:53,475] p537 {1241576256.py:4} INFO - download_and_load_query_results, enter\n",
      "[2024-11-11 03:00:54,479] p537 {1241576256.py:17} INFO - Time to complete query: 1.1928610801696777s\n",
      "[2024-11-11 03:00:54,647] p537 {1241576256.py:24} INFO - downloading file from S3_BUCKET_NAME=athena-ek976, s3_path=data/results/d691202c-c9be-43a7-bc60-e6108b85a745.csv, to local file athena_query_results.csv\n",
      "[2024-11-11 03:00:54,743] p537 {1241576256.py:31} INFO - results dataframe shape is (1, 1)\n",
      "[2024-11-11 03:00:54,746] p537 {<timed exec>:14} INFO -      _col0\n",
      "0  6000000\n",
      "[2024-11-11 03:00:54,758] p537 {<timed exec>:15} INFO - Data fetched in 1.472409725189209s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 ms, sys: 37.1 ms, total: 273 ms\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "q = 'SELECT count(*) from \"AwsDataCatalog\".\"a05\".\"a05\";'\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=q,\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "logger.info(response)\n",
    "df_data = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "logger.info(df_data.head())\n",
    "logger.info(f\"Data fetched in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09a17a2-b149-4005-95f0-288c4cc64617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-11 03:00:58,233] p537 {1370647446.py:18} INFO - {'QueryExecutionId': 'f9d29b0b-6f78-414d-b70f-b9eeca7f5b87', 'ResponseMetadata': {'RequestId': 'dc265bd0-052a-4c47-bb32-fada842359a8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Nov 2024 03:00:58 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': 'dc265bd0-052a-4c47-bb32-fada842359a8'}, 'RetryAttempts': 0}}\n",
      "[2024-11-11 03:00:58,240] p537 {1241576256.py:4} INFO - download_and_load_query_results, enter\n",
      "[2024-11-11 03:00:58,870] p537 {1241576256.py:17} INFO - Time to complete query: 5.5844526290893555s\n",
      "[2024-11-11 03:00:58,883] p537 {1241576256.py:24} INFO - downloading file from S3_BUCKET_NAME=athena-ek976, s3_path=data/results/f9d29b0b-6f78-414d-b70f-b9eeca7f5b87.csv, to local file athena_query_results.csv\n",
      "[2024-11-11 03:00:58,989] p537 {1241576256.py:31} INFO - results dataframe shape is (17, 1)\n",
      "[2024-11-11 03:00:59,011] p537 {1370647446.py:23} INFO - df_columns.csv\n",
      "[2024-11-11 03:00:59,012] p537 {1370647446.py:25} INFO - Data fetched in 5.726130485534668s\n"
     ]
    }
   ],
   "source": [
    "# Query to get all column names from the specified table\n",
    "q = '''\n",
    "SELECT column_name \n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = 'a05' \n",
    "AND table_name = 'a05';\n",
    "'''\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=q,\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "logger.info(response)\n",
    "df_columns = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "output_file = \"df_columns.csv\"\n",
    "df_columns.to_csv(output_file, index=False)\n",
    "logger.info(f\"{output_file}\")\n",
    "\n",
    "logger.info(f\"Data fetched in {time.time() - start_time}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd1b1e-34f7-4860-bbac-685d93ffc140",
   "metadata": {},
   "source": [
    "## Problem 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02720336-e30d-485e-b680-c6b48e60cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-11 03:01:04,936] p537 {2703032465.py:20} INFO - {'QueryExecutionId': '5903af11-2510-4e67-9a7d-3293483c7964', 'ResponseMetadata': {'RequestId': 'a0bdd02d-16a0-4a22-94c8-d1a8b3f5bb3b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Nov 2024 03:01:04 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': 'a0bdd02d-16a0-4a22-94c8-d1a8b3f5bb3b'}, 'RetryAttempts': 0}}\n",
      "[2024-11-11 03:01:04,937] p537 {1241576256.py:4} INFO - download_and_load_query_results, enter\n",
      "[2024-11-11 03:01:07,569] p537 {1241576256.py:17} INFO - Time to complete query: 2.7114787101745605s\n",
      "[2024-11-11 03:01:07,585] p537 {1241576256.py:24} INFO - downloading file from S3_BUCKET_NAME=athena-ek976, s3_path=data/results/5903af11-2510-4e67-9a7d-3293483c7964.csv, to local file athena_query_results.csv\n",
      "[2024-11-11 03:01:07,707] p537 {1241576256.py:31} INFO - results dataframe shape is (10, 2)\n",
      "[2024-11-11 03:01:07,718] p537 {2703032465.py:26} INFO - Results saved to prob1_results.csv\n",
      "[2024-11-11 03:01:07,721] p537 {2703032465.py:27} INFO - Data fetched and committed in 2.863743543624878s\n"
     ]
    }
   ],
   "source": [
    "# Problem 1: Get top 10 subreddits by count of comments\n",
    "q = '''\n",
    "SELECT subreddit, COUNT(id) as comment_count\n",
    "FROM \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "GROUP BY subreddit\n",
    "ORDER BY comment_count DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "start_time = time.time()\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=q,\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "logger.info(response)\n",
    "df_top_subreddits = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "\n",
    "output_file = \"prob1_results.csv\"\n",
    "df_top_subreddits.to_csv(output_file, index=False)\n",
    "logger.info(f\"Results saved to {output_file}\")\n",
    "logger.info(f\"Data fetched and committed in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1aef70-0214-4566-9a6e-68b27ea68328",
   "metadata": {},
   "source": [
    "## Problem 2: Get 10 random rows from the comments table\n",
    "This will help you understand the schema of the table which will be useful for other queries.\n",
    "Save the results in a file called `prob2_results.csv`. This needs to be commited to the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70729387-91e1-4992-b23f-a98a8a9f22b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-22 14:46:08,027] p387 {4136649772.py:19} INFO - {'QueryExecutionId': '368fd56b-4aed-46cc-8bdb-850b8164afab', 'ResponseMetadata': {'RequestId': 'c6acaccc-a525-4329-bb73-f1f4d37b4f68', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 22 Oct 2024 14:46:08 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': 'c6acaccc-a525-4329-bb73-f1f4d37b4f68'}, 'RetryAttempts': 0}}\n",
      "[2024-10-22 14:46:08,027] p387 {1241576256.py:4} INFO - download_and_load_query_results, enter\n",
      "[2024-10-22 14:46:21,548] p387 {1241576256.py:17} INFO - Time to complete query: 13.60271167755127s\n",
      "[2024-10-22 14:46:21,556] p387 {1241576256.py:24} INFO - downloading file from S3_BUCKET_NAME=athena-ek976, s3_path=data/results/368fd56b-4aed-46cc-8bdb-850b8164afab.csv, to local file athena_query_results.csv\n",
      "[2024-10-22 14:46:21,651] p387 {1241576256.py:31} INFO - results dataframe shape is (10, 17)\n",
      "[2024-10-22 14:46:21,656] p387 {4136649772.py:25} INFO - Results saved to prob2_results.csv\n",
      "[2024-10-22 14:46:21,657] p387 {4136649772.py:27} INFO - Data fetched and committed in 13.711430549621582s\n"
     ]
    }
   ],
   "source": [
    "# Problem 2: Get 10 random rows from the comments table\n",
    "q = '''\n",
    "SELECT * \n",
    "FROM \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "ORDER BY RAND()\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "start_time = time.time()\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=q,\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "logger.info(response)\n",
    "df_random_rows = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "\n",
    "output_file = \"prob2_results.csv\"\n",
    "df_random_rows.to_csv(output_file, index=False)\n",
    "logger.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "logger.info(f\"Data fetched and committed in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e442b-2db2-4c59-95ba-1c898756cf45",
   "metadata": {},
   "source": [
    "## Problem 3: Get number of comments per day per hour and sort the results in descending order of the count\n",
    "\n",
    "The response should contain the following 3 columns: `comment_date`, `comment_hour` and `comment_count`.\n",
    "Save the results in a file called `prob3_results.csv`. This needs to be commited to the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "364d8fbc-c719-4d27-b398-81eb1b8d85dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-22 14:46:21,738] p387 {862638526.py:27} INFO - {'QueryExecutionId': 'ec5fe369-e40b-44a8-ad4e-9f412ae5f566', 'ResponseMetadata': {'RequestId': '19144222-7a42-4100-9809-6b42152085db', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 22 Oct 2024 14:46:21 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': '19144222-7a42-4100-9809-6b42152085db'}, 'RetryAttempts': 0}}\n",
      "[2024-10-22 14:46:21,738] p387 {1241576256.py:4} INFO - download_and_load_query_results, enter\n",
      "[2024-10-22 14:46:23,418] p387 {1241576256.py:17} INFO - Time to complete query: 1.7490768432617188s\n",
      "[2024-10-22 14:46:23,444] p387 {1241576256.py:24} INFO - downloading file from S3_BUCKET_NAME=athena-ek976, s3_path=data/results/ec5fe369-e40b-44a8-ad4e-9f412ae5f566.csv, to local file athena_query_results.csv\n",
      "[2024-10-22 14:46:23,542] p387 {1241576256.py:31} INFO - results dataframe shape is (18, 3)\n",
      "[2024-10-22 14:46:23,547] p387 {862638526.py:35} INFO - Results saved to prob3_results.csv\n",
      "[2024-10-22 14:46:23,548] p387 {862638526.py:37} INFO - Data fetched and committed in 1.8782305717468262s\n"
     ]
    }
   ],
   "source": [
    "# Problem 3: Get number of comments per day per hour and sort the results in descending order of the count\n",
    "q = '''\n",
    "    SELECT \n",
    "        CAST(from_unixtime(created_utc) as date) as comment_date,\n",
    "        HOUR(from_unixtime(created_utc)) as comment_hour,\n",
    "        COUNT(*) AS comment_count\n",
    "    FROM\n",
    "        \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "    GROUP BY \n",
    "        CAST(from_unixtime(created_utc) as date), \n",
    "        HOUR(from_unixtime(created_utc))\n",
    "    ORDER BY \n",
    "        comment_count DESC;\n",
    "'''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=q,\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "logger.info(response)\n",
    "\n",
    "\n",
    "df_comments_per_hour = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "\n",
    "output_file = \"prob3_results.csv\"\n",
    "df_comments_per_hour.to_csv(output_file, index=False)\n",
    "logger.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "logger.info(f\"Data fetched and committed in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8c94c-bcf9-42c9-89ec-3da9f7b5c38a",
   "metadata": {},
   "source": [
    "## Problem 4: Find top 10 subreddits by the highest average score and sort the results in descending order of the average score\n",
    "\n",
    "The response should contain the following columns: `subreddit` and `avg_score`.\n",
    "Save the results in a file called `prob4_results.csv`. This needs to be commited to the repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db970d7-93a1-4e6d-b0db-95b3134571b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-22 14:46:23,629] p387 {1747469377.py:27} INFO - {'QueryExecutionId': 'fbf64ebf-6a73-4cb8-9a33-8dd7a98b27f0', 'ResponseMetadata': {'RequestId': 'f862e7c3-ca8c-4ea0-bb3a-77198017e851', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 22 Oct 2024 14:46:23 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': 'f862e7c3-ca8c-4ea0-bb3a-77198017e851'}, 'RetryAttempts': 0}}\n",
      "[2024-10-22 14:46:23,630] p387 {1241576256.py:4} INFO - download_and_load_query_results, enter\n",
      "[2024-10-22 14:46:25,440] p387 {1241576256.py:17} INFO - Time to complete query: 1.8752353191375732s\n",
      "[2024-10-22 14:46:25,459] p387 {1241576256.py:24} INFO - downloading file from S3_BUCKET_NAME=athena-ek976, s3_path=data/results/fbf64ebf-6a73-4cb8-9a33-8dd7a98b27f0.csv, to local file athena_query_results.csv\n",
      "[2024-10-22 14:46:25,542] p387 {1241576256.py:31} INFO - results dataframe shape is (10, 2)\n",
      "[2024-10-22 14:46:25,550] p387 {1747469377.py:35} INFO - Results saved to prob4_results.csv\n",
      "[2024-10-22 14:46:25,551] p387 {1747469377.py:39} INFO - Data fetched and committed in 1.9858605861663818s\n"
     ]
    }
   ],
   "source": [
    "# Problem 4: Find top 10 subreddits by the highest average score and sort the results in descending \n",
    "# order of the average score\n",
    "q = '''\n",
    "SELECT \n",
    "    subreddit,\n",
    "    AVG(score) AS avg_score\n",
    "FROM \n",
    "    \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "GROUP BY \n",
    "    subreddit\n",
    "ORDER BY \n",
    "    avg_score DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=q,\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "logger.info(response)\n",
    "\n",
    "\n",
    "df_avg_scores = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "\n",
    "output_file = \"prob4_results.csv\"\n",
    "df_avg_scores.to_csv(output_file, index=False)\n",
    "logger.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "logger.info(f\"Data fetched and committed in {time.time() - start_time}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff1748e4-2e87-421f-bcaa-5ba009b68813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-22 14:46:33,117] p387 {1740892584.py:28} INFO - {'QueryExecutionId': '4be8a71c-f0af-40d5-a55a-6fdd32490bfc', 'ResponseMetadata': {'RequestId': '833810bb-65d6-488f-be41-71646292e5bc', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 22 Oct 2024 14:46:33 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': '833810bb-65d6-488f-be41-71646292e5bc'}, 'RetryAttempts': 0}}\n",
      "[2024-10-22 14:46:33,119] p387 {1241576256.py:4} INFO - download_and_load_query_results, enter\n",
      "[2024-10-22 14:46:37,220] p387 {1241576256.py:17} INFO - Time to complete query: 4.169274091720581s\n",
      "[2024-10-22 14:46:37,229] p387 {1241576256.py:24} INFO - downloading file from S3_BUCKET_NAME=athena-ek976, s3_path=data/results/4be8a71c-f0af-40d5-a55a-6fdd32490bfc.csv, to local file athena_query_results.csv\n",
      "[2024-10-22 14:46:37,306] p387 {1241576256.py:31} INFO - results dataframe shape is (5, 4)\n",
      "[2024-10-22 14:46:37,311] p387 {1740892584.py:36} INFO - Results saved to prob5_results.csv\n",
      "[2024-10-22 14:46:37,312] p387 {1740892584.py:39} INFO - Data fetched and committed in 4.261467456817627s\n"
     ]
    }
   ],
   "source": [
    "# Problem 5: Top 5 most Controversial Comments in a r\\datascience subreddit\n",
    "q = '''\n",
    "SELECT \n",
    "    author,\n",
    "    body,\n",
    "    score,\n",
    "    controversiality\n",
    "FROM \n",
    "    \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "WHERE \n",
    "    subreddit = 'datascience'\n",
    "ORDER BY \n",
    "    controversiality DESC, score DESC\n",
    "LIMIT 5;\n",
    "'''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=q,\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "logger.info(response)\n",
    "\n",
    "\n",
    "df_controversial_comments = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "\n",
    "output_file = \"prob5_results.csv\"\n",
    "df_controversial_comments.to_csv(output_file, index=False)\n",
    "logger.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "logger.info(f\"Data fetched and committed in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff7835-aeca-4759-94c9-94a3368f9521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
