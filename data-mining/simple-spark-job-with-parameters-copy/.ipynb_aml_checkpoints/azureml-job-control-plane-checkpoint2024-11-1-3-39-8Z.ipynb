{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Spark Job on AzureML\n",
    "\n",
    "This notebook provides an example of how to define and run a job on AzureML using Spark. This notebook is the _control plane_, meaning it creates a connection to the AzureML workspace, defines the job, and submits the job.\n",
    "\n",
    "**This Jupyter notebook should be run from within a compute instance on AzureML, in a Python kernel, specifically `Python 3.10 - SDK v2 (Python 3.10.11)`**. \n",
    "\n",
    "As you can see from the files contained in this `job` subdirectory, there are several files:\n",
    "\n",
    "- A parametrized Python script with pyspark code that is submitted to a Spark cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a client connection to the AzureML workspace\n",
    "\n",
    "The following cell creates a connection object called `azureml_client` which has a connection to the AzureML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1731375810369
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient, spark, Input, Output\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import UserIdentityConfiguration\n",
    "\n",
    "azureml_client = MLClient.from_config(\n",
    "    DefaultAzureCredential(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Job\n",
    "\n",
    "The following cell defines the job. It is an object of [Spark Class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.spark?view=azure-python) that contains the required information to run a job:\n",
    "\n",
    "- The cluster size\n",
    "- The script to run\n",
    "- The parameters for the script\n",
    "\n",
    "In the example below, we are using the `pyspark-script-job.py` script which is parametrized. As you can see, the parameters are the following:\n",
    "\n",
    "- `input_object_store_base_url` (**don't forget the trailing slashes /**): \n",
    "    - Here you will use a base URL of the `s3://<BUCKETNAME>/` form for Sagemaker, \n",
    "    - or `wasbs://<CONTAINER-NAME>@<STORAGE-ACCOUNT>.blob.core.windows.net/` \n",
    "    - or `azureml://datastores/workspaceblobstore/paths/` for AzureML. **Don't forget the trailing slash /.**\n",
    "- `input_path`: The path to read from\n",
    "- `output_object_store_base_url`: \n",
    "- `output_path`: The path to write to\n",
    "- `subreddits`: a comma separated string of subreddit names\n",
    "\n",
    "The PySpark script accepts the object store location for the raw data, in this case a single month. Then the job filters the original data and writes the filtered data out. This is designed to be used for either submissions or comments, not both.\n",
    "\n",
    "For more information about the parameters used in the job definition, [read the documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-submit-spark-jobs?view=azureml-api-2&tabs=sdk#submit-a-standalone-spark-job).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1731375844443
    }
   },
   "outputs": [],
   "source": [
    "job1_comments = spark(\n",
    "    description=\"westies-comments\",\n",
    "    code=\"./\",\n",
    "    entry={\"file\": \"pyspark-reddit-filter-script-job.py\"},\n",
    "    driver_cores=1,\n",
    "    driver_memory=\"7g\",\n",
    "    executor_cores=4,\n",
    "    executor_memory=\"7g\",\n",
    "    executor_instances=6,\n",
    "    resources={\n",
    "        \"instance_type\": \"Standard_E4S_V3\",\n",
    "        \"runtime_version\": \"3.3\",\n",
    "    },\n",
    "    inputs={       \n",
    "        \"input_object_store_base_url\": \"wasbs://reddit-project@dsan6000fall2024.blob.core.windows.net/\",\n",
    "        \"input_path\": \"202306-202407/comments/\",\n",
    "        \"output_object_store_base_url\": \"azureml://datastores/workspaceblobstore/paths/\",\n",
    "        \"output_path\": \"westies/comments/\",\n",
    "        \"subreddits\": \"WestHighlandTerriers,westies\"\n",
    "    },\n",
    "    identity=UserIdentityConfiguration(),\n",
    "    args=\"--input_object_store_base_url ${{inputs.input_object_store_base_url}} --input_path ${{inputs.input_path}} --output_object_store_base_url ${{inputs.output_object_store_base_url}} --output_path ${{inputs.output_path}} --subreddits ${{inputs.subreddits}}\"\n",
    ")\n",
    "\n",
    "job2_submissions = spark(\n",
    "    description=\"westies-submissions\",\n",
    "    code=\"./\",\n",
    "    entry={\"file\": \"pyspark-script-job.py\"},\n",
    "    driver_cores=1,\n",
    "    driver_memory=\"7g\",\n",
    "    executor_cores=4,\n",
    "    executor_memory=\"7g\",\n",
    "    executor_instances=6,\n",
    "    resources={\n",
    "        \"instance_type\": \"Standard_E4S_V3\",\n",
    "        \"runtime_version\": \"3.4\",\n",
    "    },\n",
    "    inputs={       \n",
    "        \"input_object_store_base_url\": \"wasbs://reddit-project@dsan6000fall2024.blob.core.windows.net/\",\n",
    "        \"input_path\": \"202306-202407/submissions/\",\n",
    "        \"output_object_store_base_url\": \"azureml://datastores/workspaceblobstore/paths/\",\n",
    "        \"output_path\": \"westies/submissions/\",\n",
    "        \"subreddits\": \"WestHighlandTerriers,westies\"\n",
    "    },\n",
    "    identity=UserIdentityConfiguration(),\n",
    "    args=\"--input_object_store_base_url ${{inputs.input_object_store_base_url}} --input_path ${{inputs.input_path}} --output_object_store_base_url ${{inputs.output_object_store_base_url}} --output_path ${{inputs.output_path}} --subreddits ${{inputs.subreddits}}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the job\n",
    "\n",
    "The following cell takes the job you defined above and submits it. If you are submitting multiple jobs, you may want to create separate job definition objects for clarity. You can submit more than one job, just remember that each job will spin up a Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1731375861539
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "\u001b[32mUploading project-tutorials (0.51 MBs): 100%|██████████| 514798/514798 [00:00<00:00, 1612063.87it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job1_object = azureml_client.jobs.create_or_update(job1_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1731375892859
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading project-tutorials (0.51 MBs): 100%|██████████| 514798/514798 [00:00<00:00, 1398860.37it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job2_object = azureml_client.jobs.create_or_update(job2_submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Job Studio URL\n",
    "\n",
    "Once you submit the job, you can navigate to it in the AzureML Studio and monitor it's progress. There are ways to do it through the SDK but for now just use the Studio. These are unattended jobs, which means you can shut down this notebook and the Compute Instance, but the job will go through it's lifecycle:\n",
    "\n",
    "- Job is submitted\n",
    "- Job is queued\n",
    "- Job is run\n",
    "- Job completes (assuming no errors)\n",
    "\n",
    "**Each job's Studio URL will be different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1731375905109
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ml.azure.com/runs/cyan_lunch_frrhmxy3j0?wsid=/subscriptions/21ff0fc0-dd2c-450d-93b7-96eeb3699b22/resourcegroups/prof-azureml/workspaces/prof-azureml&tid=fd571193-38cb-437b-bb55-60f28d67b643\n"
     ]
    }
   ],
   "source": [
    "job1_url = job1_object.studio_url\n",
    "print(job1_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1731360053055
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ml.azure.com/runs/cool_apple_77y7w93jt5?wsid=/subscriptions/21ff0fc0-dd2c-450d-93b7-96eeb3699b22/resourcegroups/prof-azureml/workspaces/prof-azureml&tid=fd571193-38cb-437b-bb55-60f28d67b643\n"
     ]
    }
   ],
   "source": [
    "job2_url = job1_object.studio_url\n",
    "print(job2_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeff17a1aa7735c2f7cb3a6d691fe1b4d4c3b8d2d650f644ad0f24e1b8e3f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
