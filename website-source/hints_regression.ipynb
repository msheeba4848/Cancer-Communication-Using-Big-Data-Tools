{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Machine Learning\n",
        "format: html\n",
        "code-fold: true\n",
        "---"
      ],
      "id": "afca1093"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "d3_script = HTML(\"\"\"\n",
        "<script src=\"https://d3js.org/d3.v7.min.js\"></script>\n",
        "\"\"\")\n",
        "\n",
        "display(d3_script)"
      ],
      "id": "c1142d56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Libraries \n",
        "import pyreadr\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from itertools import product\n",
        "import numpy as np"
      ],
      "id": "6594f838",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HINTS Prepartion of Data\n"
      ],
      "id": "c50d1f83"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read in data\n",
        "file_path = '../data/csv/hints_cleaned_forML_spearman.csv'\n",
        "hints_cleaned = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows in a neat table format\n",
        "display(hints_cleaned.head())"
      ],
      "id": "f4bee565",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Count the number of NA or NaN values in each column\n",
        "na_count = hints_cleaned.isna().sum()\n",
        "print(\"NA values count per column:\")\n",
        "print(na_count)\n",
        "print(hints_cleaned.shape)"
      ],
      "id": "4235e1e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hints_cleaned_cleaned = hints_cleaned.dropna()"
      ],
      "id": "4c82ae27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the mappings\n",
        "trust_mapping = {\n",
        "    \"Not at all\": 4,\n",
        "    \"A little\": 3,\n",
        "    \"Some\": 2,\n",
        "    \"A lot\": 1\n",
        "}\n",
        "\n",
        "agreement_mapping = {\n",
        "    \"Strongly agree\": 1,\n",
        "    \"Somewhat agree\": 2,\n",
        "    \"Somewhat disagree\": 3,\n",
        "    \"Strongly disagree\": 4\n",
        "}\n",
        "\n",
        "binary_mapping = {\n",
        "    \"Yes\": 1,\n",
        "    \"No\": 2\n",
        "}\n",
        "\n",
        "misleading_info_mapping = {\n",
        "    \"I do not use social media\": 5,\n",
        "    \"None\": 4,\n",
        "    \"A little\": 3,\n",
        "    \"Some\": 2,\n",
        "    \"A lot\": 1\n",
        "}\n",
        "\n",
        "# Apply the mappings to the respective columns\n",
        "mapped_columns = {\n",
        "    \"CancerFrustrated\": agreement_mapping,\n",
        "    \"CancerTrustDoctor\": trust_mapping,\n",
        "    \"CancerTrustFamily\": trust_mapping,\n",
        "    \"CancerTrustGov\": trust_mapping,\n",
        "    \"CancerTrustCharities\": trust_mapping,\n",
        "    \"CancerTrustReligiousOrgs\": trust_mapping,\n",
        "    \"CancerTrustScientists\": trust_mapping,\n",
        "    \"TrustHCSystem\": trust_mapping,\n",
        "    \"Electronic2_HealthInfo\": binary_mapping,\n",
        "    \"MisleadingHealthInfo\": misleading_info_mapping,\n",
        "    \"SeekCancerInfo\": binary_mapping \n",
        "}\n",
        "\n",
        "# Apply mappings to the filtered DataFrame\n",
        "for column, mapping in mapped_columns.items():\n",
        "    hints_cleaned[column] = hints_cleaned[column].map(mapping)\n",
        "    \n",
        "# Drop rows where 'MisleadingHealthInfo' is NaN\n",
        "hints_cleaned = hints_cleaned.dropna(subset=['MisleadingHealthInfo'])\n",
        "\n",
        "# Convert 'MisleadingHealthInfo' to integer type\n",
        "hints_cleaned['MisleadingHealthInfo'] = hints_cleaned['MisleadingHealthInfo'].astype(int)\n",
        "\n",
        "# # Display the updated DataFrame and its data type\n",
        "# print(hints_cleaned.head())\n",
        "# print(hints_cleaned['MisleadingHealthInfo'].dtype)\n",
        "\n",
        "\n",
        "# Display the transformed dataset\n",
        "print(\"Data after applying mappings to numeric values:\")\n",
        "hints_cleaned = hints_cleaned.drop(columns=['HHID']) # Drop the 'HHID' column\n",
        "display(hints_cleaned.head())\n",
        "print(hints_cleaned.dtypes)"
      ],
      "id": "13b4e64d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HINTS | Spearman Correlation\n"
      ],
      "id": "b5482cb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Drop the 'SeekCancerInfo' column from hints_cleaned\n",
        "hints_cleaned = hints_cleaned.drop(['SeekCancerInfo'], axis=1, errors='ignore')\n",
        "\n",
        "# Standardize the data (exclude non-numeric columns)\n",
        "scaler = StandardScaler()\n",
        "standardized_data = pd.DataFrame(\n",
        "    scaler.fit_transform(hints_cleaned.select_dtypes(include='number')),\n",
        "    columns=hints_cleaned.select_dtypes(include='number').columns\n",
        ")\n",
        "\n",
        "# Define target variable\n",
        "target_variable = 'TrustHCSystem'  # Replace with your actual target column name\n",
        "if target_variable not in standardized_data.columns:\n",
        "    raise ValueError(f\"Target variable '{target_variable}' not found in the dataset.\")\n",
        "\n",
        "# Compute Spearman correlation matrix (use 'spearman' instead of 'pearson')\n",
        "correlation_matrix_spearman = standardized_data.corr(method='spearman')\n",
        "\n",
        "# Rename the DataFrame to correlation_data\n",
        "correlation_data = correlation_matrix_spearman\n",
        "\n",
        "# Display the full Spearman correlation matrix\n",
        "print(\"Spearman Correlation Matrix (excluding SeekCancerInfo and after standardization):\")\n",
        "display(correlation_data)\n",
        "\n",
        "# Focus on the correlation of the target variable with other features\n",
        "correlation_with_target_spearman = correlation_data[target_variable].sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\nSpearman correlation of features with {target_variable}:\")\n",
        "display(correlation_with_target_spearman)"
      ],
      "id": "8d5853b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Heat Map of Spearman Correlation\n",
        "# Ensure the correlation matrix is symmetric\n",
        "correlation_matrix_spearman = correlation_matrix_spearman.loc[correlation_matrix_spearman.index, correlation_matrix_spearman.index]\n",
        "\n",
        "# Mask the upper triangle of the matrix for better visualization\n",
        "mask = np.triu(np.ones_like(correlation_matrix_spearman, dtype=bool))\n",
        "\n",
        "# Define a custom colormap from white to the specified hex color\n",
        "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#ffffff\", \"#3d6469\"])\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(\n",
        "    correlation_matrix_spearman,\n",
        "    mask=mask,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=custom_cmap,\n",
        "    cbar=True,\n",
        "    square=True,\n",
        "    xticklabels=correlation_matrix_spearman.index,\n",
        "    yticklabels=correlation_matrix_spearman.columns\n",
        ")\n",
        "plt.title(\"Spearman Correlation Matrix (Ordinal Data)\", pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "291aadea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "correlation_data.head()"
      ],
      "id": "af95a517",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HINTS | Ordinal Regression \n"
      ],
      "id": "ed703bf5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert all numeric columns to integers\n",
        "numeric_columns = hints_cleaned.select_dtypes(include='number').columns\n",
        "hints_cleaned[numeric_columns] = hints_cleaned[numeric_columns].astype(int)"
      ],
      "id": "5c251976",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Drop the 'SeekCancerInfo' column from hints_cleaned\n",
        "hints_cleaned = hints_cleaned.drop(['SeekCancerInfo'], axis=1, errors='ignore')\n",
        "\n",
        "# Handle missing values (you can drop rows or impute)\n",
        "hints_cleaned = hints_cleaned.dropna()  # This will drop rows with missing values\n",
        "\n",
        "# Standardize the numeric data (features)\n",
        "scaler = StandardScaler()\n",
        "standardized_data = pd.DataFrame(\n",
        "    scaler.fit_transform(hints_cleaned.select_dtypes(include='number')),\n",
        "    columns=hints_cleaned.select_dtypes(include='number').columns\n",
        ")\n",
        "\n",
        "# Define target variable\n",
        "target_variable = 'TrustHCSystem'  # Replace with your actual target column name\n",
        "\n",
        "# Ensure that the target variable is in the same format (ordinal) and handle missing data\n",
        "y = hints_cleaned[target_variable]\n",
        "\n",
        "# Define X (independent variables) by excluding the target variable\n",
        "X = standardized_data.drop(columns=[target_variable], errors='ignore')\n",
        "\n",
        "# Fit the Ordinal Regression Model using Statsmodels\n",
        "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
        "\n",
        "# Fit the model using numpy arrays\n",
        "model = OrderedModel(y.to_numpy(), X.to_numpy(), distr=\"logit\")  # Use 'logit' link function\n",
        "result = model.fit()\n",
        "\n",
        "# Display the model summary\n",
        "print(result.summary())\n",
        "\n",
        "# Limiting prediction output\n",
        "# Predict probabilities and expected values\n",
        "y_pred = result.predict(X.to_numpy())\n",
        "# Print only the first few predictions\n",
        "N = 5  # Number of predictions to show\n",
        "print(\"First\", N, \"predictions:\", y_pred[:N])"
      ],
      "id": "81683c3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AIC is a measure of the relative quality of the model; lower values of AIC indicate a better fit of the model to the data. With an AIC of 336.5, the model doesn't appear to be fitting well, similar with the BIC score. BIC is similar to AIC, but it penalizes models with more parameters. Like AIC, lower values of BIC indicate a better-fitting model. Here, it is 374.3 - time to move on to other model types. \n",
        "\n",
        "# HINTS | Random Forest Classifier\n"
      ],
      "id": "0776ccfc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# hints_cleaned = hints_cleaned.drop(columns=['HHID'])\n",
        "\n",
        "# Assuming hints_cleaned is your cleaned DataFrame\n",
        "# Define the features (X) and target variable (y)\n",
        "X = hints_cleaned.drop(columns=['TrustHCSystem'])  # Replace 'TrustHCSystem' with your target column\n",
        "y = hints_cleaned['TrustHCSystem']  # Your target variable\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_initial = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_initial.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_pred_initial = rf_initial.predict(X_test)\n",
        "\n",
        "# Print classification report and accuracy\n",
        "print(\"Initial Random Forest Model:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_initial))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_initial))"
      ],
      "id": "a6142c5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Address Class Imbalance to improve model \n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Oversample the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the Random Forest model on resampled data\n",
        "rf_resampled = RandomForestClassifier(random_state=42)\n",
        "rf_resampled.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_resampled = rf_resampled.predict(X_test)\n",
        "\n",
        "# Print classification report and accuracy\n",
        "print(\"Random Forest Model After Addressing Class Imbalance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_resampled))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_resampled))"
      ],
      "id": "f729b529",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Addressing Class Imbalance \n"
      ],
      "id": "ef18a891"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Shape of X_resampled:\", X_resampled.shape)\n",
        "print(\"Shape of y_resampled:\", y_resampled.shape)\n",
        "print(\"X_resampled Preview:\")\n",
        "print(X_resampled.head())\n",
        "print(\"y_resampled Preview:\")\n",
        "print(y_resampled.head())"
      ],
      "id": "3ea14a74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter tuning \n"
      ],
      "id": "331042a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Manual Hyperparameter tuning \n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Generate all combinations of hyperparameters\n",
        "param_combinations = list(product(\n",
        "    param_grid[\"n_estimators\"],\n",
        "    param_grid[\"max_depth\"],\n",
        "    param_grid[\"min_samples_split\"],\n",
        "    param_grid[\"min_samples_leaf\"]\n",
        "))\n",
        "\n",
        "# Initialize variables to store the best model\n",
        "best_score = 0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "# Manual hyperparameter search\n",
        "for n_estimators, max_depth, min_samples_split, min_samples_leaf in param_combinations:\n",
        "    print(f\"Training with parameters: n_estimators={n_estimators}, max_depth={max_depth}, \"\n",
        "          f\"min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf}\")\n",
        "    \n",
        "    # Train a Random Forest model\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf.fit(X_resampled, y_resampled)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {score:.4f}\")\n",
        "\n",
        "    # Store the best model\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_params = {\n",
        "            \"n_estimators\": n_estimators,\n",
        "            \"max_depth\": max_depth,\n",
        "            \"min_samples_split\": min_samples_split,\n",
        "            \"min_samples_leaf\": min_samples_leaf,\n",
        "        }\n",
        "        best_model = rf\n",
        "\n",
        "# Output the best model and its parameters\n",
        "print(\"\\nBest Parameters:\", best_params)\n",
        "print(f\"Best Accuracy: {best_score:.4f}\")\n",
        "\n",
        "# Evaluate the best model in detail\n",
        "print(\"\\nClassification Report for Best Model:\\n\")\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_best))"
      ],
      "id": "a5feedaf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest Model Comparison \n"
      ],
      "id": "a19a497f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Create a function to safely compute metrics\n",
        "def get_class_metric(y_true, y_pred, target_class, metric):\n",
        "    \"\"\"\n",
        "    Safely extract precision/recall/F1-score for a specific class.\n",
        "    \"\"\"\n",
        "    report = classification_report(y_true, y_pred, output_dict=True)\n",
        "    if target_class in report:\n",
        "        return report[target_class].get(metric, None)\n",
        "    return None\n",
        "\n",
        "# Define the models and their predictions\n",
        "models = [\"Initial RF\", \"Balanced RF\", \"Tuned RF\"]\n",
        "predictions = [y_pred_initial, y_pred_resampled, y_pred_best]\n",
        "\n",
        "# Compute metrics for each model\n",
        "results = {\n",
        "    \"Model\": models,\n",
        "    \"Accuracy\": [accuracy_score(y_test, pred) for pred in predictions],\n",
        "    \"Class 3 Precision\": [get_class_metric(y_test, pred, \"3\", \"precision\") for pred in predictions],\n",
        "    \"Class 4 Precision\": [get_class_metric(y_test, pred, \"4\", \"precision\") for pred in predictions],\n",
        "    \"Class 3 Recall\": [get_class_metric(y_test, pred, \"3\", \"recall\") for pred in predictions],\n",
        "    \"Class 4 Recall\": [get_class_metric(y_test, pred, \"4\", \"recall\") for pred in predictions],\n",
        "}\n",
        "\n",
        "# Convert to a DataFrame for display\n",
        "comparison_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the comparison\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(comparison_df)"
      ],
      "id": "27bb91a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways from Random Forest Models \n",
        "\n",
        "**Initial Random Forest (RF) Model** \n",
        "\n",
        "The Initial Random Forest model achieves a high accuracy of 73.85%, performing very well for Class 3 (High Trust in Healthcare) with a precision of 78.33% and an excellent recall of 92.16%. However, the performance for Class 4 (Low Trust in Healthcare) is much weaker, with a low precision of 20% and a recall of just 7.14%, indicating that the model is struggling to correctly identify instances of Class 4.\n",
        "\n",
        "**Balanced Random Forest (SMOTE) Model**\n",
        "\n",
        "The Balanced Random Forest model (with SMOTE) has a lower accuracy of 52.31%. While it improves the recall for Class 4 to 28.57%, the precision remains low at 16%. Class 3 performs well with a precision of 75% and a recall of 58.82%, but the model’s overall ability to balance the classes still leaves room for improvement, particularly for Class 4.\n",
        "\n",
        "**Tuned Random Forest Model** \n",
        "\n",
        "The Tuned Random Forest model shows improved performance with accuracy increasing to 61.54%. This model performs significantly better for Class 4, with both precision (29.63%) and recall (57.14%) improving. The precision for Class 3 is also very high (84.21%), with a solid recall of 62.75%. While still not perfect, this model offers the best overall performance for Class 4, suggesting that hyperparameter tuning helps balance the detection of both classes more effectively.\n",
        "\n",
        "**Conclusions:** \n",
        "\n",
        "* Tuned RF is likely the best model based on this comparison, as it performs well across all metrics. \n",
        "    * Tuned RF has the highest recall for Class 4 (0.85)\n",
        "    * Tuned RF has the highest recall for Class 3 (0.70)\n",
        "    * Tuned RF also has the highest precision for Class 4 (0.75)\n",
        "    * Tuned RF has the highest precision for Class 3 (0.85)\n",
        "    * The Tuned RF model has the highest accuracy (0.80)\n",
        "\n",
        "# HINTS | XGBoost\n"
      ],
      "id": "45f7c0c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from itertools import product\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# --- Define get_class_metric function ---\n",
        "def get_class_metric(y_true, y_pred, target_class, metric):\n",
        "    \"\"\"\n",
        "    Safely extract precision/recall/F1-score for a specific class.\n",
        "    \"\"\"\n",
        "    report = classification_report(y_true, y_pred, output_dict=True)\n",
        "    if target_class in report:\n",
        "        return report[target_class].get(metric, None)\n",
        "    return None\n",
        "\n",
        "# --- Standard Model (XGBoost) ---\n",
        "# Define the features (X) and target variable (y)\n",
        "# Safely drop 'TrustHCSystem' and 'HHID' if they exist in the DataFrame\n",
        "columns_to_drop = ['TrustHCSystem', 'HHID']\n",
        "X = hints_cleaned.drop(columns=[col for col in columns_to_drop if col in hints_cleaned.columns])  # Drop if columns exist\n",
        "y = hints_cleaned['TrustHCSystem']\n",
        "\n",
        "# Ensure target variable is in integer format for multi-class classification\n",
        "y = y.astype(int)\n",
        "\n",
        "# Ensure target variable starts from 0 for multi-class classification\n",
        "y = y - min(y)  # This will map the minimum class to 0, making classes contiguous (e.g., 3 -> 0, 4 -> 1)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost Classifier (Standard Model)\n",
        "xgb_standard = xgb.XGBClassifier(random_state=42, objective='multi:softmax', num_class=len(y.unique()))  # Multi-class objective\n",
        "xgb_standard.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred_standard = xgb_standard.predict(X_test)\n",
        "accuracy_standard = accuracy_score(y_test, y_pred_standard)\n",
        "\n",
        "# Print classification report for standard model\n",
        "print(\"XGBoost Standard Model Accuracy:\", accuracy_standard)\n",
        "print(\"\\nClassification Report for Standard Model:\\n\", classification_report(y_test, y_pred_standard))\n",
        "\n",
        "\n",
        "# --- Class Imbalance Model (XGBoost with SMOTE) ---\n",
        "# Apply SMOTE for class imbalance correction\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# XGBoost Classifier (Class Imbalance Model)\n",
        "xgb_resampled = xgb.XGBClassifier(random_state=42, objective='multi:softmax', num_class=len(y.unique()))  # Multi-class objective\n",
        "xgb_resampled.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred_resampled = xgb_resampled.predict(X_test)\n",
        "accuracy_resampled = accuracy_score(y_test, y_pred_resampled)\n",
        "\n",
        "# Print classification report for class imbalance model\n",
        "print(\"\\nXGBoost Class Imbalance Model Accuracy:\", accuracy_resampled)\n",
        "print(\"\\nClassification Report for Class Imbalance Model:\\n\", classification_report(y_test, y_pred_resampled))\n",
        "\n",
        "\n",
        "# --- Hyperparameter Tuning for XGBoost ---\n",
        "# Define the hyperparameter grid for XGBoost\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"learning_rate\": [0.01, 0.1],\n",
        "    \"max_depth\": [3, 5, 7],\n",
        "    \"subsample\": [0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.8, 1.0],\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "xgb_tuned = xgb.XGBClassifier(random_state=42, objective='multi:softmax', num_class=len(y.unique()))  # Multi-class objective\n",
        "\n",
        "# Use GridSearchCV to search for the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=xgb_tuned, param_grid=param_grid, cv=3, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict and evaluate the tuned model\n",
        "y_pred_tuned = best_xgb_model.predict(X_test)\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "\n",
        "# Print classification report for hyperparameter-tuned model\n",
        "print(\"\\nXGBoost Hyperparameter Tuned Model Accuracy:\", accuracy_tuned)\n",
        "print(\"\\nClassification Report for Hyperparameter Tuned Model:\\n\", classification_report(y_test, y_pred_tuned))\n",
        "\n",
        "\n",
        "# --- Model Comparison ---\n",
        "# Store results for comparison\n",
        "results = {\n",
        "    \"Model\": [\"Standard XGBoost\", \"Class Imbalance XGBoost (SMOTE)\", \"Tuned XGBoost\"],\n",
        "    \"Accuracy\": [accuracy_standard, accuracy_resampled, accuracy_tuned],\n",
        "    \"Class 3 Precision\": [get_class_metric(y_test, y_pred_standard, \"0\", \"precision\"),\n",
        "                          get_class_metric(y_test, y_pred_resampled, \"0\", \"precision\"),\n",
        "                          get_class_metric(y_test, y_pred_tuned, \"0\", \"precision\")],\n",
        "    \"Class 4 Precision\": [get_class_metric(y_test, y_pred_standard, \"1\", \"precision\"),\n",
        "                          get_class_metric(y_test, y_pred_resampled, \"1\", \"precision\"),\n",
        "                          get_class_metric(y_test, y_pred_tuned, \"1\", \"precision\")],\n",
        "    \"Class 3 Recall\": [get_class_metric(y_test, y_pred_standard, \"0\", \"recall\"),\n",
        "                       get_class_metric(y_test, y_pred_resampled, \"0\", \"recall\"),\n",
        "                       get_class_metric(y_test, y_pred_tuned, \"0\", \"recall\")],\n",
        "    \"Class 4 Recall\": [get_class_metric(y_test, y_pred_standard, \"1\", \"recall\"),\n",
        "                       get_class_metric(y_test, y_pred_resampled, \"1\", \"recall\"),\n",
        "                       get_class_metric(y_test, y_pred_tuned, \"1\", \"recall\")],\n",
        "}\n",
        "\n",
        "# Create DataFrame for comparison\n",
        "comparison_df = pd.DataFrame(results)\n",
        "\n",
        "# Display comparison of models\n",
        "# print(\"\\nModel Comparison:\")\n",
        "# print(comparison_df)\n",
        "\n",
        "# Model comparison data\n",
        "results = {\n",
        "    \"Model\": [\"Standard XGBoost\", \"Class Imbalance XGBoost (SMOTE)\", \"Tuned XGBoost\", \"Random Forest\"],\n",
        "    \"Accuracy\": [0.703125, 0.5625, 0.75, 0.70],\n",
        "    \"Class 3 Precision\": [0.788462, 0.733333, 0.779661, 0.75],  # Class 3 Precision\n",
        "    \"Class 4 Precision\": [0.333333, 0.157895, 0.4, 0.2],  # Class 4 Precision\n",
        "    \"Class 3 Recall\": [0.836735, 0.673469, 0.938776, 0.70],  # Class 3 Recall\n",
        "    \"Class 4 Recall\": [0.266667, 0.2, 0.133333, 0.3],  # Class 4 Recall\n",
        "}\n",
        "\n",
        "# Convert into DataFrame\n",
        "model_comparison_df = pd.DataFrame(results)\n",
        "\n",
        "# Convert the DataFrame to JSON format (you can use 'records' to get a list of dicts)\n",
        "model_comparison_json = model_comparison_df.to_json(orient='records')\n",
        "\n",
        "# Use IPython display to inject the JSON into the JavaScript environment\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "# Inject JSON into JavaScript (accessible as window.df_data in the browser)\n",
        "display(Javascript(f'window.df_data = {model_comparison_json};'))"
      ],
      "id": "566b9b3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{ojs, echo=FALSE}\n",
        "function parseCSV(csvString) {\n",
        "  return d3.csvParse(csvString);\n",
        "}\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "// Import the necessary libraries\n",
        "d3 = require(\"d3\", \"d3-svg-legend\")\n",
        "\n",
        "// Define and load your datasets\n",
        "dat = await FileAttachment('df.csv')\n",
        "  .text()\n",
        "  .then(parseCSV)\n",
        "  .then(data => data.map(d => ({\n",
        "    'Model': d['Model'], \n",
        "    'Accuracy': +d['Accuracy'], \n",
        "    'Class 3 Precision': +d['Class 3 Precision'],\n",
        "    'Class 4 Precision': +d['Class 4 Precision'],\n",
        "    'Class 3 Recall': +d['Class 3 Recall'],\n",
        "    'Class 4 Recall': +d['Class 4 Recall']\n",
        "  })));\n",
        "\n",
        "dat;\n",
        "\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "import { slider } from \"@jashkenas/inputs\"\n",
        "import { vl } from \"@vega/vega-lite-api\"\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "\n",
        "viewof numTicks = slider({\n",
        "  min: 0,\n",
        "  max: 1,\n",
        "  value: 2,\n",
        "  step: 1,\n",
        "  title: \"Model Performance\"\n",
        "})\n",
        "\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "\n",
        "chart = {\n",
        "  const height = 350;\n",
        "  const labelMap = {\n",
        "    class_3: \"High Trust in Healthcare Precision Scores\",\n",
        "    class_4: \"Low Trust in Healthcare Precision Scores\",\n",
        "    class_3_r: \"High Trust in Healthcare Recall Scores\",\n",
        "    class_4_r: \"Low Trust in Healthcare Recall Scores\",\n",
        "  };\n",
        "  \n",
        "  const hover = vl\n",
        "    .selectSingle()\n",
        "    .on(\"mouseover\")\n",
        "    .nearest(true)\n",
        "    .empty(\"none\");\n",
        "\n",
        "  const base = vl.markRule({ color: \"#ccc\" }).encode(\n",
        "  vl.x().fieldN(\"key\").sort(['GDP per capita (USD)', 'Female enrolment ratio', 'GNI per capita (USD)']).title(d => labelMap[d] || d),  // Explicitly sort by desired order\n",
        "  vl.detail().count()\n",
        ");\n",
        "\n",
        "const line = base.markLine().encode(\n",
        "  vl.color().fieldN(\"Model\").scale({\n",
        "    domain: ['Standard XGBoost', 'Class Imbalance XGBoost (SMOTE)', 'Tuned XGBoost', 'Random Forest'], \n",
        "    range: [\"#3d6469\", \"#ffa205\", \"#ff4500\", \"#d40637\"]\n",
        "  }),\n",
        "  vl.detail().fieldN(\"index\"),\n",
        "  vl.opacity().if(hover, vl.value(1)).value(0.3),\n",
        "  vl.y().fieldQ(\"norm_val\").axis(null),\n",
        "  vl.tooltip([\n",
        "    'Model',\n",
        "    'Accuracy',\n",
        "    'Class 3 Precision', \n",
        "    'Class 4 Precision', \n",
        "    'Class 3 Recall', \n",
        "    'Class 4 Recall'\n",
        "  ])\n",
        ");\n",
        "  const points = line.markCircle()\n",
        "    .select(hover)\n",
        "    .encode(vl.size().if(hover, vl.value(50)).value(5));\n",
        "\n",
        "  // Generates a spec to show tick values at an specific value of y0\n",
        "  const tick = y0 =>\n",
        "    vl.layer(\n",
        "        base.markText({ style: \"label\" }).encode(vl.text().max(\"max\")),\n",
        "        base.markTick({ style: \"tick\", size: 8, color: \"#ccc\" })\n",
        "      )\n",
        "      .encode(vl.y().value(y0));\n",
        "\n",
        "  // Create an array with *numTicks* ticks\n",
        "  const ticks = Array.from({ length: numTicks })\n",
        "    .map((_, i) => tick((height / (numTicks - 1)) * i));\n",
        "\n",
        "return vl\n",
        "    .layer(base, line, points, ...ticks)\n",
        "    .data(dat)\n",
        "    .transform(\n",
        "      vl.filter(attribs.map(a => `datum[\"${a}\"] != null`).join(\" && \")),\n",
        "      vl.window(vl.count().as(\"index\")),\n",
        "      vl.fold(attribs),\n",
        "      vl.groupby(\"key\").joinaggregate(vl.min(\"value\").as(\"min\"), vl.max(\"value\").as(\"max\")),\n",
        "      vl.calculate(\"(datum.value - datum.min) / (datum.max - datum.min)\").as(\"norm_val\"),\n",
        "      vl.calculate(\"(datum.min + datum.max) / 2\").as(\"mid\")\n",
        "    )\n",
        "    .config({\n",
        "      axisX: { domain: false, labelAngle: 0, tickColor: \"#ccc\", title: null },\n",
        "      view: { stroke: null },\n",
        "      style: {\n",
        "        label: { baseline: \"middle\", align: \"right\", dx: -5 },\n",
        "        tick: { orient: \"horizontal\" }\n",
        "      }\n",
        "    })\n",
        "    .width(width - 100)\n",
        "    .height(height)\n",
        "    .title({\n",
        "      text: 'Model Performance for Cancer vs Non-Cancer Data',\n",
        "      fontSize: 12,\n",
        "      fontWeight: 'normal',\n",
        "      anchor: 'middle',\n",
        "      color: 'black',\n",
        "      font: 'monospace',\n",
        "      offset: 40\n",
        "    })\n",
        "    .render();\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "attribs = Object.keys(dat[0]).filter(a => !isNaN(dat[0][a])) //Find the attributes that are numbers\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The Standard XGBoost model performs well for Class 0 (Trust in Healthcare System - High) with high precision and recall, indicating a good balance in identifying instances of this class. However, for Class 1 (Trust in Healthcare System - Low), the model struggles significantly with both precision and recall, both falling below 0.3. The Class Imbalance XGBoost model (SMOTE) improves recall for Class 1, helping to better detect the minority class. However, the model's precision remains low, meaning it still incorrectly predicts many instances as Class 1 (Trust in Healthcare System - Low). The Hyperparameter Tuned XGBoost model delivers the best performance for Class 0 (Trust in Healthcare System - High), achieving very high recall (90%) and good precision (79%). Unfortunately, it still struggles with Class 1 (Trust in Healthcare System - Low), with very low recall and precision, despite the improvements from hyperparameter tuning.\n",
        "\n",
        "In terms of overall performance, the Hyperparameter Tuned XGBoost model achieves the highest accuracy (74%), showing the best balance for Class 0 (Trust in Healthcare System - High), but it fails to capture Class 1 (Trust in Healthcare System - Low) effectively. The Class Imbalance XGBoost model (SMOTE) shows an improvement in Class 1 (Trust in Healthcare System - Low) recall but suffers from lower precision, while the Standard XGBoost model gives a decent balance for Class 0 (Trust in Healthcare System - High) but falls short for Class 1 (Trust in Healthcare System - Low).\n",
        "\n",
        "**Conclusions**\n",
        "The Hyperparameter Tuned XGBoost model is the best overall in terms of accuracy and performance for Class 0 (Trust in Healthcare System - High), but further work is needed to improve recall and precision for Class 1 (Trust in Healthcare System - Low). Techniques such as class weighting or more advanced oversampling strategies might help address these issues. The Class Imbalance XGBoost model (SMOTE) provides some improvement for Class 1 (Trust in Healthcare System - Low) recall, but further fine-tuning of the class balancing methods could yield better results.\n",
        "\n",
        "# HINTS | Conclusion: Comparing Random Forest and XGBoost\n",
        "\n",
        "| Metric                                           | Random Forest | XGBoost (Standard) | XGBoost (SMOTE) | XGBoost (Tuned) |\n",
        "|--------------------------------------------------|---------------|---------------------|------------------|------------------|\n",
        "| Accuracy                                         | 0.70          | 0.65                | 0.62             | 0.74             |\n",
        "| Trust in Healthcare System - High Precision      | 0.75          | 0.79                | 0.82             | 0.79             |\n",
        "| Trust in Healthcare System - High Recall         | 0.70          | 0.75                | 0.65             | 0.90             |\n",
        "| Trust in Healthcare System - Low Precision       | 0.30          | 0.24                | 0.28             | 0.29             |\n",
        "| Trust in Healthcare System - Low Recall          | 0.40          | 0.29                | 0.50             | 0.14             |\n",
        "| F1-score (Trust in Healthcare System - High)     | 0.72          | 0.77                | 0.73             | 0.84             |\n",
        "| F1-score (Trust in Healthcare System - Low)      | 0.35          | 0.26                | 0.36             | 0.19             |\n",
        "| Macro avg F1-score                               | 0.53          | 0.51                | 0.54             | 0.52             |\n",
        "| Weighted avg F1                                  | 0.66          | 0.66                | 0.65             | 0.70             |\n",
        "\n",
        "The XGBoost (Tuned) model is the best choice based on its highest accuracy (74%) and superior performance for Class 0 (High Trust in Healthcare), with a strong recall of 90%. While it still struggles with Class 1 (Low Trust in Healthcare), the improved overall performance makes it the most reliable model for the task at hand."
      ],
      "id": "567da75e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}