{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d04ee1-7e6c-4d65-8e7f-2242e3adf4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e3bb7-b7ac-4ee2-a736-8f1a6e03fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6384709-3dfd-4368-b461-8933ffa46ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES_TO_FIX: str = \"files.txt\"\n",
    "FILES_TO_FIX: str = \"missing_files.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0992ff-8433-4ada-9a7b-d3b7ff4c8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(FILES_TO_FIX).read_text()\n",
    "files = files.split(\"\\n\")\n",
    "logger.info(f\"there {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a08139-8250-4a74-8233-4c422c76b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83b5cf-e207-4d09-be7d-40ecaaf8494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "def handle_file(params):\n",
    "    f, idx, num_files = params\n",
    "    st = time.perf_counter()\n",
    "    s3_path = f\"s3://bigdatateaching/{f}\"\n",
    "    logger.info(f\"{idx} of {num_files}, start for {s3_path}\")\n",
    "    df = pd.read_parquet(s3_path)\n",
    "    cols = df.columns\n",
    "    if \"edited\" in cols:\n",
    "        df.edited = df.edited.astype(float)\n",
    "    if \"created_utc\" in cols:\n",
    "        df.created_utc = df.created_utc.astype(int)\n",
    "    if \"retrieved_on\" in cols:\n",
    "        df.retrieved_on = df.retrieved_on.astype(int)\n",
    "    s3_dest_path = f\"s3://bigdatateaching/reddit-project/{f}\"\n",
    "    df.to_parquet(s3_dest_path, compression='zstd')\n",
    "    tt = time.perf_counter() - st\n",
    "    logger.info(f\"{idx} of {num_files}, finished handling for {s3_path}, written to {s3_dest_path}, time taken = {tt}\")\n",
    "\n",
    "overall_st = time.perf_counter()\n",
    "num_files = len(files)\n",
    "processes = 1\n",
    "pool = mp.Pool(processes)\n",
    "\n",
    "\n",
    "def exclude_file(f):\n",
    "    files_to_exclude = [\"comments_RC_2023-11.zst_163.parquet\", \n",
    "                        \"comments_RC_2024-05.zst_200.parquet\",\n",
    "                        \"comments_RC_2024-05.zst_251.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_222.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_65.parquet\",\n",
    "                        \"comments_RC_2024-02.zst_197.parquet\",\n",
    "                        \"comments_RC_2023-07.zst_74.parquet\",\n",
    "                        \"comments_RC_2023-07.zst_75.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_223.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_47.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_48.parquet\", \n",
    "                        \"comments_RC_2023-11.zst_5.parquet\"\n",
    "                       ]\n",
    "    for exclude_pattern in files_to_exclude:\n",
    "        if exclude_pattern in f:\n",
    "            return True\n",
    "    return False\n",
    "file_tuples = [(f, i+1, num_files) for i, f in enumerate(files) if exclude_file(f) is False]\n",
    "logger.info(f\"there are {len(file_tuples)} to handle\")\n",
    "with mp.Pool(processes) as p:\n",
    "    result = p.map(handle_file, file_tuples)\n",
    "overall_total_time = time.perf_counter() - st\n",
    "logger.info(f\"{len(files)} files handled, overall time taken = {tt}\")\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
