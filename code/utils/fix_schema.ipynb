{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d04ee1-7e6c-4d65-8e7f-2242e3adf4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075e3bb7-b7ac-4ee2-a736-8f1a6e03fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6384709-3dfd-4368-b461-8933ffa46ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES_TO_FIX: str = \"files.txt\"\n",
    "FILES_TO_FIX: str = \"missing_files.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0992ff-8433-4ada-9a7b-d3b7ff4c8a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 18:35:30,191,INFO,1519321262,1519321262.py,3,there 21 files\n"
     ]
    }
   ],
   "source": [
    "files = Path(FILES_TO_FIX).read_text()\n",
    "files = files.split(\"\\n\")\n",
    "logger.info(f\"there {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a08139-8250-4a74-8233-4c422c76b97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reddit/parquet/comments/yyyy=2023/mm=07/comments_RC_2023-07.zst_74.parquet'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c83b5cf-e207-4d09-be7d-40ecaaf8494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 18:48:46,264,INFO,<timed exec>,<timed exec>,46,there are 9 to handle\n",
      "2024-10-06 18:48:46,270,INFO,<timed exec>,<timed exec>,7,10 of 21, start for s3://bigdatateaching/reddit/parquet/comments/yyyy=2023/mm=11/comments_RC_2023-12.zst_30.parquet\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "When getting information for key 'reddit/parquet/comments/yyyy=2023/mm=11/comments_RC_2023-12.zst_30.parquet' in bucket 'bigdatateaching': AWS Error ACCESS_DENIED during HeadObject operation: No response body.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:49\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:8\u001b[0m, in \u001b[0;36mhandle_file\u001b[0;34m(params)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    268\u001b[0m     path,\n\u001b[1;32m    269\u001b[0m     filesystem,\n\u001b[1;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/parquet/core.py:1776\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1770\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1771\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1773\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/parquet/core.py:1329\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m         filesystem \u001b[38;5;241m=\u001b[39m LocalFileSystem(use_mmap\u001b[38;5;241m=\u001b[39mmemory_map)\n\u001b[0;32m-> 1329\u001b[0m finfo \u001b[38;5;241m=\u001b[39m \u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m finfo\u001b[38;5;241m.\u001b[39mis_file:\n\u001b[1;32m   1331\u001b[0m     single_file \u001b[38;5;241m=\u001b[39m path_or_paths\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/_fs.pyx:581\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.get_file_info\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: When getting information for key 'reddit/parquet/comments/yyyy=2023/mm=11/comments_RC_2023-12.zst_30.parquet' in bucket 'bigdatateaching': AWS Error ACCESS_DENIED during HeadObject operation: No response body."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "def handle_file(params):\n",
    "    f, idx, num_files = params\n",
    "    st = time.perf_counter()\n",
    "    s3_path = f\"s3://bigdatateaching/{f}\"\n",
    "    logger.info(f\"{idx} of {num_files}, start for {s3_path}\")\n",
    "    df = pd.read_parquet(s3_path)\n",
    "    cols = df.columns\n",
    "    if \"edited\" in cols:\n",
    "        df.edited = df.edited.astype(float)\n",
    "    if \"created_utc\" in cols:\n",
    "        df.created_utc = df.created_utc.astype(int)\n",
    "    if \"retrieved_on\" in cols:\n",
    "        df.retrieved_on = df.retrieved_on.astype(int)\n",
    "    s3_dest_path = f\"s3://bigdatateaching/reddit-project/{f}\"\n",
    "    df.to_parquet(s3_dest_path, compression='zstd')\n",
    "    tt = time.perf_counter() - st\n",
    "    logger.info(f\"{idx} of {num_files}, finished handling for {s3_path}, written to {s3_dest_path}, time taken = {tt}\")\n",
    "\n",
    "overall_st = time.perf_counter()\n",
    "num_files = len(files)\n",
    "processes = 1\n",
    "pool = mp.Pool(processes)\n",
    "\n",
    "\n",
    "def exclude_file(f):\n",
    "    files_to_exclude = [\"comments_RC_2023-11.zst_163.parquet\", \n",
    "                        \"comments_RC_2024-05.zst_200.parquet\",\n",
    "                        \"comments_RC_2024-05.zst_251.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_222.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_65.parquet\",\n",
    "                        \"comments_RC_2024-02.zst_197.parquet\",\n",
    "                        \"comments_RC_2023-07.zst_74.parquet\",\n",
    "                        \"comments_RC_2023-07.zst_75.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_223.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_47.parquet\",\n",
    "                        \"comments_RC_2023-11.zst_48.parquet\", \n",
    "                        \"comments_RC_2023-11.zst_5.parquet\"\n",
    "                       ]\n",
    "    for exclude_pattern in files_to_exclude:\n",
    "        if exclude_pattern in f:\n",
    "            return True\n",
    "    return False\n",
    "file_tuples = [(f, i+1, num_files) for i, f in enumerate(files) if exclude_file(f) is False]\n",
    "logger.info(f\"there are {len(file_tuples)} to handle\")\n",
    "# with mp.Pool(processes) as p:\n",
    "#     result = p.map(handle_file, file_tuples)\n",
    "_ = [handle_file(ft) for ft in file_tuples]\n",
    "overall_total_time = time.perf_counter() - st\n",
    "logger.info(f\"{len(files)} files handled, overall time taken = {tt}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb6a3a-0678-4f8d-9d86-b2655aea03e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
