{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n",
        "\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "output_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "#df_read_back.show(5)\n",
        "#df_read_back.printSchema()\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "df_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\"))) \\\n",
        "            .select(\"text\", \"sentiment_result.result\")\n",
        "\n",
        "# Show the results\n",
        "#df_flat.show(truncate=False)\n",
        "\n",
        "# Create a 100% predictable dataset\n",
        "df = spark.createDataFrame([\n",
        "    (\"I feel great\", \"pos\", \"cancer\"),\n",
        "    (\"This is amazing\", \"pos\", \"non-cancer\"),\n",
        "    (\"Not good at all\", \"neg\", \"cancer\"),\n",
        "    (\"Terrible experience\", \"neg\", \"non-cancer\")\n",
        "], [\"text\", \"sentiment\", \"cancer\"])\n",
        "\n",
        "print(df.show())\n",
        "\n",
        "#df = df_flat\n",
        "df = df.withColumnRenamed(\"result\", \"sentiment\")\n",
        "df.show()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 18,
              "statement_ids": [
                18
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 7,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 74,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 18:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\n#df_read_back.show(5)\n#df_read_back.printSchema()\n\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"text\", \"sentiment_result.result\")\n\n# Show the results\n#df_flat.show(truncate=False)\n\n# Create a 100% predictable dataset\ndf = spark.createDataFrame([\n    (\"I feel great\", \"pos\", \"cancer\"),\n    (\"This is amazing\", \"po...",
                    "submissionTime": "2024-12-08T02:19:02.165GMT",
                    "completionTime": "2024-12-08T02:19:02.394GMT",
                    "stageIds": [
                      93
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 73,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 18:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\n#df_read_back.show(5)\n#df_read_back.printSchema()\n\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"text\", \"sentiment_result.result\")\n\n# Show the results\n#df_flat.show(truncate=False)\n\n# Create a 100% predictable dataset\ndf = spark.createDataFrame([\n    (\"I feel great\", \"pos\", \"cancer\"),\n    (\"This is amazing\", \"po...",
                    "submissionTime": "2024-12-08T02:19:01.936GMT",
                    "completionTime": "2024-12-08T02:19:02.154GMT",
                    "stageIds": [
                      92
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 4,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 4,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 4,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 72,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 18:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\n#df_read_back.show(5)\n#df_read_back.printSchema()\n\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"text\", \"sentiment_result.result\")\n\n# Show the results\n#df_flat.show(truncate=False)\n\n# Create a 100% predictable dataset\ndf = spark.createDataFrame([\n    (\"I feel great\", \"pos\", \"cancer\"),\n    (\"This is amazing\", \"po...",
                    "submissionTime": "2024-12-08T02:19:01.800GMT",
                    "completionTime": "2024-12-08T02:19:01.929GMT",
                    "stageIds": [
                      91
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 71,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 18:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\n#df_read_back.show(5)\n#df_read_back.printSchema()\n\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"text\", \"sentiment_result.result\")\n\n# Show the results\n#df_flat.show(truncate=False)\n\n# Create a 100% predictable dataset\ndf = spark.createDataFrame([\n    (\"I feel great\", \"pos\", \"cancer\"),\n    (\"This is amazing\", \"po...",
                    "submissionTime": "2024-12-08T02:19:01.526GMT",
                    "completionTime": "2024-12-08T02:19:01.738GMT",
                    "stageIds": [
                      90
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 70,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 18:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\n#df_read_back.show(5)\n#df_read_back.printSchema()\n\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"text\", \"sentiment_result.result\")\n\n# Show the results\n#df_flat.show(truncate=False)\n\n# Create a 100% predictable dataset\ndf = spark.createDataFrame([\n    (\"I feel great\", \"pos\", \"cancer\"),\n    (\"This is amazing\", \"po...",
                    "submissionTime": "2024-12-08T02:19:01.331GMT",
                    "completionTime": "2024-12-08T02:19:01.517GMT",
                    "stageIds": [
                      89
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 4,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 4,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 4,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 69,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 18:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\n#df_read_back.show(5)\n#df_read_back.printSchema()\n\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"text\", \"sentiment_result.result\")\n\n# Show the results\n#df_flat.show(truncate=False)\n\n# Create a 100% predictable dataset\ndf = spark.createDataFrame([\n    (\"I feel great\", \"pos\", \"cancer\"),\n    (\"This is amazing\", \"po...",
                    "submissionTime": "2024-12-08T02:19:01.123GMT",
                    "completionTime": "2024-12-08T02:19:01.320GMT",
                    "stageIds": [
                      88
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 68,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 18:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\n#df_read_back.show(5)\n#df_read_back.printSchema()\n\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"text\", \"sentiment_result.result\")\n\n# Show the results\n#df_flat.show(truncate=False)\n\n# Create a 100% predictable dataset\ndf = spark.createDataFrame([\n    (\"I feel great\", \"pos\", \"cancer\"),\n    (\"This is amazing\", \"po...",
                    "submissionTime": "2024-12-08T02:19:00.866GMT",
                    "completionTime": "2024-12-08T02:19:00.971GMT",
                    "stageIds": [
                      87
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "36",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T02:19:00.5231611Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T02:19:00.6395467Z",
              "execution_finish_time": "2024-12-08T02:19:03.1006981Z",
              "parent_msg_id": "ac7683bd-5348-418f-8ccd-f473f5ffeaf4"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 36, 18, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+-------------------+---------+----------+\n|               text|sentiment|    cancer|\n+-------------------+---------+----------+\n|       I feel great|      pos|    cancer|\n|    This is amazing|      pos|non-cancer|\n|    Not good at all|      neg|    cancer|\n|Terrible experience|      neg|non-cancer|\n+-------------------+---------+----------+\n\nNone\n+-------------------+---------+----------+\n|               text|sentiment|    cancer|\n+-------------------+---------+----------+\n|       I feel great|      pos|    cancer|\n|    This is amazing|      pos|non-cancer|\n|    Not good at all|      neg|    cancer|\n|Terrible experience|      neg|non-cancer|\n+-------------------+---------+----------+\n\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1733624343193
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Tokenizer and stop words removal\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "\n",
        "# Vectorization and TF-IDF\n",
        "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Label encoding\n",
        "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n",
        "\n",
        "# Fit and transform\n",
        "processed_df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Split the data (use all data for training for simplicity)\n",
        "train_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", modelType=\"multinomial\")\n",
        "nb_model = nb.fit(train_data)\n",
        "\n",
        "# Predict\n",
        "predictions = nb_model.transform(test_data)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 19,
              "statement_ids": [
                19
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 8,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 58,
                    "dataRead": 58,
                    "rowCount": 2,
                    "usageDescription": "",
                    "jobId": 82,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 19:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"cancer\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Split the data (use all data for training for simplicity)\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Naive Bayes model\nnb = NaiveB...",
                    "submissionTime": "2024-12-08T02:19:09.350GMT",
                    "completionTime": "2024-12-08T02:19:09.850GMT",
                    "stageIds": [
                      107,
                      106
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at NaiveBayes.scala:194",
                    "dataWritten": 0,
                    "dataRead": 1929,
                    "rowCount": 3,
                    "usageDescription": "",
                    "jobId": 81,
                    "name": "collect at NaiveBayes.scala:194",
                    "description": "Job group for statement 19:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"cancer\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Split the data (use all data for training for simplicity)\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Naive Bayes model\nnb = NaiveB...",
                    "submissionTime": "2024-12-08T02:19:09.016GMT",
                    "completionTime": "2024-12-08T02:19:09.076GMT",
                    "stageIds": [
                      104,
                      105
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at NaiveBayes.scala:194",
                    "dataWritten": 1929,
                    "dataRead": 0,
                    "rowCount": 3,
                    "usageDescription": "",
                    "jobId": 80,
                    "name": "collect at NaiveBayes.scala:194",
                    "description": "Job group for statement 19:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"cancer\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Split the data (use all data for training for simplicity)\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Naive Bayes model\nnb = NaiveB...",
                    "submissionTime": "2024-12-08T02:19:08.435GMT",
                    "completionTime": "2024-12-08T02:19:08.956GMT",
                    "stageIds": [
                      103
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 2386,
                    "rowCount": 8,
                    "usageDescription": "",
                    "jobId": 79,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 19:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"cancer\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Split the data (use all data for training for simplicity)\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Naive Bayes model\nnb = NaiveB...",
                    "submissionTime": "2024-12-08T02:19:07.676GMT",
                    "completionTime": "2024-12-08T02:19:07.758GMT",
                    "stageIds": [
                      102,
                      101
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 2386,
                    "dataRead": 0,
                    "rowCount": 8,
                    "usageDescription": "",
                    "jobId": 78,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 19:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"cancer\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Split the data (use all data for training for simplicity)\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Naive Bayes model\nnb = NaiveB...",
                    "submissionTime": "2024-12-08T02:19:07.104GMT",
                    "completionTime": "2024-12-08T02:19:07.594GMT",
                    "stageIds": [
                      100
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at IDF.scala:55",
                    "dataWritten": 1084,
                    "dataRead": 1084,
                    "rowCount": 16,
                    "usageDescription": "",
                    "jobId": 77,
                    "name": "treeAggregate at IDF.scala:55",
                    "description": "Job group for statement 19:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"cancer\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Split the data (use all data for training for simplicity)\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Naive Bayes model\nnb = NaiveB...",
                    "submissionTime": "2024-12-08T02:19:06.502GMT",
                    "completionTime": "2024-12-08T02:19:06.959GMT",
                    "stageIds": [
                      99,
                      98
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "top at CountVectorizer.scala:236",
                    "dataWritten": 0,
                    "dataRead": 736,
                    "rowCount": 6,
                    "usageDescription": "",
                    "jobId": 76,
                    "name": "top at CountVectorizer.scala:236",
                    "description": "Job group for statement 19:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"cancer\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Split the data (use all data for training for simplicity)\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Naive Bayes model\nnb = NaiveB...",
                    "submissionTime": "2024-12-08T02:19:06.278GMT",
                    "completionTime": "2024-12-08T02:19:06.322GMT",
                    "stageIds": [
                      96,
                      97
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at CountVectorizer.scala:233",
                    "dataWritten": 386,
                    "dataRead": 386,
                    "rowCount": 12,
                    "usageDescription": "",
                    "jobId": 75,
                    "name": "count at CountVectorizer.scala:233",
                    "description": "Job group for statement 19:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"cancer\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Split the data (use all data for training for simplicity)\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Naive Bayes model\nnb = NaiveB...",
                    "submissionTime": "2024-12-08T02:19:05.650GMT",
                    "completionTime": "2024-12-08T02:19:06.263GMT",
                    "stageIds": [
                      94,
                      95
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 16,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 16,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 16,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "36",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T02:19:05.2725136Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T02:19:05.4108314Z",
              "execution_finish_time": "2024-12-08T02:19:10.3197279Z",
              "parent_msg_id": "ec866d75-c3c2-48a5-91b3-faa6acdfa065"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 36, 19, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 1.00\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733624350423
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}