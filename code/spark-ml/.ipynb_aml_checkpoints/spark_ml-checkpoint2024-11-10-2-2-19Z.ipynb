{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n",
        "\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733795708739
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "cancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\n",
        "cancer_df = spark.read.parquet(cancer_path)\n",
        "\n",
        "not_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\n",
        "not_cancer = spark.read.parquet(not_cancer_path)\n",
        "\n",
        "cancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n",
        "\n",
        "not_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n",
        "\n",
        "combined_df = cancer_df.union(not_cancer)\n",
        "combined_df = combined_df.select('text', 'source')\n",
        "\n",
        "combined_df.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1733795740945
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
        "from pyspark.ml.classification import NaiveBayes, LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Tokenizer and stop words removal\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "\n",
        "# Vectorization and TF-IDF\n",
        "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Label encoding\n",
        "indexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n",
        "\n",
        "# Fit and transform\n",
        "processed_df = pipeline.fit(combined_df).transform(combined_df)\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n",
        "\n",
        "# Fit and transform\n",
        "processed_df = pipeline.fit(combined_df).transform(combined_df)\n",
        "\n",
        "# Split the data 80-20\n",
        "train_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", modelType=\"multinomial\")\n",
        "nb_model = nb.fit(train_data)\n",
        "\n",
        "# Predict\n",
        "predictions = nb_model.transform(test_data)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator()\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733796100509
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Precision (Multiclass)\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "print(f\"Test Precision: {precision:.2f}\")\n",
        "\n",
        "# Weighted Recall (Multiclass)\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "print(f\"Test Recall: {recall:.2f}\")\n",
        "\n",
        "# F1-Score (Multiclass)\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"Test F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "# AUC-ROC (For Binary Classification, works for binary classification)\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc_roc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"Test AUC-ROC: {auc_roc:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691476368
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Confusion Matrix (for Multiclass)\n",
        "# Create confusion matrix by grouping by label and prediction\n",
        "confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().toPandas()\n",
        "\n",
        "# Show the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix)\n",
        "\n",
        "# Optionally, to display it as a matrix format:\n",
        "confusion_matrix_pivot = confusion_matrix.pivot(index=\"label\", columns=\"prediction\", values=\"count\").fillna(0)\n",
        "print(\"\\nConfusion Matrix (Pivoted):\")\n",
        "print(confusion_matrix_pivot)\n",
        "\n",
        "custom_colors = [\"#d40637\", \"#3d6469\"]\n",
        "cmap = ListedColormap(custom_colors)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix_pivot, annot=True, fmt='g', cmap=cmap, cbar=True, linewidths=0.5)\n",
        "plt.title(\"Naive Bayes Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691727993
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split the data 80-20\n",
        "train_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Predict\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691708885
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Precision (Multiclass)\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "print(f\"Test Precision: {precision:.2f}\")\n",
        "\n",
        "# Weighted Recall (Multiclass)\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "print(f\"Test Recall: {recall:.2f}\")\n",
        "\n",
        "# F1-Score (Multiclass)\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"Test F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "# AUC-ROC (For Binary Classification, works for binary classification)\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc_roc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"Test AUC-ROC: {auc_roc:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691895662
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# Confusion Matrix (for Multiclass)\n",
        "# Create confusion matrix by grouping by label and prediction\n",
        "confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().toPandas()\n",
        "\n",
        "# Show the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix)\n",
        "\n",
        "# Optionally, to display it as a matrix format:\n",
        "confusion_matrix_pivot = confusion_matrix.pivot(index=\"label\", columns=\"prediction\", values=\"count\").fillna(0)\n",
        "print(\"\\nConfusion Matrix (Pivoted):\")\n",
        "print(confusion_matrix_pivot)\n",
        "\n",
        "custom_colors = [\"#d40637\", \"#3d6469\"]\n",
        "cmap = ListedColormap(custom_colors)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix_pivot, annot=True, fmt='g', cmap=cmap, cbar=True, linewidths=0.5)\n",
        "plt.title(\"Logistic Regression Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}