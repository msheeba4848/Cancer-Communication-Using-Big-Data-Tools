{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n",
        "\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 6:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)",
                    "submissionTime": "2024-12-08T20:45:53.295GMT",
                    "completionTime": "2024-12-08T20:47:16.149GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "6",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T20:44:23.4254609Z",
              "session_start_time": "2024-12-08T20:44:23.5027814Z",
              "execution_start_time": "2024-12-08T20:45:52.3609094Z",
              "execution_finish_time": "2024-12-08T20:47:20.0662398Z",
              "parent_msg_id": "d98e2046-0caf-4987-8fba-474c110a0324"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733690840162
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "cancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\n",
        "cancer_df = spark.read.parquet(cancer_path)\n",
        "\n",
        "not_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\n",
        "not_cancer = spark.read.parquet(not_cancer_path)\n",
        "\n",
        "# Add a new column 'source' indicating 'cancer' for cancer data\n",
        "cancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n",
        "\n",
        "# Add a new column 'source' indicating 'non_cancer' for non-cancer data\n",
        "not_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n",
        "\n",
        "# Combine both DataFrames using union\n",
        "combined_df = cancer_df.union(not_cancer)\n",
        "\n",
        "# Show the combined dataset\n",
        "combined_df.show()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 103892,
                    "rowCount": 5,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 7:\n\n\n# Show first 5 rows\n#df_read_back.show(5)\n#df_read_back.printSchema()\n\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"text\", \"sentiment_result.result\")\n\ndf = df_flat\ndf = df.withColumnRenamed(\"result\", \"sentiment\")\ndf.show()\n\n",
                    "submissionTime": "2024-12-08T20:49:56.364GMT",
                    "completionTime": "2024-12-08T20:50:05.217GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T20:49:54.2266732Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T20:49:54.4012142Z",
              "execution_finish_time": "2024-12-08T20:50:06.5053782Z",
              "parent_msg_id": "6a4f3171-f5fb-4645-905e-bfa565c3e66a"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+---------+\n|                text|sentiment|\n+--------------------+---------+\n|Hi, I hope this i...| negative|\n|Hi, I hope this i...| positive|\n|Hi, I hope this i...| negative|\n|Hi, I hope this i...| negative|\n|Hi, I hope this i...| positive|\n|Hi, I hope this i...| positive|\n|Hi, I hope this i...| positive|\n|Hi, I hope this i...| negative|\n|Hi, I hope this i...| positive|\n|Hi, I hope this i...| negative|\n|Hi, I hope this i...| negative|\n|Hi, I hope this i...| positive|\n|33 male, i have d...| negative|\n|33 male, i have d...| positive|\n|33 male, i have d...| negative|\n|33 male, i have d...| negative|\n|           [removed]|       na|\n|           [removed]|       na|\n|83F, Cancer dx (u...| negative|\n|83F, Cancer dx (u...| negative|\n+--------------------+---------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1733691006663
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
        "from pyspark.ml.classification import NaiveBayes, LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Tokenizer and stop words removal\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "\n",
        "# Vectorization and TF-IDF\n",
        "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Label encoding\n",
        "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n",
        "\n",
        "# Fit and transform\n",
        "processed_df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n",
        "\n",
        "# Fit and transform\n",
        "processed_df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Split the data 80-20\n",
        "train_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", modelType=\"multinomial\")\n",
        "nb_model = nb.fit(train_data)\n",
        "\n",
        "# Predict\n",
        "predictions = nb_model.transform(test_data)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator()\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 13,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 145,
                    "dataRead": 93360,
                    "rowCount": 116,
                    "usageDescription": "",
                    "jobId": 177,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:34.803GMT",
                    "completionTime": "2024-12-08T03:30:35.310GMT",
                    "stageIds": [
                      216,
                      217
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at NaiveBayes.scala:194",
                    "dataWritten": 0,
                    "dataRead": 22317,
                    "rowCount": 3,
                    "usageDescription": "",
                    "jobId": 176,
                    "name": "collect at NaiveBayes.scala:194",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:34.493GMT",
                    "completionTime": "2024-12-08T03:30:34.548GMT",
                    "stageIds": [
                      215,
                      214
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at NaiveBayes.scala:194",
                    "dataWritten": 22317,
                    "dataRead": 93215,
                    "rowCount": 103,
                    "usageDescription": "",
                    "jobId": 175,
                    "name": "collect at NaiveBayes.scala:194",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:33.938GMT",
                    "completionTime": "2024-12-08T03:30:34.446GMT",
                    "stageIds": [
                      213
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 335,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 174,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:33.351GMT",
                    "completionTime": "2024-12-08T03:30:33.412GMT",
                    "stageIds": [
                      212,
                      211
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 335,
                    "dataRead": 27679,
                    "rowCount": 101,
                    "usageDescription": "",
                    "jobId": 173,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:33.126GMT",
                    "completionTime": "2024-12-08T03:30:33.300GMT",
                    "stageIds": [
                      210
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at IDF.scala:55",
                    "dataWritten": 0,
                    "dataRead": 99109,
                    "rowCount": 100,
                    "usageDescription": "",
                    "jobId": 172,
                    "name": "treeAggregate at IDF.scala:55",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:32.437GMT",
                    "completionTime": "2024-12-08T03:30:32.881GMT",
                    "stageIds": [
                      209
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "top at CountVectorizer.scala:236",
                    "dataWritten": 0,
                    "dataRead": 266712,
                    "rowCount": 3028,
                    "usageDescription": "",
                    "jobId": 171,
                    "name": "top at CountVectorizer.scala:236",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:32.262GMT",
                    "completionTime": "2024-12-08T03:30:32.294GMT",
                    "stageIds": [
                      208,
                      207
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at CountVectorizer.scala:233",
                    "dataWritten": 30255,
                    "dataRead": 129364,
                    "rowCount": 6156,
                    "usageDescription": "",
                    "jobId": 170,
                    "name": "count at CountVectorizer.scala:233",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:31.671GMT",
                    "completionTime": "2024-12-08T03:30:32.241GMT",
                    "stageIds": [
                      205,
                      206
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 335,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 169,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:31.232GMT",
                    "completionTime": "2024-12-08T03:30:31.295GMT",
                    "stageIds": [
                      204,
                      203
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 335,
                    "dataRead": 27679,
                    "rowCount": 101,
                    "usageDescription": "",
                    "jobId": 168,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:31.050GMT",
                    "completionTime": "2024-12-08T03:30:31.211GMT",
                    "stageIds": [
                      202
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at IDF.scala:55",
                    "dataWritten": 0,
                    "dataRead": 99109,
                    "rowCount": 100,
                    "usageDescription": "",
                    "jobId": 167,
                    "name": "treeAggregate at IDF.scala:55",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:30.415GMT",
                    "completionTime": "2024-12-08T03:30:30.899GMT",
                    "stageIds": [
                      201
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "top at CountVectorizer.scala:236",
                    "dataWritten": 0,
                    "dataRead": 266712,
                    "rowCount": 3028,
                    "usageDescription": "",
                    "jobId": 166,
                    "name": "top at CountVectorizer.scala:236",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:30.228GMT",
                    "completionTime": "2024-12-08T03:30:30.271GMT",
                    "stageIds": [
                      199,
                      200
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at CountVectorizer.scala:233",
                    "dataWritten": 30255,
                    "dataRead": 129364,
                    "rowCount": 6156,
                    "usageDescription": "",
                    "jobId": 165,
                    "name": "count at CountVectorizer.scala:233",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(df).transform(df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = p...",
                    "submissionTime": "2024-12-08T03:30:29.667GMT",
                    "completionTime": "2024-12-08T03:30:30.212GMT",
                    "stageIds": [
                      197,
                      198
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "37",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T03:30:29.32686Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T03:30:29.4696346Z",
              "execution_finish_time": "2024-12-08T03:30:36.6033691Z",
              "parent_msg_id": "0348d4f8-6c47-4764-a57a-d0c9485c385b"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 37, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.31\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733628636719
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Precision (Multiclass)\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "print(f\"Test Precision: {precision:.2f}\")\n",
        "\n",
        "# Weighted Recall (Multiclass)\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "print(f\"Test Recall: {recall:.2f}\")\n",
        "\n",
        "# F1-Score (Multiclass)\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"Test F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "# AUC-ROC (For Binary Classification, works for binary classification)\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc_roc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"Test AUC-ROC: {auc_roc:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 6,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "collect at AreaUnderCurve.scala:44",
                    "dataWritten": 0,
                    "dataRead": 168,
                    "rowCount": 3,
                    "usageDescription": "",
                    "jobId": 184,
                    "name": "collect at AreaUnderCurve.scala:44",
                    "description": "Job group for statement 15:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T03:33:54.466GMT",
                    "completionTime": "2024-12-08T03:33:54.517GMT",
                    "stageIds": [
                      233,
                      234,
                      232
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 2,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at BinaryClassificationMetrics.scala:237",
                    "dataWritten": 0,
                    "dataRead": 168,
                    "rowCount": 3,
                    "usageDescription": "",
                    "jobId": 183,
                    "name": "collect at BinaryClassificationMetrics.scala:237",
                    "description": "Job group for statement 15:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T03:33:54.398GMT",
                    "completionTime": "2024-12-08T03:33:54.436GMT",
                    "stageIds": [
                      230,
                      231,
                      229
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 2,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at BinaryClassificationMetrics.scala:197",
                    "dataWritten": 326,
                    "dataRead": 93541,
                    "rowCount": 112,
                    "usageDescription": "",
                    "jobId": 182,
                    "name": "count at BinaryClassificationMetrics.scala:197",
                    "description": "Job group for statement 15:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T03:33:53.733GMT",
                    "completionTime": "2024-12-08T03:33:54.373GMT",
                    "stageIds": [
                      227,
                      228,
                      226
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3,
                    "numActiveStages": 0,
                    "numCompletedStages": 3,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 145,
                    "dataRead": 93360,
                    "rowCount": 116,
                    "usageDescription": "",
                    "jobId": 181,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 15:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T03:33:52.948GMT",
                    "completionTime": "2024-12-08T03:33:53.397GMT",
                    "stageIds": [
                      224,
                      225
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 145,
                    "dataRead": 93360,
                    "rowCount": 116,
                    "usageDescription": "",
                    "jobId": 180,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 15:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T03:33:52.338GMT",
                    "completionTime": "2024-12-08T03:33:52.813GMT",
                    "stageIds": [
                      222,
                      223
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 145,
                    "dataRead": 93360,
                    "rowCount": 116,
                    "usageDescription": "",
                    "jobId": 179,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 15:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T03:33:51.718GMT",
                    "completionTime": "2024-12-08T03:33:52.179GMT",
                    "stageIds": [
                      220,
                      221
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "37",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T03:33:51.4348784Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T03:33:51.5572278Z",
              "execution_finish_time": "2024-12-08T03:33:55.2524647Z",
              "parent_msg_id": "e46e84d4-b0c0-46a8-9d7f-ca6b9fcd0a34"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 37, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Precision: 0.55\nTest Recall: 0.31\nTest F1-Score: 0.39\nTest AUC-ROC: 0.47\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733628835376
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split the data 80-20\n",
        "train_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Predict\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Accuracy\n",
        "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Precision\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"precision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "print(f\"Test Precision: {precision:.2f}\")\n",
        "\n",
        "# Recall\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"recall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "print(f\"Test Recall: {recall:.2f}\")\n",
        "\n",
        "# F1-Score\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"Test F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "# AUC-ROC (for binary classification)\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc_roc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"Test AUC-ROC: {auc_roc:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "prediction_and_labels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(prediction_and_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(metrics.confusionMatrix().toArray())\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 114,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 123,
                    "dataRead": 93338,
                    "rowCount": 112,
                    "usageDescription": "",
                    "jobId": 123,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:25:00.334GMT",
                    "completionTime": "2024-12-08T03:25:00.964GMT",
                    "stageIds": [
                      129,
                      128
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 122,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:59.356GMT",
                    "completionTime": "2024-12-08T03:24:59.398GMT",
                    "stageIds": [
                      127
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 121,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:59.298GMT",
                    "completionTime": "2024-12-08T03:24:59.334GMT",
                    "stageIds": [
                      126
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 120,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:59.240GMT",
                    "completionTime": "2024-12-08T03:24:59.278GMT",
                    "stageIds": [
                      125
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 119,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:59.153GMT",
                    "completionTime": "2024-12-08T03:24:59.214GMT",
                    "stageIds": [
                      124
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 118,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.962GMT",
                    "completionTime": "2024-12-08T03:24:59.020GMT",
                    "stageIds": [
                      123
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 117,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.840GMT",
                    "completionTime": "2024-12-08T03:24:58.890GMT",
                    "stageIds": [
                      122
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 116,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.772GMT",
                    "completionTime": "2024-12-08T03:24:58.820GMT",
                    "stageIds": [
                      121
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 115,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.718GMT",
                    "completionTime": "2024-12-08T03:24:58.756GMT",
                    "stageIds": [
                      120
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 114,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.655GMT",
                    "completionTime": "2024-12-08T03:24:58.699GMT",
                    "stageIds": [
                      119
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 113,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.600GMT",
                    "completionTime": "2024-12-08T03:24:58.636GMT",
                    "stageIds": [
                      118
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 112,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.537GMT",
                    "completionTime": "2024-12-08T03:24:58.575GMT",
                    "stageIds": [
                      117
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 111,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.467GMT",
                    "completionTime": "2024-12-08T03:24:58.506GMT",
                    "stageIds": [
                      116
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 110,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.403GMT",
                    "completionTime": "2024-12-08T03:24:58.447GMT",
                    "stageIds": [
                      115
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 109,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.182GMT",
                    "completionTime": "2024-12-08T03:24:58.314GMT",
                    "stageIds": [
                      114
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 108,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.117GMT",
                    "completionTime": "2024-12-08T03:24:58.156GMT",
                    "stageIds": [
                      113
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 107,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:58.021GMT",
                    "completionTime": "2024-12-08T03:24:58.095GMT",
                    "stageIds": [
                      112
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 106,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:57.958GMT",
                    "completionTime": "2024-12-08T03:24:57.997GMT",
                    "stageIds": [
                      111
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 105,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:57.893GMT",
                    "completionTime": "2024-12-08T03:24:57.940GMT",
                    "stageIds": [
                      110
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 962312,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 104,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 8:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n",
                    "submissionTime": "2024-12-08T03:24:57.831GMT",
                    "completionTime": "2024-12-08T03:24:57.873GMT",
                    "stageIds": [
                      109
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "37",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T03:24:45.9780009Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T03:24:46.1014602Z",
              "execution_finish_time": "2024-12-08T03:25:03.114834Z",
              "parent_msg_id": "f1a249aa-1e1c-4b98-b184-3d09ee95d01e"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 37, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.58\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733628303268
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Precision (Multiclass)\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "print(f\"Test Precision: {precision:.2f}\")\n",
        "\n",
        "# Weighted Recall (Multiclass)\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "print(f\"Test Recall: {recall:.2f}\")\n",
        "\n",
        "# F1-Score (Multiclass)\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"Test F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "# AUC-ROC (For Binary Classification, works for binary classification)\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc_roc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"Test AUC-ROC: {auc_roc:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}