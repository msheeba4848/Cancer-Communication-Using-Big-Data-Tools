{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n",
        "\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 6:\nfrom pyspark.sql import SparkSession\n\n# Start Spark session\nspark = SparkSession.builder.appName(\"NaiveBayes_Example\").getOrCreate()\n\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)",
                    "submissionTime": "2024-12-08T20:45:53.295GMT",
                    "completionTime": "2024-12-08T20:47:16.149GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "6",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T20:44:23.4254609Z",
              "session_start_time": "2024-12-08T20:44:23.5027814Z",
              "execution_start_time": "2024-12-08T20:45:52.3609094Z",
              "execution_finish_time": "2024-12-08T20:47:20.0662398Z",
              "parent_msg_id": "d98e2046-0caf-4987-8fba-474c110a0324"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733690840162
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "cancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\n",
        "cancer_df = spark.read.parquet(cancer_path)\n",
        "\n",
        "not_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\n",
        "not_cancer = spark.read.parquet(not_cancer_path)\n",
        "\n",
        "cancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n",
        "\n",
        "not_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n",
        "\n",
        "combined_df = cancer_df.union(not_cancer)\n",
        "combined_df = combined_df.select('text', 'source')\n",
        "\n",
        "combined_df.show()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 4
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2954586,
                    "rowCount": 8192,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\nfrom pyspark.sql.functions import lit\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ncombined_df = cancer_df.union(not_cancer)\ncombined_df = combined_df.select('text', 'source')\n\ncombined_df.show()\n\n",
                    "submissionTime": "2024-12-08T20:54:58.750GMT",
                    "completionTime": "2024-12-08T20:54:59.214GMT",
                    "stageIds": [
                      9
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 4,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 4,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 4,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 11254,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\nfrom pyspark.sql.functions import lit\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ncombined_df = cancer_df.union(not_cancer)\ncombined_df = combined_df.select('text', 'source')\n\ncombined_df.show()\n\n",
                    "submissionTime": "2024-12-08T20:54:58.408GMT",
                    "completionTime": "2024-12-08T20:54:58.725GMT",
                    "stageIds": [
                      8
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\nfrom pyspark.sql.functions import lit\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ncombined_df = cancer_df.union(not_cancer)\ncombined_df = combined_df.select('text', 'source')\n\ncombined_df.show()\n\n",
                    "submissionTime": "2024-12-08T20:54:57.862GMT",
                    "completionTime": "2024-12-08T20:54:58.023GMT",
                    "stageIds": [
                      7
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\nfrom pyspark.sql.functions import lit\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ncombined_df = cancer_df.union(not_cancer)\ncombined_df = combined_df.select('text', 'source')\n\ncombined_df.show()\n\n",
                    "submissionTime": "2024-12-08T20:54:57.465GMT",
                    "completionTime": "2024-12-08T20:54:57.644GMT",
                    "stageIds": [
                      6
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T20:54:57.1225205Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T20:54:57.2466301Z",
              "execution_finish_time": "2024-12-08T20:54:59.7074395Z",
              "parent_msg_id": "5314ba4f-ead6-421e-80da-c9f1532a0a8d"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+------+\n|                text|source|\n+--------------------+------+\n|Check out Northsi...|cancer|\n|I had something s...|cancer|\n|That's an insulti...|cancer|\n|Yeah sorry, it wa...|cancer|\n|I see my colorect...|cancer|\n|The couple of ran...|cancer|\n|I’ve encountered ...|cancer|\n|I 100% agree with...|cancer|\n|You should not ha...|cancer|\n|**Your post has b...|cancer|\n|Completely agree ...|cancer|\n|Butt's have oil s...|cancer|\n|I just found I ha...|cancer|\n|in the mid 2000s ...|cancer|\n|If you know all y...|cancer|\n|Ultimately, us nu...|cancer|\n|I had one the siz...|cancer|\n|yeah I see that n...|cancer|\n|This has been a r...|cancer|\n|Day to day is dif...|cancer|\n+--------------------+------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1733691299788
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
        "from pyspark.ml.classification import NaiveBayes, LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Tokenizer and stop words removal\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "\n",
        "# Vectorization and TF-IDF\n",
        "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Label encoding\n",
        "indexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n",
        "\n",
        "# Fit and transform\n",
        "processed_df = pipeline.fit(combined_df).transform(combined_df)\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n",
        "\n",
        "# Fit and transform\n",
        "processed_df = pipeline.fit(combined_df).transform(combined_df)\n",
        "\n",
        "# Split the data 80-20\n",
        "train_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", modelType=\"multinomial\")\n",
        "nb_model = nb.fit(train_data)\n",
        "\n",
        "# Predict\n",
        "predictions = nb_model.transform(test_data)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator()\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 14
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 244,
                    "dataRead": 2976957,
                    "rowCount": 20008,
                    "usageDescription": "",
                    "jobId": 40,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:35.660GMT",
                    "completionTime": "2024-12-08T20:57:36.698GMT",
                    "stageIds": [
                      64,
                      65
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 244,
                    "dataRead": 2976957,
                    "rowCount": 20008,
                    "usageDescription": "",
                    "jobId": 39,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:34.240GMT",
                    "completionTime": "2024-12-08T20:57:35.374GMT",
                    "stageIds": [
                      63,
                      62
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at NaiveBayes.scala:194",
                    "dataWritten": 0,
                    "dataRead": 489950,
                    "rowCount": 2,
                    "usageDescription": "",
                    "jobId": 38,
                    "name": "collect at NaiveBayes.scala:194",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:33.414GMT",
                    "completionTime": "2024-12-08T20:57:33.806GMT",
                    "stageIds": [
                      60,
                      61
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 7,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 6,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at NaiveBayes.scala:194",
                    "dataWritten": 489950,
                    "dataRead": 2976713,
                    "rowCount": 20002,
                    "usageDescription": "",
                    "jobId": 37,
                    "name": "collect at NaiveBayes.scala:194",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:32.167GMT",
                    "completionTime": "2024-12-08T20:57:33.326GMT",
                    "stageIds": [
                      59
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 6,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 6,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 6,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 1772,
                    "rowCount": 6,
                    "usageDescription": "",
                    "jobId": 36,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:31.323GMT",
                    "completionTime": "2024-12-08T20:57:31.362GMT",
                    "stageIds": [
                      57,
                      58
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 7,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 6,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 1772,
                    "dataRead": 66381,
                    "rowCount": 20006,
                    "usageDescription": "",
                    "jobId": 35,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:31.095GMT",
                    "completionTime": "2024-12-08T20:57:31.297GMT",
                    "stageIds": [
                      56
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 6,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 6,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 6,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at IDF.scala:55",
                    "dataWritten": 81946,
                    "dataRead": 3058659,
                    "rowCount": 20012,
                    "usageDescription": "",
                    "jobId": 34,
                    "name": "treeAggregate at IDF.scala:55",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:29.400GMT",
                    "completionTime": "2024-12-08T20:57:30.875GMT",
                    "stageIds": [
                      54,
                      55
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "top at CountVectorizer.scala:236",
                    "dataWritten": 0,
                    "dataRead": 6199560,
                    "rowCount": 69054,
                    "usageDescription": "",
                    "jobId": 33,
                    "name": "top at CountVectorizer.scala:236",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:28.687GMT",
                    "completionTime": "2024-12-08T20:57:28.767GMT",
                    "stageIds": [
                      52,
                      53
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 6,
                    "numSkippedTasks": 6,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 6,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at CountVectorizer.scala:233",
                    "dataWritten": 882205,
                    "dataRead": 3858918,
                    "rowCount": 183614,
                    "usageDescription": "",
                    "jobId": 32,
                    "name": "count at CountVectorizer.scala:233",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:27.703GMT",
                    "completionTime": "2024-12-08T20:57:28.674GMT",
                    "stageIds": [
                      51,
                      50
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 0,
                    "dataRead": 1772,
                    "rowCount": 6,
                    "usageDescription": "",
                    "jobId": 31,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:27.140GMT",
                    "completionTime": "2024-12-08T20:57:27.194GMT",
                    "stageIds": [
                      48,
                      49
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 7,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 6,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at StringIndexer.scala:204",
                    "dataWritten": 1772,
                    "dataRead": 66381,
                    "rowCount": 20006,
                    "usageDescription": "",
                    "jobId": 30,
                    "name": "collect at StringIndexer.scala:204",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:26.905GMT",
                    "completionTime": "2024-12-08T20:57:27.098GMT",
                    "stageIds": [
                      47
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 6,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 6,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 6,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at IDF.scala:55",
                    "dataWritten": 81262,
                    "dataRead": 3057975,
                    "rowCount": 20012,
                    "usageDescription": "",
                    "jobId": 29,
                    "name": "treeAggregate at IDF.scala:55",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:25.122GMT",
                    "completionTime": "2024-12-08T20:57:26.646GMT",
                    "stageIds": [
                      45,
                      46
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "top at CountVectorizer.scala:236",
                    "dataWritten": 0,
                    "dataRead": 6281824,
                    "rowCount": 69054,
                    "usageDescription": "",
                    "jobId": 28,
                    "name": "top at CountVectorizer.scala:236",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:24.259GMT",
                    "completionTime": "2024-12-08T20:57:24.341GMT",
                    "stageIds": [
                      43,
                      44
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 6,
                    "numSkippedTasks": 6,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 6,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at CountVectorizer.scala:233",
                    "dataWritten": 882205,
                    "dataRead": 3858918,
                    "rowCount": 183614,
                    "usageDescription": "",
                    "jobId": 27,
                    "name": "count at CountVectorizer.scala:233",
                    "description": "Job group for statement 12:\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Tokenizer and stop words removal\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# Vectorization and TF-IDF\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n\n# Label encoding\nindexer = StringIndexer(inputCol=\"source\", outputCol=\"label\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, indexer])\n\n# Fit and transform\nprocessed_df = pipeline.fit(combined_df).transform(combined_df)\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, i...",
                    "submissionTime": "2024-12-08T20:57:23.095GMT",
                    "completionTime": "2024-12-08T20:57:24.243GMT",
                    "stageIds": [
                      42,
                      41
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T20:57:22.5360691Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T20:57:22.6916057Z",
              "execution_finish_time": "2024-12-08T20:57:37.1007222Z",
              "parent_msg_id": "6ca284f4-358d-41a7-8809-ee82bc444fa0"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.78\nTest Accuracy: 0.78\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691457224
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Precision (Multiclass)\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "print(f\"Test Precision: {precision:.2f}\")\n",
        "\n",
        "# Weighted Recall (Multiclass)\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "print(f\"Test Recall: {recall:.2f}\")\n",
        "\n",
        "# F1-Score (Multiclass)\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"Test F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "# AUC-ROC (For Binary Classification, works for binary classification)\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc_roc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"Test AUC-ROC: {auc_roc:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 7
                },
                "jobs": [
                  {
                    "displayName": "collect at AreaUnderCurve.scala:44",
                    "dataWritten": 0,
                    "dataRead": 250,
                    "rowCount": 2,
                    "usageDescription": "",
                    "jobId": 47,
                    "name": "collect at AreaUnderCurve.scala:44",
                    "description": "Job group for statement 13:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T20:57:55.278GMT",
                    "completionTime": "2024-12-08T20:57:55.322GMT",
                    "stageIds": [
                      81,
                      82,
                      80
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 14,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 12,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at BinaryClassificationMetrics.scala:237",
                    "dataWritten": 0,
                    "dataRead": 250,
                    "rowCount": 2,
                    "usageDescription": "",
                    "jobId": 46,
                    "name": "collect at BinaryClassificationMetrics.scala:237",
                    "description": "Job group for statement 13:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T20:57:55.189GMT",
                    "completionTime": "2024-12-08T20:57:55.232GMT",
                    "stageIds": [
                      78,
                      79,
                      77
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 14,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 12,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at BinaryClassificationMetrics.scala:197",
                    "dataWritten": 250,
                    "dataRead": 524,
                    "rowCount": 8,
                    "usageDescription": "",
                    "jobId": 45,
                    "name": "count at BinaryClassificationMetrics.scala:197",
                    "description": "Job group for statement 13:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T20:57:55.101GMT",
                    "completionTime": "2024-12-08T20:57:55.171GMT",
                    "stageIds": [
                      74,
                      75,
                      76
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 14,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 6,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "dataWritten": 274,
                    "dataRead": 2976987,
                    "rowCount": 20008,
                    "usageDescription": "",
                    "jobId": 44,
                    "name": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "description": "Job group for statement 13:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T20:57:53.882GMT",
                    "completionTime": "2024-12-08T20:57:55.074GMT",
                    "stageIds": [
                      72,
                      73
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 244,
                    "dataRead": 2976957,
                    "rowCount": 20008,
                    "usageDescription": "",
                    "jobId": 43,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 13:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T20:57:52.579GMT",
                    "completionTime": "2024-12-08T20:57:53.536GMT",
                    "stageIds": [
                      70,
                      71
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 244,
                    "dataRead": 2976957,
                    "rowCount": 20008,
                    "usageDescription": "",
                    "jobId": 42,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 13:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T20:57:51.195GMT",
                    "completionTime": "2024-12-08T20:57:52.314GMT",
                    "stageIds": [
                      68,
                      69
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 244,
                    "dataRead": 2976957,
                    "rowCount": 20008,
                    "usageDescription": "",
                    "jobId": 41,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 13:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T20:57:49.805GMT",
                    "completionTime": "2024-12-08T20:57:50.920GMT",
                    "stageIds": [
                      66,
                      67
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T20:57:49.4393517Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T20:57:49.5723882Z",
              "execution_finish_time": "2024-12-08T20:57:56.2778251Z",
              "parent_msg_id": "bb4d53d3-0019-461e-8e80-20b1841e4825"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Precision: 0.78\nTest Recall: 0.78\nTest F1-Score: 0.78\nTest AUC-ROC: 0.78\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691476368
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix (for Multiclass)\n",
        "# Create confusion matrix by grouping by label and prediction\n",
        "confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().toPandas()\n",
        "\n",
        "# Show the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix)\n",
        "\n",
        "# Optionally, to display it as a matrix format:\n",
        "confusion_matrix_pivot = confusion_matrix.pivot(index=\"label\", columns=\"prediction\", values=\"count\").fillna(0)\n",
        "print(\"\\nConfusion Matrix (Pivoted):\")\n",
        "print(confusion_matrix_pivot)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_7436/2968613908.py:3",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 264,
                    "name": "toPandas at /tmp/ipykernel_7436/2968613908.py:3",
                    "description": "Job group for statement 16:\n# Confusion Matrix (for Multiclass)\n# Create confusion matrix by grouping by label and prediction\nconfusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().toPandas()\n\n# Show the confusion matrix\nprint(\"\nConfusion Matrix:\")\nprint(confusion_matrix)\n\n# Optionally, to display it as a matrix format:\nconfusion_matrix_pivot = confusion_matrix.pivot(index=\"label\", columns=\"prediction\", values=\"count\").fillna(0)\nprint(\"\nConfusion Matrix (Pivoted):\")\nprint(confusion_matrix_pivot)",
                    "submissionTime": "2024-12-08T21:02:06.418GMT",
                    "completionTime": "2024-12-08T21:02:07.117GMT",
                    "stageIds": [
                      514,
                      515
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 7,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 6,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_7436/2968613908.py:3",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 263,
                    "name": "toPandas at /tmp/ipykernel_7436/2968613908.py:3",
                    "description": "Job group for statement 16:\n# Confusion Matrix (for Multiclass)\n# Create confusion matrix by grouping by label and prediction\nconfusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().toPandas()\n\n# Show the confusion matrix\nprint(\"\nConfusion Matrix:\")\nprint(confusion_matrix)\n\n# Optionally, to display it as a matrix format:\nconfusion_matrix_pivot = confusion_matrix.pivot(index=\"label\", columns=\"prediction\", values=\"count\").fillna(0)\nprint(\"\nConfusion Matrix (Pivoted):\")\nprint(confusion_matrix_pivot)",
                    "submissionTime": "2024-12-08T21:02:04.936GMT",
                    "completionTime": "2024-12-08T21:02:06.301GMT",
                    "stageIds": [
                      513
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 6,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 6,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 6,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:01:58.7869329Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:01:59.8644281Z",
              "execution_finish_time": "2024-12-08T21:02:07.920355Z",
              "parent_msg_id": "5cf6fc68-d466-476a-bece-9110691635aa"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nConfusion Matrix:\n   label  prediction  count\n0    1.0         1.0   1434\n1    1.0         0.0    584\n2    0.0         1.0    581\n3    0.0         0.0   1374\n\nConfusion Matrix (Pivoted):\nprediction   0.0   1.0\nlabel                 \n0.0         1374   581\n1.0          584  1434\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691727993
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split the data 80-20\n",
        "train_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Predict\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 107
                },
                "jobs": [
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 262,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:45.363GMT",
                    "completionTime": "2024-12-08T21:01:46.326GMT",
                    "stageIds": [
                      511,
                      512
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 261,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:44.349GMT",
                    "completionTime": "2024-12-08T21:01:44.581GMT",
                    "stageIds": [
                      509,
                      510
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 260,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:44.061GMT",
                    "completionTime": "2024-12-08T21:01:44.271GMT",
                    "stageIds": [
                      507,
                      508
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 259,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:43.738GMT",
                    "completionTime": "2024-12-08T21:01:43.980GMT",
                    "stageIds": [
                      506,
                      505
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 258,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:43.459GMT",
                    "completionTime": "2024-12-08T21:01:43.667GMT",
                    "stageIds": [
                      503,
                      504
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 257,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:43.183GMT",
                    "completionTime": "2024-12-08T21:01:43.398GMT",
                    "stageIds": [
                      502,
                      501
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 279720,
                    "dataRead": 4564096,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 256,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:42.874GMT",
                    "completionTime": "2024-12-08T21:01:43.119GMT",
                    "stageIds": [
                      499,
                      500
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 280436,
                    "dataRead": 4564812,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 255,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:42.499GMT",
                    "completionTime": "2024-12-08T21:01:42.747GMT",
                    "stageIds": [
                      497,
                      498
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 281042,
                    "dataRead": 4565418,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 254,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:42.194GMT",
                    "completionTime": "2024-12-08T21:01:42.435GMT",
                    "stageIds": [
                      495,
                      496
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 281993,
                    "dataRead": 4566369,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 253,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:41.915GMT",
                    "completionTime": "2024-12-08T21:01:42.132GMT",
                    "stageIds": [
                      494,
                      493
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 282523,
                    "dataRead": 4566899,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 252,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:41.641GMT",
                    "completionTime": "2024-12-08T21:01:41.850GMT",
                    "stageIds": [
                      491,
                      492
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 283412,
                    "dataRead": 4567788,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 251,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:41.370GMT",
                    "completionTime": "2024-12-08T21:01:41.576GMT",
                    "stageIds": [
                      489,
                      490
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 283312,
                    "dataRead": 4567688,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 250,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:41.099GMT",
                    "completionTime": "2024-12-08T21:01:41.307GMT",
                    "stageIds": [
                      488,
                      487
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 283730,
                    "dataRead": 4568106,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 249,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:40.801GMT",
                    "completionTime": "2024-12-08T21:01:41.039GMT",
                    "stageIds": [
                      485,
                      486
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 283527,
                    "dataRead": 4567903,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 248,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:40.535GMT",
                    "completionTime": "2024-12-08T21:01:40.739GMT",
                    "stageIds": [
                      484,
                      483
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 285942,
                    "dataRead": 4570318,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 247,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:40.228GMT",
                    "completionTime": "2024-12-08T21:01:40.453GMT",
                    "stageIds": [
                      481,
                      482
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 287558,
                    "dataRead": 4571934,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 246,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:39.948GMT",
                    "completionTime": "2024-12-08T21:01:40.166GMT",
                    "stageIds": [
                      479,
                      480
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 288154,
                    "dataRead": 4572530,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 245,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:39.678GMT",
                    "completionTime": "2024-12-08T21:01:39.878GMT",
                    "stageIds": [
                      477,
                      478
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 289800,
                    "dataRead": 4574176,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 244,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:39.425GMT",
                    "completionTime": "2024-12-08T21:01:39.616GMT",
                    "stageIds": [
                      476,
                      475
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at RDDLossFunction.scala:61",
                    "dataWritten": 291861,
                    "dataRead": 4576237,
                    "rowCount": 17,
                    "usageDescription": "",
                    "jobId": 243,
                    "name": "treeAggregate at RDDLossFunction.scala:61",
                    "description": "Job group for statement 15:\n\n\n# Split the data 80-20\ntrain_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train Logistic Regression model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_data)\n\n# Predict\npredictions = lr_model.transform(test_data)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\n\n",
                    "submissionTime": "2024-12-08T21:01:39.093GMT",
                    "completionTime": "2024-12-08T21:01:39.362GMT",
                    "stageIds": [
                      473,
                      474
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:01:10.8612415Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:01:11.0667026Z",
              "execution_finish_time": "2024-12-08T21:01:48.7127368Z",
              "parent_msg_id": "70592487-4b56-40dc-8fce-1205f9eef8b7"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.71\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691708885
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Precision (Multiclass)\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "print(f\"Test Precision: {precision:.2f}\")\n",
        "\n",
        "# Weighted Recall (Multiclass)\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "print(f\"Test Recall: {recall:.2f}\")\n",
        "\n",
        "# F1-Score (Multiclass)\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"Test F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "# AUC-ROC (For Binary Classification, works for binary classification)\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc_roc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"Test AUC-ROC: {auc_roc:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 17,
              "statement_ids": [
                17
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 7
                },
                "jobs": [
                  {
                    "displayName": "collect at AreaUnderCurve.scala:44",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 271,
                    "name": "collect at AreaUnderCurve.scala:44",
                    "description": "Job group for statement 17:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T21:04:53.934GMT",
                    "completionTime": "2024-12-08T21:04:53.952GMT",
                    "stageIds": [
                      531,
                      532,
                      530
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 14,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 12,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at BinaryClassificationMetrics.scala:237",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 270,
                    "name": "collect at BinaryClassificationMetrics.scala:237",
                    "description": "Job group for statement 17:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T21:04:53.892GMT",
                    "completionTime": "2024-12-08T21:04:53.921GMT",
                    "stageIds": [
                      527,
                      528,
                      529
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 14,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 12,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at BinaryClassificationMetrics.scala:197",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 269,
                    "name": "count at BinaryClassificationMetrics.scala:197",
                    "description": "Job group for statement 17:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T21:04:53.837GMT",
                    "completionTime": "2024-12-08T21:04:53.878GMT",
                    "stageIds": [
                      524,
                      525,
                      526
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 14,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 6,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 268,
                    "name": "sortByKey at BinaryClassificationMetrics.scala:189",
                    "description": "Job group for statement 17:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T21:04:52.950GMT",
                    "completionTime": "2024-12-08T21:04:53.828GMT",
                    "stageIds": [
                      522,
                      523
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 267,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 17:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T21:04:51.710GMT",
                    "completionTime": "2024-12-08T21:04:52.712GMT",
                    "stageIds": [
                      520,
                      521
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 266,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 17:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T21:04:50.557GMT",
                    "completionTime": "2024-12-08T21:04:51.460GMT",
                    "stageIds": [
                      518,
                      519
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collectAsMap at MulticlassMetrics.scala:61",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 265,
                    "name": "collectAsMap at MulticlassMetrics.scala:61",
                    "description": "Job group for statement 17:\n# Weighted Precision (Multiclass)\nevaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nprecision = evaluator_precision.evaluate(predictions)\nprint(f\"Test Precision: {precision:.2f}\")\n\n# Weighted Recall (Multiclass)\nevaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nrecall = evaluator_recall.evaluate(predictions)\nprint(f\"Test Recall: {recall:.2f}\")\n\n# F1-Score (Multiclass)\nevaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nf1_score = evaluator_f1.evaluate(predictions)\nprint(f\"Test F1-Score: {f1_score:.2f}\")\n\n# AUC-ROC (For Binary Classification, works for binary classification)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc_roc = evaluator_auc.evaluate(predictions)\nprint(f\"Test AUC-ROC: {auc_roc:.2f}\")",
                    "submissionTime": "2024-12-08T21:04:49.274GMT",
                    "completionTime": "2024-12-08T21:04:50.301GMT",
                    "stageIds": [
                      517,
                      516
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "44",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:04:48.7966866Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:04:49.0416614Z",
              "execution_finish_time": "2024-12-08T21:04:55.5759406Z",
              "parent_msg_id": "61f2192d-bf52-4b4e-b860-b2b38ad0915d"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 44, 17, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Precision: 0.71\nTest Recall: 0.71\nTest F1-Score: 0.71\nTest AUC-ROC: 0.71\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733691895662
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}