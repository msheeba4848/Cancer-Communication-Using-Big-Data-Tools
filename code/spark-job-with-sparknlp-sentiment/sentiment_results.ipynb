{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Review the Results of the NLP Pretrained Model\n",
        "\n",
        "This model is very basic, but this folder shows you how to create an NLP pipeline with a pretrained model.\n",
        "\n",
        "The .py file saved the results to a parquet that is stored in our container. We are reading it back here to review."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "35",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T00:11:28.836372Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T00:11:28.9550184Z",
              "execution_finish_time": "2024-12-08T00:11:29.2559484Z",
              "parent_msg_id": "18225b25-c6b4-4e8b-b8c8-46c58caaef8f"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 35, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f784a4885b0>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-81e25436:44163\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1.5.2.20240522.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1733616689366
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "# the parquet path again\n",
        "output_path = f\"{workspace_wasbs_base_url}nlp_result_sample_submissions.parquet\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 736376,
                    "rowCount": 6,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"{workspace_wasbs_base_url}nlp_result_sample_submissions.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-08T00:11:42.558GMT",
                    "completionTime": "2024-12-08T00:11:44.834GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"{workspace_wasbs_base_url}nlp_result_sample_submissions.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-08T00:11:35.271GMT",
                    "completionTime": "2024-12-08T00:11:40.023GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "35",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T00:11:34.3288211Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T00:11:34.4712371Z",
              "execution_finish_time": "2024-12-08T00:11:46.6749431Z",
              "parent_msg_id": "1f8e30f0-5f1b-4256-824d-531346e4c2d3"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 35, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|                text|            document|            sentence|               token|               spell|              lemmas|               stems|                 pos|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|Hi, I hope this i...|[{document, 0, 85...|[{document, 0, 23...|[{token, 0, 1, Hi...|[{token, 0, 1, Hi...|[{token, 0, 1, Hi...|[{token, 0, 1, hi...|[{pos, 0, 1, NNP,...|\n|33 male, i have d...|[{document, 0, 27...|[{document, 0, 68...|[{token, 0, 1, 33...|[{token, 0, 1, 33...|[{token, 0, 1, 33...|[{token, 0, 1, 33...|[{pos, 0, 1, CD, ...|\n|           [removed]|[{document, 0, 8,...|[{document, 0, 8,...|[{token, 0, 8, [r...|[{token, 0, 8, [r...|[{token, 0, 8, [r...|[{token, 0, 8, [r...|[{pos, 0, 8, NN, ...|\n|           [removed]|[{document, 0, 8,...|[{document, 0, 8,...|[{token, 0, 8, [r...|[{token, 0, 8, [r...|[{token, 0, 8, [r...|[{token, 0, 8, [r...|[{pos, 0, 8, NN, ...|\n|83F, Cancer dx (u...|[{document, 0, 27...|[{document, 0, 16...|[{token, 0, 2, 83...|[{token, 0, 2, 83...|[{token, 0, 2, 83...|[{token, 0, 2, 83...|[{pos, 0, 2, CD, ...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733616706779
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)\n",
        "df_read_back.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 376055,
                    "rowCount": 6,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 12:\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ndf_read_back.printSchema()",
                    "submissionTime": "2024-12-08T00:13:46.346GMT",
                    "completionTime": "2024-12-08T00:13:46.713GMT",
                    "stageIds": [
                      7
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 12:\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ndf_read_back.printSchema()",
                    "submissionTime": "2024-12-08T00:13:46.000GMT",
                    "completionTime": "2024-12-08T00:13:46.182GMT",
                    "stageIds": [
                      6
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "35",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T00:13:45.700677Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T00:13:45.8097292Z",
              "execution_finish_time": "2024-12-08T00:13:47.3310075Z",
              "parent_msg_id": "dab03efd-7026-40c0-9d43-76b7bf8ec107"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 35, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|                text|            document|            sentence|               token|             checked|           sentiment|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|Hi, I hope this i...|[{document, 0, 85...|[{document, 0, 23...|[{token, 0, 1, Hi...|[{token, 0, 1, Hi...|[{sentiment, 0, 2...|\n|33 male, i have d...|[{document, 0, 27...|[{document, 0, 68...|[{token, 0, 1, 33...|[{token, 0, 1, 33...|[{sentiment, 0, 6...|\n|           [removed]|[{document, 0, 8,...|[{document, 0, 8,...|[{token, 0, 8, [r...|[{token, 0, 8, [r...|[{sentiment, 0, 8...|\n|           [removed]|[{document, 0, 8,...|[{document, 0, 8,...|[{token, 0, 8, [r...|[{token, 0, 8, [r...|[{sentiment, 0, 8...|\n|83F, Cancer dx (u...|[{document, 0, 27...|[{document, 0, 16...|[{token, 0, 2, 83...|[{token, 0, 2, 83...|[{sentiment, 0, 1...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\nroot\n |-- text: string (nullable = true)\n |-- document: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = true)\n |    |    |-- end: integer (nullable = true)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = true)\n |-- sentence: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = true)\n |    |    |-- end: integer (nullable = true)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = true)\n |-- token: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = true)\n |    |    |-- end: integer (nullable = true)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = true)\n |-- checked: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = true)\n |    |    |-- end: integer (nullable = true)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = true)\n |-- sentiment: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = true)\n |    |    |-- end: integer (nullable = true)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = true)\n\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733616827461
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "df_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\"))) \\\n",
        "            .select(\"sentiment_result.result\")\n",
        "\n",
        "# Show the results\n",
        "df_flat.show(truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 27679,
                    "rowCount": 5,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\nfrom pyspark.sql import functions as F\ndf_flat = df_read_back.withColumn(\"sentiment_result\", F.explode(F.col(\"sentiment\")))             .select(\"sentiment_result.result\")\n\n# Show the results\ndf_flat.show(truncate=False)",
                    "submissionTime": "2024-12-08T00:13:50.929GMT",
                    "completionTime": "2024-12-08T00:13:51.140GMT",
                    "stageIds": [
                      8
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "35",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T00:13:50.5925016Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T00:13:50.7250099Z",
              "execution_finish_time": "2024-12-08T00:13:51.5656261Z",
              "parent_msg_id": "cb5e44fc-7109-4075-8bc3-26a4d3f98686"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 35, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------+\n|result  |\n+--------+\n|negative|\n|positive|\n|negative|\n|negative|\n|positive|\n|positive|\n|positive|\n|negative|\n|positive|\n|negative|\n|negative|\n|positive|\n|negative|\n|positive|\n|negative|\n|negative|\n|na      |\n|na      |\n|negative|\n|negative|\n+--------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733616831672
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = f\"{workspace_wasbs_base_url}nlp_sentiment_results_sample_submissions.parquet\"\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 14,
              "statement_ids": [
                14
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 45287,
                    "rowCount": 6,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 14:\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_results_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-08T00:13:55.462GMT",
                    "completionTime": "2024-12-08T00:13:55.724GMT",
                    "stageIds": [
                      10
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 14:\noutput_path = f\"{workspace_wasbs_base_url}nlp_sentiment_results_sample_submissions.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-08T00:13:55.139GMT",
                    "completionTime": "2024-12-08T00:13:55.344GMT",
                    "stageIds": [
                      9
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "35",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T00:13:54.8626912Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T00:13:54.9746985Z",
              "execution_finish_time": "2024-12-08T00:13:56.5106087Z",
              "parent_msg_id": "217a197a-9cf9-4c3d-8f01-097cf804402a"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 35, 14, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+--------------------+\n|                text|              result|\n+--------------------+--------------------+\n|Hi, I hope this i...|[negative, positi...|\n|33 male, i have d...|[negative, positi...|\n|           [removed]|                [na]|\n|           [removed]|                [na]|\n|83F, Cancer dx (u...|[negative, negati...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733616836614
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_read_back.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "35",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T00:14:02.9181823Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T00:14:04.8940827Z",
              "execution_finish_time": "2024-12-08T00:14:05.1900635Z",
              "parent_msg_id": "625f2ae1-91da-47e1-96ce-834f403b4a32"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 35, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root\n |-- text: string (nullable = true)\n |-- result: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733616845321
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_read_back['result'])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "35",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T00:14:08.2373921Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T00:14:08.3607157Z",
              "execution_finish_time": "2024-12-08T00:14:08.6495926Z",
              "parent_msg_id": "eaee2a01-f6c7-492a-9d39-5c2e762b3a3b"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 35, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Column<'result'>\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733616848756
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}