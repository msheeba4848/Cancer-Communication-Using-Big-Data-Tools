{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "47",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:33:11.4444823Z",
              "session_start_time": "2024-12-08T21:33:11.4743222Z",
              "execution_start_time": "2024-12-08T21:36:00.2744042Z",
              "execution_finish_time": "2024-12-08T21:36:00.6338309Z",
              "parent_msg_id": "d831ceac-0ef5-4e7e-8b30-ad401efbd277"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 47, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f10926f85b0>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-71e54967:38259\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1.5.2.20240522.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733693760815
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 22,
              "statement_ids": [
                22
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "47",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:43:51.2870929Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:43:51.406484Z",
              "execution_finish_time": "2024-12-08T21:43:51.7021193Z",
              "parent_msg_id": "86c44a15-d1a3-4731-ab05-27764c9ac315"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 47, 22, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733694231816
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "import pandas\n",
        "\n",
        "cancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\n",
        "cancer_df = spark.read.parquet(cancer_path)\n",
        "\n",
        "not_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\n",
        "not_cancer = spark.read.parquet(not_cancer_path)\n",
        "\n",
        "cancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n",
        "\n",
        "not_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n",
        "\n",
        "df = cancer_df.union(not_cancer)\n",
        "df = df.select('text', 'source')\n",
        "\n",
        "df = df.toPandas()\n",
        "\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 34,
              "statement_ids": [
                34
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 3
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_9229/3287297446.py:17",
                    "dataWritten": 0,
                    "dataRead": 2976713,
                    "rowCount": 20000,
                    "usageDescription": "",
                    "jobId": 15,
                    "name": "toPandas at /tmp/ipykernel_9229/3287297446.py:17",
                    "description": "Job group for statement 34:\nfrom pyspark.sql.functions import lit\nimport pandas\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ndf = cancer_df.union(not_cancer)\ndf = df.select('text', 'source')\n\ndf = df.toPandas()\n\ndf.head()",
                    "submissionTime": "2024-12-08T21:50:54.331GMT",
                    "completionTime": "2024-12-08T21:50:54.727GMT",
                    "stageIds": [
                      15
                    ],
                    "jobGroup": "34",
                    "status": "SUCCEEDED",
                    "numTasks": 6,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 6,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 6,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 14,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 34:\nfrom pyspark.sql.functions import lit\nimport pandas\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ndf = cancer_df.union(not_cancer)\ndf = df.select('text', 'source')\n\ndf = df.toPandas()\n\ndf.head()",
                    "submissionTime": "2024-12-08T21:50:53.900GMT",
                    "completionTime": "2024-12-08T21:50:54.081GMT",
                    "stageIds": [
                      14
                    ],
                    "jobGroup": "34",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 34:\nfrom pyspark.sql.functions import lit\nimport pandas\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ndf = cancer_df.union(not_cancer)\ndf = df.select('text', 'source')\n\ndf = df.toPandas()\n\ndf.head()",
                    "submissionTime": "2024-12-08T21:50:53.644GMT",
                    "completionTime": "2024-12-08T21:50:53.723GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "34",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "47",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:50:53.3056588Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:50:53.4735041Z",
              "execution_finish_time": "2024-12-08T21:50:54.982432Z",
              "parent_msg_id": "b6e21510-4f64-4b04-a800-cbd64c288650"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 47, 34, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "                                                text  source\n0  Check out Northside Hospital -Atlanta, Piedmon...  cancer\n1  I had something similar and managed to get it ...  cancer\n2  That's an insulting offer, and I'm a LPN. Out ...  cancer\n3  Yeah sorry, it was my mistake. I meant to say ...  cancer\n4  I see my colorectal dr tomorrow. My disease is...  cancer",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Check out Northside Hospital -Atlanta, Piedmon...</td>\n      <td>cancer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I had something similar and managed to get it ...</td>\n      <td>cancer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>That's an insulting offer, and I'm a LPN. Out ...</td>\n      <td>cancer</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Yeah sorry, it was my mistake. I meant to say ...</td>\n      <td>cancer</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I see my colorectal dr tomorrow. My disease is...</td>\n      <td>cancer</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1733694655164
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['text']]\n",
        "df = df.rename(columns={'text': 'text_column'})\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 35,
              "statement_ids": [
                35
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "47",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:51:16.5517366Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:51:16.6574619Z",
              "execution_finish_time": "2024-12-08T21:51:16.9497423Z",
              "parent_msg_id": "b2225914-3fb3-4427-a892-c46aca0410c3"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 47, 35, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/plain": "                                         text_column\n0  Check out Northside Hospital -Atlanta, Piedmon...\n1  I had something similar and managed to get it ...\n2  That's an insulting offer, and I'm a LPN. Out ...\n3  Yeah sorry, it was my mistake. I meant to say ...\n4  I see my colorectal dr tomorrow. My disease is...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_column</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Check out Northside Hospital -Atlanta, Piedmon...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I had something similar and managed to get it ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>That's an insulting offer, and I'm a LPN. Out ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Yeah sorry, it was my mistake. I meant to say ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I see my colorectal dr tomorrow. My disease is...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733694677068
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nrclex"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 11,
              "statement_ids": [
                7,
                8,
                9,
                10,
                11
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "47",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:38:20.8676013Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:38:20.967827Z",
              "execution_finish_time": "2024-12-08T21:38:54.9142918Z",
              "parent_msg_id": "77b5e976-76cb-4e26-b87a-0356ca6911ff"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 47, 11, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting nrclex\n  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 KB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: textblob in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nrclex) (0.15.3)\nRequirement already satisfied: nltk>=3.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from textblob->nrclex) (3.6.2)\nRequirement already satisfied: regex in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (2022.10.31)\nRequirement already satisfied: joblib in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (1.2.0)\nRequirement already satisfied: tqdm in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (4.64.1)\nRequirement already satisfied: click in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (8.1.3)\nBuilding wheels for collected packages: nrclex\n  Building wheel for nrclex (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Created wheel for nrclex: filename=NRCLex-3.0.0-py3-none-any.whl size=43311 sha256=f80b0ee2347f052e40d5aa44283af8fd9d769e77ef51b76ef1c8f06503ee582a\n  Stored in directory: /home/trusted-service-user/.cache/pip/wheels/d2/10/44/6abfb1234298806a145fd6bcaec8cbc712e88dd1cd6cb242fa\nSuccessfully built nrclex\nInstalling collected packages: nrclex\nSuccessfully installed nrclex-3.0.0\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\nYou should consider upgrading via the '/nfs4/pyenv-402ac994-ebe3-47c4-9480-be3453578112/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Warning: PySpark kernel has been restarted to use updated packages.\n\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nrclex\n",
        "import nltk\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Download NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample dataframe with a text column\n",
        "#data = {'text_column': [\"I love coding!\", \"I hate bugs in my code.\", \"This is a neutral statement.\", \n",
        "#                        \"I enjoy solving problems!\", \"I get frustrated when things don't work.\"]}\n",
        "#df = pd.DataFrame(data)\n",
        "\n",
        "# Define a function to get sentiment from NRC lexicon\n",
        "def get_sentiment(text):\n",
        "    if isinstance(text, str):\n",
        "        # Initialize NRC lexicon for each text\n",
        "        nrc_lex = nrclex.NRCLex(text)\n",
        "        affect_frequencies = nrc_lex.affect_frequencies\n",
        "        \n",
        "        # Extracting positive, negative, neutral, and other emotions including 'trust'\n",
        "        sentiment = {\n",
        "            \"positive\": affect_frequencies.get(\"positive\", 0),\n",
        "            \"negative\": affect_frequencies.get(\"negative\", 0),\n",
        "            \"neutral\": affect_frequencies.get(\"neutral\", 0),\n",
        "            \"anger\": affect_frequencies.get(\"anger\", 0),\n",
        "            \"fear\": affect_frequencies.get(\"fear\", 0),\n",
        "            \"disgust\": affect_frequencies.get(\"disgust\", 0),\n",
        "            \"sadness\": affect_frequencies.get(\"sadness\", 0),\n",
        "            \"joy\": affect_frequencies.get(\"joy\", 0),\n",
        "            \"surprise\": affect_frequencies.get(\"surprise\", 0),\n",
        "            \"trust\": affect_frequencies.get(\"trust\", 0)  # Added trust emotion\n",
        "        }\n",
        "        return sentiment\n",
        "    return {\n",
        "        \"positive\": 0, \"negative\": 0, \"neutral\": 0, \"anger\": 0, \"fear\": 0, \n",
        "        \"disgust\": 0, \"sadness\": 0, \"joy\": 0, \"surprise\": 0, \"trust\": 0  # Default 0 for all emotions\n",
        "    }\n",
        "\n",
        "# Function to process a chunk of data\n",
        "def process_chunk(chunk):\n",
        "    chunk[['positive', 'negative', 'neutral', 'anger', 'fear', 'disgust', 'sadness', 'joy', 'surprise', 'trust']] = \\\n",
        "        chunk['text_column'].apply(get_sentiment).apply(pd.Series)\n",
        "    return chunk\n",
        "\n",
        "# Function to split dataframe into chunks\n",
        "def split_dataframe(df, num_chunks):\n",
        "    chunk_size = len(df) // num_chunks\n",
        "    return [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "# Number of chunks for parallel processing (adjust based on your system's cores)\n",
        "num_chunks = 4  # or a value that suits your setup\n",
        "\n",
        "# Step 1: Split the DataFrame into chunks\n",
        "chunks = split_dataframe(df, num_chunks)\n",
        "\n",
        "# Step 2: Use multiprocessing to process the chunks in parallel\n",
        "with Pool(processes=num_chunks) as pool:\n",
        "    result_chunks = pool.map(process_chunk, chunks)\n",
        "\n",
        "# Step 3: Combine the processed chunks back into a single DataFrame\n",
        "final_df = pd.concat(result_chunks, ignore_index=True)\n",
        "\n",
        "# Display the final dataframe with sentiment columns\n",
        "print(final_df)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 36,
              "statement_ids": [
                36
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "47",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:51:29.114391Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:51:29.2389609Z",
              "execution_finish_time": "2024-12-08T21:51:45.7619608Z",
              "parent_msg_id": "2628649e-8cb0-46b6-8857-b520f8e2aace"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 47, 36, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /home/trusted-service-\n[nltk_data]     user/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "                                             text_column  positive  negative  \\\n0      Check out Northside Hospital -Atlanta, Piedmon...  0.198020  0.089109   \n1      I had something similar and managed to get it ...  0.666667  0.000000   \n2      That's an insulting offer, and I'm a LPN. Out ...  0.275862  0.103448   \n3      Yeah sorry, it was my mistake. I meant to say ...  0.000000  0.500000   \n4      I see my colorectal dr tomorrow. My disease is...  0.000000  0.166667   \n...                                                  ...       ...       ...   \n19995  Switch update was just a normal firmware updat...  0.000000  0.000000   \n19996                                          [removed]  0.000000  0.000000   \n19997  When will someone just fucking follow glorious...  0.000000  0.166667   \n19998                                  Ew, STFU already.  0.000000  0.000000   \n19999  Szemely szerint csak bemutatkoznek neki udvari...  0.000000  0.000000   \n\n       neutral     anger      fear   disgust   sadness       joy  surprise  \\\n0          0.0  0.049505  0.079208  0.039604  0.059406  0.128713  0.059406   \n1          0.0  0.000000  0.333333  0.000000  0.000000  0.000000  0.000000   \n2          0.0  0.057471  0.057471  0.045977  0.068966  0.080460  0.068966   \n3          0.0  0.000000  0.000000  0.000000  0.500000  0.000000  0.000000   \n4          0.0  0.166667  0.166667  0.166667  0.166667  0.000000  0.000000   \n...        ...       ...       ...       ...       ...       ...       ...   \n19995      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n19996      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n19997      0.0  0.166667  0.166667  0.000000  0.166667  0.000000  0.166667   \n19998      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n19999      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n          trust  \n0      0.138614  \n1      0.000000  \n2      0.149425  \n3      0.000000  \n4      0.000000  \n...         ...  \n19995  0.000000  \n19996  0.000000  \n19997  0.000000  \n19998  0.000000  \n19999  0.000000  \n\n[20000 rows x 11 columns]\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733694705871
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}