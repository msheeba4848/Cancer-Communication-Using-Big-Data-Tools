{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "\n",
        "## You need to add the spark-nlp jar to the spark session\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.1\") \\\n",
        "    .getOrCreate()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "46",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:20:35.324856Z",
              "session_start_time": "2024-12-08T21:20:35.3681907Z",
              "execution_start_time": "2024-12-08T21:21:35.457824Z",
              "execution_finish_time": "2024-12-08T21:21:35.8163753Z",
              "parent_msg_id": "4141ab43-ffec-4502-b5b7-0964fb4630a6"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 46, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733692895924
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "cancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\n",
        "cancer_df = spark.read.parquet(cancer_path)\n",
        "\n",
        "not_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\n",
        "not_cancer = spark.read.parquet(not_cancer_path)\n",
        "\n",
        "cancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n",
        "\n",
        "not_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n",
        "\n",
        "df = cancer_df.union(not_cancer)\n",
        "df = df.select('text', 'source')\n",
        "\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 4
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2987586,
                    "rowCount": 8192,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 7:\nfrom pyspark.sql.functions import lit\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ndf = cancer_df.union(not_cancer)\ndf = df.select('text', 'source')\n\ndf.show()",
                    "submissionTime": "2024-12-08T21:21:54.775GMT",
                    "completionTime": "2024-12-08T21:21:56.414GMT",
                    "stageIds": [
                      3
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 4,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 4,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 4,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 22508,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 7:\nfrom pyspark.sql.functions import lit\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ndf = cancer_df.union(not_cancer)\ndf = df.select('text', 'source')\n\ndf.show()",
                    "submissionTime": "2024-12-08T21:21:53.776GMT",
                    "completionTime": "2024-12-08T21:21:54.750GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 7:\nfrom pyspark.sql.functions import lit\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ndf = cancer_df.union(not_cancer)\ndf = df.select('text', 'source')\n\ndf.show()",
                    "submissionTime": "2024-12-08T21:21:49.224GMT",
                    "completionTime": "2024-12-08T21:21:51.998GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 7:\nfrom pyspark.sql.functions import lit\n\ncancer_path = f\"{workspace_wasbs_base_url}cancer_subreddit_sentiment.parquet\"\ncancer_df = spark.read.parquet(cancer_path)\n\nnot_cancer_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit_sentiment.parquet\"\nnot_cancer = spark.read.parquet(not_cancer_path)\n\ncancer_df = cancer_df.withColumn(\"source\", lit(\"cancer\"))\n\nnot_cancer = not_cancer.withColumn(\"source\", lit(\"non_cancer\"))\n\ndf = cancer_df.union(not_cancer)\ndf = df.select('text', 'source')\n\ndf.show()",
                    "submissionTime": "2024-12-08T21:21:43.838GMT",
                    "completionTime": "2024-12-08T21:21:47.494GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "46",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:21:42.8453263Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:21:42.975555Z",
              "execution_finish_time": "2024-12-08T21:21:57.7058986Z",
              "parent_msg_id": "ad7c7b6a-7b31-4e23-8bfd-4399cbdec8c0"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 46, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+------+\n|                text|source|\n+--------------------+------+\n|Check out Northsi...|cancer|\n|I had something s...|cancer|\n|That's an insulti...|cancer|\n|Yeah sorry, it wa...|cancer|\n|I see my colorect...|cancer|\n|The couple of ran...|cancer|\n|I’ve encountered ...|cancer|\n|I 100% agree with...|cancer|\n|You should not ha...|cancer|\n|**Your post has b...|cancer|\n|Completely agree ...|cancer|\n|Butt's have oil s...|cancer|\n|I just found I ha...|cancer|\n|in the mid 2000s ...|cancer|\n|If you know all y...|cancer|\n|Ultimately, us nu...|cancer|\n|I had one the siz...|cancer|\n|yeah I see that n...|cancer|\n|This has been a r...|cancer|\n|Day to day is dif...|cancer|\n+--------------------+------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1733692917830
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nrclex"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 12,
              "statement_ids": [
                8,
                9,
                10,
                11,
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "46",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:22:00.8660658Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:22:00.9737004Z",
              "execution_finish_time": "2024-12-08T21:22:27.6331216Z",
              "parent_msg_id": "c3c07a90-ea2c-43e4-8a6d-33decb3541e9"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 46, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting nrclex\n  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\nRequirement already satisfied: textblob in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nrclex) (0.15.3)\n  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from textblob->nrclex) (3.6.2)\nRequirement already satisfied: regex in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (2022.10.31)\nRequirement already satisfied: joblib in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (1.2.0)\nRequirement already satisfied: click in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (8.1.3)\nRequirement already satisfied: tqdm in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (4.64.1)\nBuilding wheels for collected packages: nrclex\n  Building wheel for nrclex (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Created wheel for nrclex: filename=NRCLex-3.0.0-py3-none-any.whl size=43311 sha256=7fc78cec826b6b074a89b75b73dbb1c40bc0608836d8729c5daadcfae654b51f\n  Stored in directory: /home/trusted-service-user/.cache/pip/wheels/d2/10/44/6abfb1234298806a145fd6bcaec8cbc712e88dd1cd6cb242fa\nSuccessfully built nrclex\nInstalling collected packages: nrclex\nSuccessfully installed nrclex-3.0.0\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\nYou should consider upgrading via the '/nfs4/pyenv-07a6af9c-1663-4ec1-9220-bf0c45c7f436/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Warning: PySpark kernel has been restarted to use updated packages.\n\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nrclex\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Download NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize NRC lexicon\n",
        "nrc_lex = nrclex.NRCLex()\n",
        "\n",
        "# Define a function to get sentiment from NRC lexicon\n",
        "def get_sentiment(text):\n",
        "    if isinstance(text, str):\n",
        "        nrc_lex.affect_frequencies = nrc_lex.analyze(text)\n",
        "        # Extracting positive, negative, and neutral sentiment\n",
        "        sentiment = {\n",
        "            \"positive\": nrc_lex.affect_frequencies.get(\"positive\", 0),\n",
        "            \"negative\": nrc_lex.affect_frequencies.get(\"negative\", 0),\n",
        "            \"neutral\": nrc_lex.affect_frequencies.get(\"neutral\", 0)\n",
        "        }\n",
        "        return sentiment\n",
        "    return {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
        "\n",
        "# Function to apply NRC lexicon sentiment analysis on a DataFrame chunk\n",
        "def process_chunk(chunk):\n",
        "    chunk[['positive', 'negative', 'neutral']] = chunk['text_column'].apply(get_sentiment).apply(pd.Series)\n",
        "    return chunk\n",
        "\n",
        "# Step 1: Load the Parquet file into a Pandas DataFrame\n",
        "#parquet_path = 'your_parquet_file.parquet'\n",
        "#df = pd.read_parquet(parquet_path)\n",
        "\n",
        "# Step 2: Split the DataFrame into chunks (for example, split into 4 chunks)\n",
        "num_chunks = 4  # Number of chunks to split the DataFrame\n",
        "chunk_size = len(df) // num_chunks\n",
        "chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "# Step 3: Set up the multiprocessing pool and apply the function to each chunk\n",
        "with Pool(processes=num_chunks) as pool:\n",
        "    result_chunks = pool.map(process_chunk, chunks)\n",
        "\n",
        "# Step 4: Combine the processed chunks back into a single DataFrame\n",
        "final_df = pd.concat(result_chunks, ignore_index=True)\n",
        "\n",
        "# Step 5: Save the updated DataFrame to a new Parquet file\n",
        "final_df.to_parquet('sentiment_analysis_parallel_output.parquet')\n",
        "\n",
        "# Optional: Print the first few rows to check the results\n",
        "print(final_df.head())\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 18,
              "statement_ids": [
                18
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "46",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T21:22:25.036772Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T21:22:37.2109194Z",
              "execution_finish_time": "2024-12-08T21:22:43.5376912Z",
              "parent_msg_id": "954937a3-0386-4f5e-8f9a-1180cf37749c"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 46, 18, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /home/trusted-service-\n[nltk_data]     user/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "NRCLex.__init__() missing 1 required positional argument: 'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize NRC lexicon\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m nrc_lex \u001b[38;5;241m=\u001b[39m \u001b[43mnrclex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNRCLex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Define a function to get sentiment from NRC lexicon\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentiment\u001b[39m(text):\n",
            "\u001b[0;31mTypeError\u001b[0m: NRCLex.__init__() missing 1 required positional argument: 'text'"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733692963679
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}