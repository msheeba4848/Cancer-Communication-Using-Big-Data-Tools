[
  {
    "objectID": "eda_reddit.html",
    "href": "eda_reddit.html",
    "title": "Cancer & Reddit",
    "section": "",
    "text": "|"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using Reddit Pushshift to Enrich Understanding of Cancer Communication",
    "section": "",
    "text": "Project Authors: Tiana Le, Sheeba Moghal, Ishaan Babbar, Liz Kovalchuk\nIn the digital age, where information is readily available at our fingertips, understanding how people navigate and trust health information is critical. With countless sources such as healthcare professionals, government agencies, social media, and online forums, it is essential to explore what the United States population believes about the reliability and accessibility of health information. To better understand this phenomenon, data was gathered from an online forum called Reddit. The Reddit dataset comprises textual data that offers valuable insights into the information shared online about health, particularly topics related to cancer. Using NLP methods, we can explore the most common topics and also the sentiment toward these topics. In addition to the textual data, we will also use the Health Information National Trends Survey (HINTS), which is administered by National Cancer Institute. In this survey, people were asked directly about what they think about health information. Using both the Reddit data and HINTS, we seek to understand trust, frustrations, and overall sentiment about healthcare and more specifically, cancer information.\nOur project explores the Pushshift Reddit Dataset to examine trust in healthcare, likely narrowing specifically to r/cancer forums. While there are some assumptions that need to be made to pair our corroborating dataset, the Health Information National Trends Survey (HINTS) survey, with the Reddit dataset, we believe that we compare and use both of these datasets to investigate the following questions:\n\nIs public sentiment toward healthcare and cancer generally positive, or negative, in an online social platform?\nIs there evidence of trust (or lack thereof) when the public discusses healthcare?\nEvidence of other interpersonal conflict in healthcare discussions and emotion / sentiment analysis (e.g. sadness, disgust, surprise, joy, anger).\n\n\n\n\n\n\nThe Health Information National Trends Survey (HINTS) is a significant research initiative launched by the National Cancer Institute (NCI) in 2001. It aims to assess how adults in the United States access and utilize health information with a particular focus on cancer-related topics. HINTS provides a rich source of data that helps researchers understand public interactions with health information, including trends in health communication, knowledge, attitudes, and behaviors regarding cancer prevention and control.\nIt is an excellent starting place for structured questionnaires that would be helfpul to enhance in a “more accessible” format of Reddit data.\nHINTS employs a cross-sectional, nationally representative survey methodology targeting non-institutionalized adults aged 18 years and older. This design allows for the collection of data that reflects the diverse experiences and perspectives of the U.S. population13. Annual Administration: Initially conducted biennially, HINTS has transitioned to an annual survey format. This change enhances the ability to track evolving trends in health information access and utilization over time.\nThe survey seeks information on:\n\nHealth information-seeking behaviors\nCancer prevention and screening practices\nKnowledge and perceptions related to cancer risks\nAccess to healthcare services\nUtilization of technology for health information37.\nPublicly Available Data: HINTS data is freely accessible through its website, allowing researchers, policymakers, and public health professionals to utilize this information for various applications, including the design and evaluation of health communication programs9.\n\nThough HINTS was initially conducted biennially, HINTS has transitioned to an annual survey format. This change enhances the ability to track evolving trends in health information access and utilization over time.\n\n\n\nThe Pushshift Reddit dataset is a comprehensive collection of submissions and comments from Reddit, designed to facilitate social media research by providing easy access to historical data. Launched in 2015, Pushshift has become a crucial resource for researchers interested in analyzing user-generated content on one of the largest social media platforms.\nThe data leveraged from the Reddit dataset was from the provided repository of pre-processed data:\n\nSpans the June-2023 to July-2024 in the 202306-202407 directory\nSpans Jan-2021 to March-2023 in the 202101-202303 directory\n\nThe data queried is of both submissions and comments from the PushShift Dataset.\n\n\n\n\n\nFinney Rutten, L. J., Arora, N. K., Bakken, S., & Hesse, B. W. (2022). Expanding the Health Information National Trends Survey research: A comparison of health information seeking behaviors across countries. Health Communication, 37(1), 1-10. https://doi.org/10.1080/10810730.2022.2134522\nNational Cancer Institute. (2021). Health Information National Trends Survey (HINTS). Retrieved from https://hints.cancer.gov\nKreps, G. L., & Neuhauser, L. (2020). The health information national trends survey: Research and implications for health communication. PubMed. https://pubmed.ncbi.nlm.nih.gov/33970822/\nZannettou, S., Caulfield, T., & De Cristofaro, E. (2020). The Pushshift Reddit dataset. arXiv. https://doi.org/10.48550/arXiv.2001.08435\nPushshift. (n.d.). Pushshift Reddit dataset. In Papers with Code. Retrieved from https://paperswithcode.com/dataset/pushshift-reddit"
  },
  {
    "objectID": "index.html#the-health-information-national-trends-survey-hints",
    "href": "index.html#the-health-information-national-trends-survey-hints",
    "title": "Introduction",
    "section": "The Health Information National Trends Survey (HINTS)",
    "text": "The Health Information National Trends Survey (HINTS)\nThe Health Information National Trends Survey (HINTS) is a significant research initiative launched by the National Cancer Institute (NCI) in 2001. It aims to assess how adults in the United States access and utilize health information with a particular focus on cancer-related topics. HINTS provides a rich source of data that helps researchers understand public interactions with health information, including trends in health communication, knowledge, attitudes, and behaviors regarding cancer prevention and control.\nIt is an excellent starting place for structured questionnaires that would be helfpul to enhance in a “more accessible” format of Reddit data.\nHINTS employs a cross-sectional, nationally representative survey methodology targeting non-institutionalized adults aged 18 years and older. This design allows for the collection of data that reflects the diverse experiences and perspectives of the U.S. population13. Annual Administration: Initially conducted biennially, HINTS has transitioned to an annual survey format. This change enhances the ability to track evolving trends in health information access and utilization over time.\nThe survey seeks information on:\n\nHealth information-seeking behaviors\nCancer prevention and screening practices\nKnowledge and perceptions related to cancer risks\nAccess to healthcare services\nUtilization of technology for health information37.\nPublicly Available Data: HINTS data is freely accessible through its website, allowing researchers, policymakers, and public health professionals to utilize this information for various applications, including the design and evaluation of health communication programs9.\n\nThough HINTS was initially conducted biennially, HINTS has transitioned to an annual survey format. This change enhances the ability to track evolving trends in health information access and utilization over time."
  },
  {
    "objectID": "index.html#the-reddit-pushshift-data-set",
    "href": "index.html#the-reddit-pushshift-data-set",
    "title": "Introduction",
    "section": "The Reddit PushShift Data Set",
    "text": "The Reddit PushShift Data Set\nThe Pushshift Reddit dataset is a comprehensive collection of submissions and comments from Reddit, designed to facilitate social media research by providing easy access to historical data. Launched in 2015, Pushshift has become a crucial resource for researchers interested in analyzing user-generated content on one of the largest social media platforms.\nThe data leveraged from the Reddit dataset was from the provided repository of pre-processed data:\n\nSpans the June-2023 to July-2024 in the 202306-202407 directory\nSpans Jan-2021 to March-2023 in the 202101-202303 directory\n\nThe data queried is of both submissions and comments from the PushShift Dataset."
  },
  {
    "objectID": "hints_regression.html",
    "href": "hints_regression.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Code\nfrom IPython.display import HTML\n\nd3_script = HTML(\"\"\"\n&lt;script src=\"https://d3js.org/d3.v7.min.js\"&gt;&lt;/script&gt;\n\"\"\")\n\ndisplay(d3_script)\nCode\n# Libraries \nimport pyreadr\nimport pandas as pd\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom itertools import product\nimport numpy as np"
  },
  {
    "objectID": "hints_regression.html#hints-prepartion-of-data",
    "href": "hints_regression.html#hints-prepartion-of-data",
    "title": "Machine Learning",
    "section": "HINTS Prepartion of Data",
    "text": "HINTS Prepartion of Data\n\n\nCode\n# Read in data\nfile_path = '../data/csv/hints_cleaned_forML_spearman.csv'\nhints_cleaned = pd.read_csv(file_path)\n\n# Display the first few rows in a neat table format\ndisplay(hints_cleaned.head())\n\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n21000330\nYes\nSomewhat disagree\nSome\nNot at all\nSome\nSome\nNot at all\nA lot\nYes\nA lot\nA little\n\n\n1\n21000976\nYes\nSomewhat agree\nA lot\nSome\nSome\nSome\nSome\nA lot\nYes\nSome\nA little\n\n\n2\n21001112\nYes\nSomewhat disagree\nA little\nA little\nNot at all\nNot at all\nNot at all\nA little\nNo\nA lot\nNot at all\n\n\n3\n21001283\nYes\nSomewhat disagree\nA lot\nSome\nNot at all\nA little\nSome\nNot at all\nNo\nI do not use social media\nNot at all\n\n\n4\n21001548\nYes\nStrongly agree\nA lot\nSome\nNot at all\nSome\nA lot\nA little\nYes\nSome\nA little\n\n\n\n\n\n\n\n\n\n\nCode\n# Count the number of NA or NaN values in each column\nna_count = hints_cleaned.isna().sum()\nprint(\"NA values count per column:\")\nprint(na_count)\nprint(hints_cleaned.shape)\n\n\nNA values count per column:\nHHID                        0\nSeekCancerInfo              0\nCancerFrustrated            0\nCancerTrustDoctor           0\nCancerTrustFamily           0\nCancerTrustGov              0\nCancerTrustCharities        0\nCancerTrustReligiousOrgs    0\nCancerTrustScientists       0\nElectronic2_HealthInfo      0\nMisleadingHealthInfo        3\nTrustHCSystem               0\ndtype: int64\n(323, 12)\n\n\n\n\nCode\nhints_cleaned_cleaned = hints_cleaned.dropna()\n\n\n\n\nCode\n# Define the mappings\ntrust_mapping = {\n    \"Not at all\": 4,\n    \"A little\": 3,\n    \"Some\": 2,\n    \"A lot\": 1\n}\n\nagreement_mapping = {\n    \"Strongly agree\": 1,\n    \"Somewhat agree\": 2,\n    \"Somewhat disagree\": 3,\n    \"Strongly disagree\": 4\n}\n\nbinary_mapping = {\n    \"Yes\": 1,\n    \"No\": 2\n}\n\nmisleading_info_mapping = {\n    \"I do not use social media\": 5,\n    \"None\": 4,\n    \"A little\": 3,\n    \"Some\": 2,\n    \"A lot\": 1\n}\n\n# Apply the mappings to the respective columns\nmapped_columns = {\n    \"CancerFrustrated\": agreement_mapping,\n    \"CancerTrustDoctor\": trust_mapping,\n    \"CancerTrustFamily\": trust_mapping,\n    \"CancerTrustGov\": trust_mapping,\n    \"CancerTrustCharities\": trust_mapping,\n    \"CancerTrustReligiousOrgs\": trust_mapping,\n    \"CancerTrustScientists\": trust_mapping,\n    \"TrustHCSystem\": trust_mapping,\n    \"Electronic2_HealthInfo\": binary_mapping,\n    \"MisleadingHealthInfo\": misleading_info_mapping,\n    \"SeekCancerInfo\": binary_mapping \n}\n\n# Apply mappings to the filtered DataFrame\nfor column, mapping in mapped_columns.items():\n    hints_cleaned[column] = hints_cleaned[column].map(mapping)\n    \n# Drop rows where 'MisleadingHealthInfo' is NaN\nhints_cleaned = hints_cleaned.dropna(subset=['MisleadingHealthInfo'])\n\n# Convert 'MisleadingHealthInfo' to integer type\nhints_cleaned['MisleadingHealthInfo'] = hints_cleaned['MisleadingHealthInfo'].astype(int)\n\n# # Display the updated DataFrame and its data type\n# print(hints_cleaned.head())\n# print(hints_cleaned['MisleadingHealthInfo'].dtype)\n\n\n# Display the transformed dataset\nprint(\"Data after applying mappings to numeric values:\")\nhints_cleaned = hints_cleaned.drop(columns=['HHID']) # Drop the 'HHID' column\ndisplay(hints_cleaned.head())\nprint(hints_cleaned.dtypes)\n\n\nData after applying mappings to numeric values:\nSeekCancerInfo              int64\nCancerFrustrated            int64\nCancerTrustDoctor           int64\nCancerTrustFamily           int64\nCancerTrustGov              int64\nCancerTrustCharities        int64\nCancerTrustReligiousOrgs    int64\nCancerTrustScientists       int64\nElectronic2_HealthInfo      int64\nMisleadingHealthInfo        int64\nTrustHCSystem               int64\ndtype: object\n\n\n\n\n\n\n\n\n\n\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n1\n3\n2\n4\n2\n2\n4\n1\n1\n1\n3\n\n\n1\n1\n2\n1\n2\n2\n2\n2\n1\n1\n2\n3\n\n\n2\n1\n3\n3\n3\n4\n4\n4\n3\n2\n1\n4\n\n\n3\n1\n3\n1\n2\n4\n3\n2\n4\n2\n5\n4\n\n\n4\n1\n1\n1\n2\n4\n2\n1\n3\n1\n2\n3"
  },
  {
    "objectID": "hints_regression.html#random-forest-model-comparison",
    "href": "hints_regression.html#random-forest-model-comparison",
    "title": "Machine Learning",
    "section": "Random Forest Model Comparison",
    "text": "Random Forest Model Comparison\n\n\nCode\nimport pandas as pd\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Create a function to safely compute metrics\ndef get_class_metric(y_true, y_pred, target_class, metric):\n    \"\"\"\n    Safely extract precision/recall/F1-score for a specific class.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True)\n    if target_class in report:\n        return report[target_class].get(metric, None)\n    return None\n\n# Define the models and their predictions\nmodels = [\"Initial RF\", \"Balanced RF\", \"Tuned RF\"]\npredictions = [y_pred_initial, y_pred_resampled, y_pred_best]\n\n# Compute metrics for each model\nresults = {\n    \"Model\": models,\n    \"Accuracy\": [accuracy_score(y_test, pred) for pred in predictions],\n    \"Class 3 Precision\": [get_class_metric(y_test, pred, \"3\", \"precision\") for pred in predictions],\n    \"Class 4 Precision\": [get_class_metric(y_test, pred, \"4\", \"precision\") for pred in predictions],\n    \"Class 3 Recall\": [get_class_metric(y_test, pred, \"3\", \"recall\") for pred in predictions],\n    \"Class 4 Recall\": [get_class_metric(y_test, pred, \"4\", \"recall\") for pred in predictions],\n}\n\n# Convert to a DataFrame for display\ncomparison_df = pd.DataFrame(results)\n\n# Display the comparison\nprint(\"\\nModel Comparison:\")\nprint(comparison_df)\n\n\n\nModel Comparison:\n         Model  Accuracy  Class 3 Precision  Class 4 Precision  \\\n0   Initial RF  0.750000           0.779661           0.400000   \n1  Balanced RF  0.609375           0.760870           0.222222   \n2     Tuned RF  0.625000           0.777778           0.263158   \n\n   Class 3 Recall  Class 4 Recall  \n0        0.938776        0.133333  \n1        0.714286        0.266667  \n2        0.714286        0.333333"
  },
  {
    "objectID": "hints_regression.html#takeaways-from-random-forest-models",
    "href": "hints_regression.html#takeaways-from-random-forest-models",
    "title": "Machine Learning",
    "section": "Takeaways from Random Forest Models",
    "text": "Takeaways from Random Forest Models\nInitial Random Forest (RF) Model\nThe Initial Random Forest model achieves a high accuracy of 73.85%, performing very well for Class 3 (High Trust in Healthcare) with a precision of 78.33% and an excellent recall of 92.16%. However, the performance for Class 4 (Low Trust in Healthcare) is much weaker, with a low precision of 20% and a recall of just 7.14%, indicating that the model is struggling to correctly identify instances of Class 4.\nBalanced Random Forest (SMOTE) Model\nThe Balanced Random Forest model (with SMOTE) has a lower accuracy of 52.31%. While it improves the recall for Class 4 to 28.57%, the precision remains low at 16%. Class 3 performs well with a precision of 75% and a recall of 58.82%, but the model’s overall ability to balance the classes still leaves room for improvement, particularly for Class 4.\nTuned Random Forest Model\nThe Tuned Random Forest model shows improved performance with accuracy increasing to 61.54%. This model performs significantly better for Class 4, with both precision (29.63%) and recall (57.14%) improving. The precision for Class 3 is also very high (84.21%), with a solid recall of 62.75%. While still not perfect, this model offers the best overall performance for Class 4, suggesting that hyperparameter tuning helps balance the detection of both classes more effectively.\nConclusions:\n\nTuned RF is likely the best model based on this comparison, as it performs well across all metrics.\n\nTuned RF has the highest recall for Class 4 (0.85)\nTuned RF has the highest recall for Class 3 (0.70)\nTuned RF also has the highest precision for Class 4 (0.75)\nTuned RF has the highest precision for Class 3 (0.85)\nThe Tuned RF model has the highest accuracy (0.80)"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "About the Data and Data Cleaning",
    "section": "",
    "text": "To explore the use of online sources in health communication, two publicly available datasets were used, HINTS and Reddit Pushshift. The Health Information National Trends Survey (HINTS) collects information about the Amercian’s use of cancer-related information. In this project, eleven questions from the HINTS survey were collected for review. These questions focus on gathering information about individuals’ behaviors, trust, and perceptions related to cancer information and health communication. These questions collectively provide insights into the sources and levels of trust Americans place in health and cancer information, their experiences in searching for such information, and their perceptions of the reliability of online and social media health content. HINTS is used to better understand how the American public looks for health information for themselves or their loved ones.\nThe Reddit Pushshift dataset includes free text data from the Reddit online forum where users can post and look for information. It also includes subreddits, which branch off into various subreddits—dedicated communities centered around specific topics. By examining this dataset, this project complements insights from HINTS, offering a unique perspective on how individuals seek, share, and discuss cancer-related health information online. The analysis of Reddit data allows researchers to explore the dynamic and informal exchanges that occur in digital forums to better our understanding of how cancer information is communicated."
  },
  {
    "objectID": "data_cleaning.html#hints-preparation-of-data",
    "href": "data_cleaning.html#hints-preparation-of-data",
    "title": "About the Data and Data Cleaning",
    "section": "HINTS Preparation of Data",
    "text": "HINTS Preparation of Data\n\n\nCode\nimport pyreadr\nimport pandas as pd\nfrom IPython.display import display\n\n# Load the .rda file\nresult = pyreadr.read_r('/Users/elizabethkovalchuk/Documents/DSAN6000/Project/fall-2024-project-team-35/data/HINTS6_R_20240524/hints6_public.rda')\n\n# Extract the DataFrame from the loaded data\nhints = result['public']  # Assuming 'public' is the name of the R object in the file\n\n# Specify the columns to select\ncolumns = [\n    \"HHID\", \"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\",\n    \"CancerTrustFamily\", \"CancerTrustGov\", \"CancerTrustCharities\",\n    \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\", \"Electronic2_HealthInfo\",\n    \"MisleadingHealthInfo\", \"TrustHCSystem\"\n]\n\n# Select the relevant columns\nhints_select = hints[columns]\n\n# # Convert the 'updatedate' column if required (commented for now)\n# hints_select['updatedate'] = pd.to_datetime(hints_select['updatedate'] / 1000, unit='s')\n\n# Preview the first few rows\nprint(\"Sample data from the HINTS dataset:\")\ndisplay(hints_select.head())\nprint(f\"Shape of the original dataset: {hints_select.shape}\")\n\n\nSample data from the HINTS dataset:\nShape of the original dataset: (6252, 12)\n\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n21000006\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nQuestion answered in error (Commission Error)\nI do not use social media\nVery\n\n\n1\n21000009\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nSome\nA lot\nSome\nSome\nA lot\nYes\nI do not use social media\nVery\n\n\n2\n21000020\nYes\nSomewhat disagree\nA lot\nSome\nSome\nA little\nNot at all\nA lot\nYes\nSome\nSomewhat\n\n\n3\n21000022\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nInapplicable, coded 2 in UseInternet\nI do not use social media\nSomewhat\n\n\n4\n21000039\nNo\nInapplicable, coded 2 in SeekCancerInfo\nSome\nSome\nSome\nNot at all\nNot at all\nSome\nYes\nA lot\nSomewhat\n\n\n\n\n\n\n\n\n\n\nCode\n# Count missing values in each column\nmissing_values = hints_select.isna().sum()\n\n# Display the count of missing values\nprint(\"Missing values per column:\")\ndisplay(missing_values)\n\n\nMissing values per column:\n\n\nHHID                        0\nSeekCancerInfo              0\nCancerFrustrated            0\nCancerTrustDoctor           0\nCancerTrustFamily           0\nCancerTrustGov              0\nCancerTrustCharities        0\nCancerTrustReligiousOrgs    0\nCancerTrustScientists       0\nElectronic2_HealthInfo      0\nMisleadingHealthInfo        0\nTrustHCSystem               0\ndtype: int64\n\n\n\n\nCode\n# List of ordinal columns\nordinal_columns = [\n    \"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\",\n    \"CancerTrustFamily\", \"CancerTrustGov\", \"CancerTrustCharities\",\n    \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\", \"Electronic2_HealthInfo\",\n    \"MisleadingHealthInfo\", \"TrustHCSystem\"\n]\n\n# Display unique values for each ordinal column\nprint(\"Unique values for ordinal columns:\")\nfor column in ordinal_columns:\n    unique_values = hints_select[column].unique()\n    print(f\"\\nColumn: {column}\")\n    print(f\"Unique Values: {unique_values}\")\n\n\nUnique values for ordinal columns:\n\nColumn: SeekCancerInfo\nUnique Values: ['No', 'Yes', 'Missing data (Not Ascertained)']\nCategories (3, object): ['Missing data (Not Ascertained)', 'No', 'Yes']\n\nColumn: CancerFrustrated\nUnique Values: ['Inapplicable, coded 2 in SeekCancerInfo', 'Somewhat disagree', 'Strongly disagree', 'Somewhat agree', 'Strongly agree', 'Question answered in error (Commission Error)', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (9, object): ['Inapplicable, coded 2 in SeekCancerInfo', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', ..., 'Somewhat agree', 'Somewhat disagree', 'Strongly agree', 'Strongly disagree']\n\nColumn: CancerTrustDoctor\nUnique Values: ['A lot', 'Some', 'Not at all', 'A little', 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustFamily\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'A little', 'Not at all', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustGov\nUnique Values: ['Missing data (Not Ascertained)', 'A lot', 'Some', 'A little', 'Not at all', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustCharities\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'A little', 'Not at all', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustReligiousOrgs\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'Not at all', 'A little', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustScientists\nUnique Values: ['Missing data (Not Ascertained)', 'A lot', 'Some', 'A little', 'Not at all', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: Electronic2_HealthInfo\nUnique Values: ['Question answered in error (Commission Error)', 'Yes', 'Inapplicable, coded 2 in UseInternet', 'No', 'Missing data (Not Ascertained)', 'Missing data (Filter Missing)']\nCategories (6, object): ['Inapplicable, coded 2 in UseInternet', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'No', 'Question answered in error (Commission Error)', 'Yes']\n\nColumn: MisleadingHealthInfo\nUnique Values: ['I do not use social media', 'Some', 'A lot', 'A little', 'None', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se...]\nCategories (7, object): ['A little', 'A lot', 'I do not use social media', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se..., 'None', 'Some']\n\nColumn: TrustHCSystem\nUnique Values: ['Very', 'Somewhat', 'A little', 'Not at all', 'Missing data (Web partial - Question Never Se..., 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (7, object): ['A little', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se..., 'Multiple responses selected in error', 'Not at all', 'Somewhat', 'Very']\n\n\n\n\nCode\n# Define the valid scales for each column\nvalid_scales = {\n    \"CancerFrustrated\": ['Somewhat disagree', 'Strongly disagree', 'Somewhat agree', 'Strongly agree'],\n    \"CancerTrustDoctor\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustFamily\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustGov\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustCharities\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustReligiousOrgs\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustScientists\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"TrustHCSystem\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"Electronic2_HealthInfo\": ['Yes', 'No'], \n    \"MisleadingHealthInfo\": ['I do not use social media', 'None', 'A little', 'Some', 'A lot']  \n}\n\n# Create a copy of the original DataFrame\nhints_cleaned = hints_select.copy()\n\n# Filter the DataFrame\nfor column, scale in valid_scales.items():\n    hints_cleaned = hints_cleaned[hints_cleaned[column].isin(scale)]\n\n# Display the cleaned dataset and its shape\nprint(\"Data after filtering invalid values:\")\ndisplay(hints_cleaned.head())\nprint(f\"Shape of the cleaned dataset: {hints_cleaned.shape}\")\n\n\nData after filtering invalid values:\nShape of the cleaned dataset: (323, 12)\n\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n51\n21000330\nYes\nSomewhat disagree\nSome\nNot at all\nSome\nSome\nNot at all\nA lot\nYes\nA lot\nA little\n\n\n112\n21000976\nYes\nSomewhat agree\nA lot\nSome\nSome\nSome\nSome\nA lot\nYes\nSome\nA little\n\n\n136\n21001112\nYes\nSomewhat disagree\nA little\nA little\nNot at all\nNot at all\nNot at all\nA little\nNo\nA lot\nNot at all\n\n\n157\n21001283\nYes\nSomewhat disagree\nA lot\nSome\nNot at all\nA little\nSome\nNot at all\nNo\nI do not use social media\nNot at all\n\n\n181\n21001548\nYes\nStrongly agree\nA lot\nSome\nNot at all\nSome\nA lot\nA little\nYes\nSome\nA little\n\n\n\n\n\n\n\n\n\n\nCode\n# Count the number of NA or NaN values in each column\nna_count = hints_cleaned.isna().sum()\n#print(\"NA values count per column:\")\n#print(na_count)\n#print(hints_cleaned.shape)\n# Count unique values in the 'SeekCancerInfo' column\nvalue_counts = hints_cleaned['SeekCancerInfo'].value_counts()\n#print(\"Unique value counts in 'SeekCancerInfo':\")\n#print(value_counts)\n\n# Save the cleaned dataset to an Excel file\noutput_file = \"../data/csv/hints_cleaned_forML_spearman.csv\"\nhints_cleaned.to_csv(output_file, index=False)\n\n#print(f\"Cleaned dataset saved as {output_file}\")\n\n\nUnique value counts in 'SeekCancerInfo':\nSeekCancerInfo\nYes                               323\nMissing data (Not Ascertained)      0\nNo                                  0\nName: count, dtype: int64\nCleaned dataset saved as ../data/csv/hints_cleaned_forML_spearman.csv"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "NLP",
    "section": "",
    "text": "Two different types of sentiment analysis was performed on both the cancer and non-cancer subreddits. In this method, a pretrained SparNLP pipeline in Azure ML was used to classify the comments has positive, negative, or neutral.\n\n\n\nsentiment_label\ncount_cancer\ncount_non_cancer\n\n\n\n\npositive\n5929\n5715\n\n\nnegative\n4071\n4285\n\n\n\n\nData Loading Comments from cancer-related and non-cancer subreddits were loaded as Parquet files saved in an Azure Blob container.\nSentiment Analysis using SparkNLP Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral.\nWeighted Scores A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment.\nSentiment Labeling Comments were labeled based on their weighted scores (positive, negative, or neutral).\nGrouping and Aggregation The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits.\nStatistical Testing A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories showed a difference in positive and negative sentiment in the cancer and non-cancer subreddits.\n\nThe source code for the sentiment analysis using SparkNLP is here:\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\nResults of Chi-square Test\nChi-square statistic: 9.325853191514204\nP-value: 0.0022594309243996907\nDegrees of freedom: 1\nExpected frequencies: [[5822. 5822.]\n[4178. 4178.]]\nTo assess the difference between positive and negative sentiment in the cancer and non-cancer subreddits the odds ratio (OR) is calculated as:\n\\[Odds Ratio (OR) = \\frac{(\\text{Positive Cancer} / \\text{Positive Non-Cancer})}{(\\text{Negative Cancer} / \\text{Negative Non-Cancer})}\\]\n\\[OR = \\frac{(5929/5715)}{(4071/4285)}\\]\n\\[OR = \\frac{(5929 \\times 4285)}{(5715 \\times 4071)}\\]\n\\[OR = 1.065\\]\n\\[\\beta = ln(OR)\\]\n\\[\\beta = ln(1.065) = 0.063\\]\nThe coefficient indicates the log odds of cancer for the “positive” sentiment label relative to the “negative” sentiment label. A positive value of 0.063 suggests that there is a slight increase in “positive” sentiment in the cancer subreddits compared to the non-cancer subreddits.\nThe p-value of 0.002 confirms a statistical significance. However, with the odds ratio, the positive sentiment is only slightly increased.\nPositive sentiment was higher in the cancer subreddit indicating the these groups may provide support and resiliency in the communities who use these subreddits. In comparison to the rest of Reddit (the non-cancer subreddits), users may look for look and discuss for more information about experience with treatment, logistics, and navigating a cancer diagnosis, fostering a space for encouragement and information.\n\n\n\n\n\n\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubreddit\npositive\nnegative\nneutral\nanger\nfear\ndisgust\nsadness\njoy\nsurprise\ntrust\n\n\n\n\nCancer\n0.202806\n0.119079\n0.0\n0.037681\n0.067383\n0.033798\n0.058226\n0.044399\n0.028933\n0.119079\n\n\nNon-Cancer\n0.170739\n0.096125\n0.0\n0.034273\n0.041384\n0.023316\n0.034463\n0.044963\n0.024675\n0.082917\n\n\n\n\n\n\n\n\n\nGiven the results of the sentiment analysis, the frequency of popular words in the cancer subreddit were assess using term frequency-inverse document frequency (TF-IDF). This method was used to identify unique or prominent keywords.\n\nDATA PREPARATION\n\n\nTwo separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits.\nThe datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer).\nText content was preprocessed into a column for TF-IDF analysis.\n\n\nTF-IDF VECTORIZATION\n\n\nThe TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets.\nCommon stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents.\n\n\nAGGREGATION & RANKING\n\n\nWords were ranked in descending order of their total TF-IDF scores for the cancer dataset.\nRankings allowed for the identification of keywords in the cancer dataset.\n\n\n\n\nWord\nTF-IDF\n\n\n\n\ntime\n604.333288\n\n\nremoved\n510.072719\n\n\nthank\n508.752837\n\n\ninformation\n491.421062\n\n\ndoctor\n480.772518\n\n\npeople\n467.593201\n\n\nwork\n443.772165\n\n\npatient\n387.074392\n\n\ncancer\n365.678933\n\n\nbest\n340.635342\n\n\n\n\n\n\nTF-IDF\n\n\nIn this word cloud we see words such as time, removed, thank, and information. Some of these words may be used in conversations to talk about cancer care and possibly gratitude from support groups in the Reddit forum.\n\n\n\n\n\nCrowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\nEmotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon, Saif Mohammad and Peter Turney, In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California."
  },
  {
    "objectID": "eda_hints.html",
    "href": "eda_hints.html",
    "title": "EDA HINTS DATA",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(gtsummary)\nlibrary(reshape2)\nlibrary(RColorBrewer)\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(caret)\n\n\nThe HINTS survey includes 527 questions. To focus on sentiment toward healthcare and cancer, we selected the questions in Table 1. In Table 1, participants were asked various questions about where they access health information, do they trust the provided health information, and if they feel frustrated about the information.\nTable 1\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSeekCancerInfo\nHave you ever looked for information about cancer from any source?\n\n\nCancerFrustrated\nBased on the results of your most recent search for information about cancer, how much do you agree or disagree: You felt frustrated during your search for the information.\n\n\nCancerTrustDoctor\nIn general, how much would you trust information about cancer from a doctor?\n\n\nCancerTrustFamily\nIn general, how much would you trust information about cancer from family or friends?\n\n\nCancerTrustGov\nIn general, how much would you trust information about cancer from government health agencies?\n\n\nCancerTrustCharities\nIn general, how much would you trust information about cancer from charitable organizations?\n\n\nCancerTrustReligiousOrgs\nIn general, how much would you trust information about cancer from religious organizations and leaders?\n\n\nCancerTrustScientists\nIn general, how much would you trust information about cancer from scientists?\n\n\nElectronic2_HealthInfo\nIn the past 12 months have you used the Internet to look for health or medical information?\n\n\nMisleadingHealthInfo\nHow much of the health information that you see on social media do you think is false or misleading?\n\n\nTrustHCSystem\nHow much do you trust the health care system (for example, hospitals, pharmacies, and other organizations involved in health care)?\n\n\n\n\n\nCode\nfile_path &lt;- '../data/csv/hints_cleaned_forML_spearman.csv'\nhints_cleaned &lt;- read.csv(file_path)\n\n# Drop the 'SeekCancerInfo' column from hints_cleaned\nhints_cleaned &lt;- hints_cleaned %&gt;%\n  select(-SeekCancerInfo, -HHID)\n\n# Drop rows with NA values\nhints_cleaned &lt;- na.omit(hints_cleaned)\n\n# print(head(hints_cleaned))\n# print(paste(\"Shape of the dataframe:\", paste(dim(hints_cleaned), collapse = \" x \")))\n\n\n\n\nCode\n# Converting to numeric \nlibrary(dplyr)\n\n# Define the mappings\ntrust_mapping &lt;- c(\"Not at all\" = 4, \"A little\" = 3, \"Some\" = 2, \"A lot\" = 1)\nagreement_mapping &lt;- c(\"Strongly agree\" = 1, \"Somewhat agree\" = 2, \"Somewhat disagree\" = 3, \"Strongly disagree\" = 4)\nbinary_mapping &lt;- c(\"Yes\" = 1, \"No\" = 2)\nmisleading_info_mapping &lt;- c(\"I do not use social media\" = 5, \"None\" = 4, \"A little\" = 3, \"Some\" = 2, \"A lot\" = 1)\n\n# Apply the mappings and transformations\nhints_cleaned &lt;- hints_cleaned %&gt;%\n  filter(!is.na(MisleadingHealthInfo)) %&gt;%\n  mutate(\n    CancerFrustrated = recode(CancerFrustrated, !!!agreement_mapping),\n    CancerTrustDoctor = recode(CancerTrustDoctor, !!!trust_mapping),\n    CancerTrustFamily = recode(CancerTrustFamily, !!!trust_mapping),\n    CancerTrustGov = recode(CancerTrustGov, !!!trust_mapping),\n    CancerTrustCharities = recode(CancerTrustCharities, !!!trust_mapping),\n    CancerTrustReligiousOrgs = recode(CancerTrustReligiousOrgs, !!!trust_mapping),\n    CancerTrustScientists = recode(CancerTrustScientists, !!!trust_mapping),\n    TrustHCSystem = recode(TrustHCSystem, !!!trust_mapping),\n    Electronic2_HealthInfo = recode(Electronic2_HealthInfo, !!!binary_mapping),\n    MisleadingHealthInfo = recode(MisleadingHealthInfo, !!!misleading_info_mapping)\n  )\n\n# Display the transformed dataset\n# cat(\"Data after applying mappings to numeric values:\\n\")\n# print(head(hints_cleaned, n = 5), digits = 2)\n\n# Display the data types of the columns\n# cat(\"\\nColumn types:\\n\")\n# str(hints_cleaned)\n\n\n\n\nCode\n# Standardize the data (excluding non-numeric columns)\nnumeric_columns &lt;- sapply(hints_cleaned, is.numeric)\nstandardized_data &lt;- hints_cleaned[, numeric_columns] %&gt;%\n  scale()  # Standardize the numeric columns\n\n# Convert the standardized data back to a data frame\nstandardized_data &lt;- as.data.frame(standardized_data)\n\n# Define target variable\ntarget_variable &lt;- 'TrustHCSystem'  # Replace with your actual target column name\n\n# Check if the target variable is in the standardized data\nif (!(target_variable %in% colnames(standardized_data))) {\n  stop(paste(\"Target variable '\", target_variable, \"' not found in the dataset.\", sep = \"\"))\n}\n\n# Compute Spearman correlation matrix\ncorrelation_matrix_spearman &lt;- cor(standardized_data, method = \"spearman\")\n\n# Rename the matrix to correlation_data\ncorrelation_data &lt;- correlation_matrix_spearman\n\n# Display the full Spearman correlation matrix\n# print(\"Spearman Correlation Matrix (excluding SeekCancerInfo and after standardization):\")\n# print(correlation_data)\n\n# Focus on the correlation of the target variable with other features\ncorrelation_with_target_spearman &lt;- correlation_data[, target_variable] %&gt;%\n  sort(decreasing = TRUE)\n\n# print(paste(\"\\nSpearman correlation of features with\", target_variable, \":\"))\n# print(correlation_with_target_spearman)\n\n\n\n\nCode\n# Convert the correlation matrix into a long format for ggplot\nmelted_correlation &lt;- melt(correlation_matrix_spearman)\n\n# Mask the upper triangle and diagonal of the correlation matrix\nmelted_correlation$value[upper.tri(correlation_matrix_spearman, diag = TRUE)] &lt;- NA\n\n# Create a custom color scale\ncustom_colors &lt;- colorRampPalette(c(\"#ffffff\", \"#3d6469\"))(100)\n\n# Plot the heatmap\nggplot(melted_correlation, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradientn(colours = custom_colors, \n                       limits = c(-1, 1), \n                       na.value = \"white\",\n                       name = \"Spearman\\nCorrelation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),\n        axis.text.y = element_text(angle = 0, hjust = 1),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"right\") +\n  labs(title = \"Spearman Correlation Matrix (Ordinal Data)\") +\n  coord_fixed() +\n  geom_text(aes(label = ifelse(is.na(value), \"\", sprintf(\"%.2f\", value))), \n            color = \"black\", size = 2.5)\n\n\n\n\n\n\n\n\n\n\n\nCode\nload('../data/HINTS6_R_20240524/hints6_public.rda')\nhints &lt;- as.data.frame(public)\n\n#print(colnames(hints))\n\ncolumns &lt;- c(\"HHID\", \"updatedate\", \"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\", \"CancerTrustFamily\", \"CancerTrustGov\", \"CancerTrustCharities\", \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\", \"Electronic2_HealthInfo\", \"MisleadingHealthInfo\", \"TrustHCSystem\")\n\nhints_select &lt;- hints %&gt;% select(all_of(columns))\n#hints_select$updatedate &lt;- hints_select$updatedate / 1000\n#hints_select$updatedate &lt;- as_datetime(hints_select$updatedate)\n\nhead(hints_select)\n\n\n      HHID  updatedate SeekCancerInfo                        CancerFrustrated\n1 21000006 13870396800             No Inapplicable, coded 2 in SeekCancerInfo\n2 21000009 13874630400             No Inapplicable, coded 2 in SeekCancerInfo\n3 21000020 13873680000            Yes                       Somewhat disagree\n4 21000022 13867891200             No Inapplicable, coded 2 in SeekCancerInfo\n5 21000039 13866336000             No Inapplicable, coded 2 in SeekCancerInfo\n6 21000043 13866595200             No Inapplicable, coded 2 in SeekCancerInfo\n  CancerTrustDoctor              CancerTrustFamily\n1             A lot Missing data (Not Ascertained)\n2             A lot                           Some\n3             A lot                           Some\n4             A lot Missing data (Not Ascertained)\n5              Some                           Some\n6             A lot                           Some\n                  CancerTrustGov           CancerTrustCharities\n1 Missing data (Not Ascertained) Missing data (Not Ascertained)\n2                          A lot                           Some\n3                           Some                       A little\n4 Missing data (Not Ascertained) Missing data (Not Ascertained)\n5                           Some                     Not at all\n6                           Some                          A lot\n        CancerTrustReligiousOrgs          CancerTrustScientists\n1 Missing data (Not Ascertained) Missing data (Not Ascertained)\n2                           Some                          A lot\n3                     Not at all                          A lot\n4 Missing data (Not Ascertained) Missing data (Not Ascertained)\n5                     Not at all                           Some\n6                       A little                          A lot\n                         Electronic2_HealthInfo      MisleadingHealthInfo\n1 Question answered in error (Commission Error) I do not use social media\n2                                           Yes I do not use social media\n3                                           Yes                      Some\n4          Inapplicable, coded 2 in UseInternet I do not use social media\n5                                           Yes                     A lot\n6                                           Yes                     A lot\n  TrustHCSystem\n1          Very\n2          Very\n3      Somewhat\n4      Somewhat\n5      Somewhat\n6      A little\n\n\n\nSurvey Responses\nIn the bar graphs, a first look at the data provides an general overview of the responses to the questions. These plots show how much participants agree or disagree to each question. For example, many participants can trust doctors a lot and less than family members.\n\n\nCode\nplot_data &lt;- hints_select %&gt;%\n  select(-HHID, -updatedate) %&gt;%  # Exclude the first two columns\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Value\") %&gt;%\n  count(Variable, Value)\n\nprint(unique(plot_data$Value))\n\n\n [1] Missing data (Not Ascertained)                  \n [2] Missing data (Filter Missing)                   \n [3] Multiple responses selected in error            \n [4] Question answered in error (Commission Error)   \n [5] Inapplicable, coded 2 in SeekCancerInfo         \n [6] Strongly agree                                  \n [7] Somewhat agree                                  \n [8] Somewhat disagree                               \n [9] Strongly disagree                               \n[10] A lot                                           \n[11] Some                                            \n[12] A little                                        \n[13] Not at all                                      \n[14] Yes                                             \n[15] No                                              \n[16] Inapplicable, coded 2 in UseInternet            \n[17] Missing data (Web partial - Question Never Seen)\n[18] None                                            \n[19] I do not use social media                       \n[20] Very                                            \n[21] Somewhat                                        \n21 Levels: Missing data (Not Ascertained) Yes ... Somewhat\n\n\nCode\nvalues &lt;- c(\"Strongly agree\", \"Somewhat agree\", \"Somewhat disagree\", \"Strongly disagree\", \"A lot\", \"Some\", \n            \"A little\", \"Not at all\", \"Yes\", \"No\", \"None\", \"I do not use social media\", \"Very\", \"Somewhat\")\n\nplot_data_filtered &lt;- plot_data %&gt;% filter(Value %in% values) \nplot_data_filtered$Value &lt;- factor(plot_data_filtered$Value, levels = sort(unique(plot_data_filtered$Value)))\n\ncolumns_1 &lt;- c(\"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\", \"CancerTrustFamily\")\nplot_data_filtered_1 &lt;- plot_data_filtered %&gt;% filter(Variable %in% columns_1)\ncolumns_2 &lt;- c(\"CancerTrustGov\", \"CancerTrustCharities\", \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\")\nplot_data_filtered_2 &lt;- plot_data_filtered %&gt;% filter(Variable %in% columns_2)\ncolumns_3 &lt;- c(\"Electronic2_HealthInfo\", \"MisleadingHealthInfo\", \"TrustHCSystem\")\nplot_data_filtered_3 &lt;- plot_data_filtered %&gt;% filter(Variable %in% columns_3) \n\np &lt;- ggplot(plot_data_filtered_1, aes(x = Value, y = n, fill = Variable)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ Variable, scales = \"free_x\") +  # Separate plots for each column\n  theme_minimal() +\n  labs(\n    title = \"HINTS Survey Responses\",\n    x = \"Responses\",\n    y = \"Count\",\n    fill = \"Question\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\np\n\n\n\n\n\n\n\n\n\nCode\npng(\"../data/HINTS6_R_20240524/HINTS_plot1.png\", width = 800, height = 600)\nprint(p)  \ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\nCode\np &lt;- ggplot(plot_data_filtered_2, aes(x = Value, y = n, fill = Variable)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ Variable, scales = \"free_x\") +  \n  theme_minimal() +\n  labs(\n    title = \"HINTS Survey Responses\",\n    x = \"Responses\",\n    y = \"Count\",\n    fill = \"Question\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\np\n\n\n\n\n\n\n\n\n\nCode\npng(\"../data/HINTS6_R_20240524/HINTS_plot2.png\", width = 800, height = 600)\nprint(p)  \ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\nCode\np &lt;- ggplot(plot_data_filtered_3, aes(x = Value, y = n, fill = Variable)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ Variable, scales = \"free_x\", nrow=2) +\n  theme_minimal() +\n  labs(\n    title = \"HINTS Survey Responses\",\n    x = \"Responses\",\n    y = \"Count\",\n    fill = \"Question\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\np\n\n\n\n\n\n\n\n\n\nCode\npng(\"../data/HINTS6_R_20240524/HINTS_plot3.png\", width = 800, height = 600)\nprint(p) \ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\nSummary Statistics of the Responses\nThe responses of the questions can be coded into a scale from 0:3. For example, “Not at all” is coded as 0, “A little” is coded as 1, “Some” is coded as 2, and “A lot” is coded as 3. After coding these responses, the mean, median, and mode is calculated to highlight overall trends in the data.\n\n\nCode\nprint(unique(hints_select$MisleadingHealthInfo))\n\n\n[1] I do not use social media                       \n[2] Some                                            \n[3] A lot                                           \n[4] A little                                        \n[5] None                                            \n[6] Missing data (Not Ascertained)                  \n[7] Missing data (Web partial - Question Never Seen)\n7 Levels: Missing data (Not Ascertained) ...\n\n\n\n\nCode\nprint(colnames(hints_select))\n\n\n [1] \"HHID\"                     \"updatedate\"              \n [3] \"SeekCancerInfo\"           \"CancerFrustrated\"        \n [5] \"CancerTrustDoctor\"        \"CancerTrustFamily\"       \n [7] \"CancerTrustGov\"           \"CancerTrustCharities\"    \n [9] \"CancerTrustReligiousOrgs\" \"CancerTrustScientists\"   \n[11] \"Electronic2_HealthInfo\"   \"MisleadingHealthInfo\"    \n[13] \"TrustHCSystem\"           \n\n\nCode\nhints_select_coded &lt;- hints_select %&gt;%\n  mutate(CancerFrustrated = as.numeric(case_when(\n    CancerFrustrated == \"Strongly disagree\" ~ \"0\",\n    CancerFrustrated == \"Somewhat disagree\" ~ \"1\",\n    CancerFrustrated == \"Somewhat agree\" ~ \"2\",\n    CancerFrustrated == \"Strongly agree\" ~ \"3\",\n    TRUE ~  NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustDoctor = as.numeric(case_when(\n    CancerTrustDoctor == \"Not at all\" ~ \"0\",\n    CancerTrustDoctor == \"A little\" ~ \"1\",\n    CancerTrustDoctor == \"Some\" ~ \"2\",\n    CancerTrustDoctor == \"A lot\" ~ \"3\",\n    TRUE ~  NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustFamily = as.numeric(case_when(\n    CancerTrustFamily == \"None\" ~ \"0\",\n    CancerTrustFamily == \"A little\" ~ \"1\",\n    CancerTrustFamily == \"Some\" ~ \"2\",\n    CancerTrustFamily == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(SeekCancerInfo = as.numeric(case_when(\n    SeekCancerInfo == \"Yes\" ~ \"1\",\n    SeekCancerInfo == \"No\" ~ \"0\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustCharities = as.numeric(case_when(\n    CancerTrustCharities == \"Not at all\" ~ \"0\",\n    CancerTrustCharities == \"A little\" ~ \"1\",\n    CancerTrustCharities == \"Some\" ~ \"2\",\n    CancerTrustCharities == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustGov = as.numeric(case_when(\n    CancerTrustGov == \"Not at all\" ~ \"0\",\n    CancerTrustGov == \"A little\" ~ \"1\",\n    CancerTrustGov == \"Some\" ~ \"2\",\n    CancerTrustGov == \"A lot\" ~ \"3\",\n    TRUE ~ NA\n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustReligiousOrgs = as.numeric(case_when(\n    CancerTrustReligiousOrgs == \"Not at all\" ~ \"0\",\n    CancerTrustReligiousOrgs == \"A little\" ~ \"1\",\n    CancerTrustReligiousOrgs == \"Some\" ~ \"2\",\n    CancerTrustReligiousOrgs == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustScientists = as.numeric(case_when(\n    CancerTrustScientists == \"Not at all\" ~ \"0\",\n    CancerTrustScientists == \"A little\" ~ \"1\",\n    CancerTrustScientists == \"Some\" ~ \"2\",\n    CancerTrustScientists == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(Electronic2_HealthInfo = as.numeric(case_when(\n    Electronic2_HealthInfo == \"Yes\" ~ \"1\",\n    Electronic2_HealthInfo == \"No\" ~ \"0\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(MisleadingHealthInfo = as.numeric(case_when(\n    MisleadingHealthInfo == \"None\" ~ \"0\",\n    MisleadingHealthInfo == \"I do not use social media\" ~ \"0\",\n    MisleadingHealthInfo == \"A little\" ~ \"1\",\n    MisleadingHealthInfo == \"Some\" ~ \"2\",\n    MisleadingHealthInfo == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(TrustHCSystem = as.numeric(case_when(\n    TrustHCSystem == \"Not at all\" ~ \"0\",\n    TrustHCSystem == \"A little\" ~ \"1\",\n    TrustHCSystem == \"Somewhat\" ~ \"2\",\n    TrustHCSystem == \"Very\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\n\nprint(head(hints_select_coded))\n\n\n      HHID  updatedate SeekCancerInfo CancerFrustrated CancerTrustDoctor\n1 21000006 13870396800              0               NA                 3\n2 21000009 13874630400              0               NA                 3\n3 21000020 13873680000              1                1                 3\n4 21000022 13867891200              0               NA                 3\n5 21000039 13866336000              0               NA                 2\n6 21000043 13866595200              0               NA                 3\n  CancerTrustFamily CancerTrustGov CancerTrustCharities\n1                NA             NA                   NA\n2                 2              3                    2\n3                 2              2                    1\n4                NA             NA                   NA\n5                 2              2                    0\n6                 2              2                    3\n  CancerTrustReligiousOrgs CancerTrustScientists Electronic2_HealthInfo\n1                       NA                    NA                     NA\n2                        2                     3                      1\n3                        0                     3                      1\n4                       NA                    NA                     NA\n5                        0                     2                      1\n6                        1                     3                      1\n  MisleadingHealthInfo TrustHCSystem\n1                    0             3\n2                    0             3\n3                    2             2\n4                    0             2\n5                    3             2\n6                    3             1\n\n\nIn the summary table below, the mean for trusting a doctor is higher than trusting the government. Given this information, we will also look at the Reddit dataset to see the level of trust users have when they mention the government in their comments versus doctors. In addition, the people who felt frustrated about the information they received about cancer is approximately 1.105. In the Reddit dataset, we also look for an equivalent using textual data by looking at positive/negative and emotion sentiment analysis on comments that include the word “cancer”.\n\n\nCode\nprint(summary(hints_select_coded))\n\n\n     HHID             updatedate        SeekCancerInfo   CancerFrustrated\n Length:6252        Min.   :1.387e+10   Min.   :0.0000   Min.   :0.000   \n Class :character   1st Qu.:1.387e+10   1st Qu.:0.0000   1st Qu.:0.000   \n Mode  :character   Median :1.387e+10   Median :0.0000   Median :1.000   \n                    Mean   :1.387e+10   Mean   :0.4654   Mean   :1.105   \n                    3rd Qu.:1.387e+10   3rd Qu.:1.0000   3rd Qu.:2.000   \n                    Max.   :1.389e+10   Max.   :1.0000   Max.   :3.000   \n                                        NA's   :17       NA's   :3420    \n CancerTrustDoctor CancerTrustFamily CancerTrustGov CancerTrustCharities\n Min.   :0.000     Min.   :1.000     Min.   :0.00   Min.   :0.000       \n 1st Qu.:2.000     1st Qu.:1.000     1st Qu.:1.00   1st Qu.:1.000       \n Median :3.000     Median :2.000     Median :2.00   Median :1.000       \n Mean   :2.656     Mean   :1.678     Mean   :1.92   Mean   :1.403       \n 3rd Qu.:3.000     3rd Qu.:2.000     3rd Qu.:3.00   3rd Qu.:2.000       \n Max.   :3.000     Max.   :3.000     Max.   :3.00   Max.   :3.000       \n NA's   :94        NA's   :783       NA's   :273    NA's   :308         \n CancerTrustReligiousOrgs CancerTrustScientists Electronic2_HealthInfo\n Min.   :0.0000           Min.   :0.000         Min.   :0.0000        \n 1st Qu.:0.0000           1st Qu.:2.000         1st Qu.:1.0000        \n Median :1.0000           Median :3.000         Median :1.0000        \n Mean   :0.9484           Mean   :2.357         Mean   :0.8534        \n 3rd Qu.:2.0000           3rd Qu.:3.000         3rd Qu.:1.0000        \n Max.   :3.0000           Max.   :3.000         Max.   :1.0000        \n NA's   :280              NA's   :218           NA's   :1130          \n MisleadingHealthInfo TrustHCSystem\n Min.   :0.000        Min.   :0.0  \n 1st Qu.:1.000        1st Qu.:2.0  \n Median :2.000        Median :2.0  \n Mean   :1.716        Mean   :2.2  \n 3rd Qu.:3.000        3rd Qu.:3.0  \n Max.   :3.000        Max.   :3.0  \n NA's   :82           NA's   :134  \n\n\n\n\nCode\nhints_select_coded_clean &lt;- drop_na(hints_select_coded)\n\ntable1 &lt;- hints_select_coded_clean %&gt;%\n  select(-HHID, -updatedate) %&gt;%\n  tbl_summary(\n    statistic = all_continuous() ~ \"{mean} ± {sd}\",  \n  )\n\ntable1\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 2,315\n1\n\n\n\n\nSeekCancerInfo\n\n\n\n\n    1\n2,315 (100%)\n\n\nCancerFrustrated\n\n\n\n\n    0\n759 (33%)\n\n\n    1\n780 (34%)\n\n\n    2\n624 (27%)\n\n\n    3\n152 (6.6%)\n\n\nCancerTrustDoctor\n\n\n\n\n    0\n11 (0.5%)\n\n\n    1\n72 (3.1%)\n\n\n    2\n441 (19%)\n\n\n    3\n1,791 (77%)\n\n\nCancerTrustFamily\n\n\n\n\n    1\n970 (42%)\n\n\n    2\n1,225 (53%)\n\n\n    3\n120 (5.2%)\n\n\nCancerTrustGov\n\n\n\n\n    0\n133 (5.7%)\n\n\n    1\n357 (15%)\n\n\n    2\n1,065 (46%)\n\n\n    3\n760 (33%)\n\n\nCancerTrustCharities\n\n\n\n\n    0\n251 (11%)\n\n\n    1\n798 (34%)\n\n\n    2\n1,086 (47%)\n\n\n    3\n180 (7.8%)\n\n\nCancerTrustReligiousOrgs\n\n\n\n\n    0\n907 (39%)\n\n\n    1\n832 (36%)\n\n\n    2\n494 (21%)\n\n\n    3\n82 (3.5%)\n\n\nCancerTrustScientists\n\n\n\n\n    0\n41 (1.8%)\n\n\n    1\n165 (7.1%)\n\n\n    2\n608 (26%)\n\n\n    3\n1,501 (65%)\n\n\nElectronic2_HealthInfo\n2,203 (95%)\n\n\nMisleadingHealthInfo\n\n\n\n\n    0\n287 (12%)\n\n\n    1\n287 (12%)\n\n\n    2\n959 (41%)\n\n\n    3\n782 (34%)\n\n\nTrustHCSystem\n\n\n\n\n    0\n60 (2.6%)\n\n\n    1\n231 (10.0%)\n\n\n    2\n1,109 (48%)\n\n\n    3\n915 (40%)\n\n\n\n1\nn (%)\n\n\n\n\n\n\n\n\n\nThe box plot provides a visualization of the median, mode, and outliers in the dataset.\n\n\nCode\nboxplot_data &lt;- hints_select_coded %&gt;%\n  select(-HHID, -updatedate)\n\n\nboxplot_data_long &lt;- boxplot_data %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Variable\", values_to = \"Value\")\n\n\nggplot(boxplot_data_long, aes(x = Variable, y = Value)) +\n  geom_boxplot(outlier.colour = \"red\", outlier.size = 1) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  \n  labs(\n    title = \"HINTS Boxplot\",\n    x = \"Variables\",\n    y = \"Values\"\n  ) + coord_cartesian(ylim = c(-1, 4)) \n\n\nWarning: Removed 6739 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nRelationship between different survey questions.\nUsing a correlation plot, the relationships are evaluated between the different survey questions. In the correlation plot below, we see an negative correlation between trust for doctors versus trusting the government, scientists, and the healthcare system. Further statistical testing can be performed to better understand this initial evaluation.\n\n\nCode\nlibrary(corrplot)\n\n\ncorrplot 0.95 loaded\n\n\nCode\ncor_matrix &lt;- drop_na(hints_select_coded)\n\n# correlation matrix\ncor_matrix &lt;- cor(cor_matrix[, sapply(cor_matrix, is.numeric)], use = \"complete.obs\")\n\n\nWarning in cor(cor_matrix[, sapply(cor_matrix, is.numeric)], use =\n\"complete.obs\"): the standard deviation is zero\n\n\nCode\n# correlation plot\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", tl.col = \"black\", tl.srt = 45, \n         addCoef.col = \"black\", number.cex = 0.7, diag = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nFurther Directions and Conclusion\nThe HINTS dataset provides insight into the perceptions of healthcare and cancer information. The trends in this dataset will be repeated in the Reddit dataset. Using the Reddit dataset, we will explore sentiments, such as positive/negative, frustrations, and trust. We will also look at word frequency count to review, which topics Reddit users commonly comment about."
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "ML",
    "section": "",
    "text": "Test Performance\nNaive Bayes\nLogistic Regression\n\n\n\n\nAccuracy\n0.775\n0.707\n\n\nPrecision\n0.777\n0.707\n\n\nRecall\n0.775\n0.707\n\n\nF1-Score\n0.775\n0.707\n\n\nAUC-ROC\n0.776\n0.707"
  },
  {
    "objectID": "eda-nlp-reddit.html",
    "href": "eda-nlp-reddit.html",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "",
    "text": "Entire comments and subcomments that contain the word “cancer”\n\n(Parallel to A1, B3a, B13 in HINTS?)\nCalculate frequency per year\n\nReddits and subreddits that contain the word “cancer”\n\nIn addition, we combed the entire Reddit PushShift dataset provided based on the following criteria:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nFrustratingCancerSearch\n“Frustrating” or “frustrat” and “cancer” (HINTS A2b)\n\n\nCancerDoctorsTrust\n“Cancer” and “doctors” or “trust” (does not need to contain “trust” since trust is included in NRC sentiment analysis) (HINTS A3a)\n\n\nCancerFamilyTrust\n“Cancer” and “family” or “friends” or various familial terms (e.g., “sister,” “brother,” “mother”) or “trust” (HINTS A3b)\n\n\nCancerGovHealthcarePrograms\n“Cancer” and government_healthcare_programs, such as [“Medicare,” “Medicaid,” “ACA,” “NIH,” etc.] or “trust” (HINTS A3c)\n\n\nCancerCharitiesTrust\n“Cancer” and cancer_charities, such as [“American Cancer Society,” “LLS,” “BCRF,” “Livestrong,” etc.] or “trust” (HINTS A3d)\n\n\nCancerReligiousOrgTrust\n“Cancer” and charitable_religious_organizations, such as [“Catholic Relief Services,” “World Vision,” “Salvation Army,” etc.] or “trust” (HINTS A3e)\n\n\nCancerScientistsTrust\n“Cancer” and top_cancer_institutes, such as [“MD Anderson,” “Mayo Clinic,” “UCSF,” “Roswell Park,” etc.] or “trust” (HINTS A3f)\n\n\nInsuranceCancer\n“Insurance” and “cancer”\n\n\nMedicareCancer\n“Medicare” and “cancer”\n\n\nMedicaidCancer\n“Medicaid” and “cancer”"
  },
  {
    "objectID": "eda-nlp-reddit.html#what-is-the-sentiment-distribution-across-these-diverse-healthcare-subreddits",
    "href": "eda-nlp-reddit.html#what-is-the-sentiment-distribution-across-these-diverse-healthcare-subreddits",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What is the sentiment distribution across these diverse healthcare subreddits?",
    "text": "What is the sentiment distribution across these diverse healthcare subreddits?\nPurpose: To understand how different healthcare communities perceive their situations, we analyzed the sentiment distribution across various healthcare-related subreddits by categorizing each comment as positive, negative, or neutral based on its sentiment score. Positive scores indicate optimistic or supportive comments, negative scores reflect frustration or concern, and neutral scores suggest factual or balanced tones. This analysis provides a snapshot of the emotional climate across healthcare subreddits, helping us gauge the general sentiment expressed by members of each community.\nOutcome: It is seen that there is right skewed distribution for ‘negative’ and positive has a left-skewed distribution with an expection for ‘WomensHealth’ subreddit. The counts on an overall scale were slightly more for negative than the positive. However, as this doesn’t specifically answer our questions in great depth, we explore further.\n\n\nCode\nsid = SentimentIntensityAnalyzer()\n\n# sentiment score and label\ndf['sentiment_score'] = df['body'].apply(lambda x: sid.polarity_scores(x)['compound'])\ndf['sentiment_label'] = df['sentiment_score'].apply(lambda x: 'positive' if x &gt; 0 else 'negative' if x &lt; 0 else 'neutral')\n#custom_palette = sns.color_palette(\"Set2\", n_colors=len(df['subreddit'].unique()))\n#plt.figure(figsize=(15, 7))\n#sns.countplot(data=df, x='sentiment_label', hue='subreddit', palette=custom_palette)\n#plt.title('Sentiment Distribution by Subreddit')\n#plt.xticks(rotation=90)\n#plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1))\n#plt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 64, 15, Finished, Available, Finished)"
  },
  {
    "objectID": "eda-nlp-reddit.html#which-emotions-are-most-common-in-specific-healthcare-communities",
    "href": "eda-nlp-reddit.html#which-emotions-are-most-common-in-specific-healthcare-communities",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "Which emotions are most common in specific healthcare communities?",
    "text": "Which emotions are most common in specific healthcare communities?\nThe goal of this analysis is to identify the most common emotions, such as sadness, anger, and trust, within different healthcare communities on Reddit. By examining the emotional tone of discussions in each subreddit, we can gain insights into the unique challenges and concerns faced by members of these communities. Understanding predominant emotions in specific subreddits helps reveal the emotional landscape of each group, highlighting areas of support, frustration, or other significant feelings that characterize their experiences.\nNote: Used three approaches here 1. TextBlob 2. Vader 3. Transformers\nOutcome: As we have usd three approaches to judge polarity, sentiment, and emotion, it’ll be broken down accordingly\n\nOn basis of Polarity: The analysis reveals varying predominant emotions across healthcare subreddits, with each community displaying unique emotional tones. Subreddits like braincancer and skincancer show notably high positive sentiment, possibly reflecting resilience or hope within these groups, while Fuckcancer and Hashimotos indicate stronger expressions of frustration or anger. Communities like CancerCaregivers and HealthInsurance have a more balanced emotional tone, with a mix of support and concerns.\nThe sentiment analysis across healthcare subreddits shows a wide range of emotional tones. Subreddits like braincancer, predental, and HealthInsurance exhibit higher positive sentiment, possibly reflecting optimism or supportive environments. In contrast, subreddits such as UlcerativeColitis, Lymphedema, and leukemia lean more negative, indicating challenging or difficult discussions. Many subreddits, including breastcancer and cancer, have a high proportion of neutral sentiment, suggesting factual or balanced discussions.\nThe analysis of emotions across healthcare subreddits reveals a diverse emotional landscape, with certain emotions being more prominent in specific communities. For example, sadness is particularly high in subreddits such as Cancersurvivors, BRCA, and predental, likely reflecting the emotional challenges related to these conditions. Fear is notably prevalent in subreddits like skincancer, thyroidcancer, and publichealth, indicating concerns over diagnosis, treatment, and potential health risks. On the other hand, joy is relatively high in WomensHealth and braincancer, suggesting instances of shared positive experiences or supportive interactions. Anger is particularly pronounced in subreddits such as Fuckcancer and Microbiome, possibly reflecting frustration with healthcare systems, treatments, or conditions.\n\n\n\nCode\n# using pretained transformeers model\n\n#MAX_LENGTH = 512\n\n#def truncate_text(text, max_length=MAX_LENGTH):\n #   tokens = text.split()  # Tokenize by whitespace\n  #  if len(tokens) &gt; max_length:\n   #     text = ' '.join(tokens[:max_length])\n    #return text\n\n#df['truncated_body'] = df['body'].apply(lambda x: truncate_text(x))\n\n# loading a pretrained emotion classification model\n#emotion_classifier = pipeline(\n #   \"text-classification\", \n  #  model=\"j-hartmann/emotion-english-distilroberta-base\", \n   # return_all_scores=True, \n   # truncation=True\n#)\n#def get_emotions_transformers(text):\n #   emotion_scores = emotion_classifier(text)[0]  # emootional scores\n    # converting these list of scores to a dictionary\n  #  return {item['label']: item['score'] for item in emotion_scores}\n\n#emotion_df = pd.DataFrame(df['body'].apply(get_emotions_transformers).tolist())\n##emotion_df['subreddit'] = df['subreddit']\n#emotion_avg = emotion_df.groupby('subreddit').mean()\n\n#plt.figure(figsize=(15, 8))\n#sns.heatmap(emotion_avg, annot=True, cmap='coolwarm')\n#plt.title('Average Emotion Scores by Subreddit')\n#plt.show()\n\n\n\n\nCode\n## using vader for emotional scoring\n\n# VADER\n#sid = SentimentIntensityAnalyzer()#\n\n#def get_vader_scores(text):\n #   scores = sid.polarity_scores(text)\n #   return {\n  #      'positive': scores['pos'],\n   #     'negative': scores['neg'],\n    #    'neutral': scores['neu'],\n     #   'compound': scores['compound']\n    #}\n\n#vader_df = pd.DataFrame(df['body'].apply(get_vader_scores).tolist())\n#vader_df['subreddit'] = df['subreddit']\n#vader_avg = vader_df.groupby('subreddit').mean()\n\n#plt.figure(figsize=(15, 8))\n#sns.heatmap(vader_avg, annot=True, cmap='coolwarm')\n#plt.title('Average VADER Sentiment Scores by Subreddit')\n#plt.show()\n\n\n\n\nCode\n# using textblob for sentiment analysis as a proxy for emotion\n\n#def get_sentiment(text):\n #   blob = TextBlob(text)\n  #  return {\n  #      'polarity': blob.sentiment.polarity,       # -1 (negative) to 1 (positive)\n  #      'subjectivity': blob.sentiment.subjectivity # 0 (objective) to 1 (subjective)\n  #  }\n\n#sentiment_df = pd.DataFrame(df['body'].apply(get_sentiment).tolist())\n#sentiment_df['subreddit'] = df['subreddit']\n#sentiment_avg = sentiment_df.groupby('subreddit').mean()\n\n#plt.figure(figsize=(15, 8))\n#sns.heatmap(sentiment_avg, annot=True, cmap='coolwarm')\n#plt.title('Average Sentiment by Subreddit')\n#plt.show()"
  },
  {
    "objectID": "eda-nlp-reddit.html#what-are-the-primary-topics-or-themes-discussed-in-each-subreddit",
    "href": "eda-nlp-reddit.html#what-are-the-primary-topics-or-themes-discussed-in-each-subreddit",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What are the primary topics or themes discussed in each subreddit?",
    "text": "What are the primary topics or themes discussed in each subreddit?\nPurpose: The goal of analyzing primary topics or themes across healthcare subreddits is to identify common areas of discussion within each community. These themes could include treatment challenges, symptom management, and the need for emotional support, among others. By understanding the primary concerns in each subreddit, we can gain insights into what matters most to different healthcare communities. Additionally, exploring correlations between these topics and sentiment may reveal how specific discussions align with positive, negative, or neutral emotions, providing a deeper understanding of each community’s unique experiences and perspectives.\nOutcome: The topic analysis across healthcare subreddits reveals a variety of key themes discussed within these communities. Topic 1 centers around understanding medical treatments, particularly radiation and prostate-related issues, suggesting a need for clarity on complex procedures. Topic 2 highlights patient-doctor relationships and the desire for reassurance regarding symptoms like lumps, indicating some mistrust or frustration with healthcare providers. Topic 3 reflects general concerns about treatment and care, as well as the challenges of navigating personal health decisions. Topic 4 focuses on experiences with pain and family dynamics, with mentions of “mom” and “dad,” suggesting that family support plays a significant role in coping with diagnoses. Lastly, Topic 5 discusses life-impacting elements of treatment, like blood work, chemotherapy, and managing side effects.\n\n\nCode\nvectorizer = CountVectorizer(max_df=0.9, min_df=10, stop_words='english')\ndtm = vectorizer.fit_transform(df['body'])\nlda = LatentDirichletAllocation(n_components=5, random_state=42)\nlda.fit(dtm)\n\n# topic modeling using LDA \nfor idx, topic in enumerate(lda.components_):\n    print(f\"Topic {idx + 1}:\")\n    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 61, 21, Finished, Available, Finished)\n\n\nTopic 1:\n['time', 'need', 'make', 'youre', 'thats', 'think', 'people', 'dont', 'like', 'just']\nTopic 2:\n['es', 'na', 'da', 'die', 'se', 'la', 'en', 'yes', 'que', 'thank']\nTopic 3:\n['really', 'did', 'ive', 'good', 'know', 'time', 'dont', 'like', 'just', 'im']\nTopic 4:\n['terms', 'question', 'automatically', 'concerns', 'subreddit', 'questions', 'does', 'removed', 'post', 'information']\nTopic 5:\n['thanks', 'post', 'concerns', 'performed', 'automatically', 'bot', 'action', 'moderators', 'contact', 'questions']"
  },
  {
    "objectID": "eda-nlp-reddit.html#are-certain-subreddits-more-likely-to-express-interpersonal-conflict-or-emotional-support",
    "href": "eda-nlp-reddit.html#are-certain-subreddits-more-likely-to-express-interpersonal-conflict-or-emotional-support",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "Are certain subreddits more likely to express interpersonal conflict or emotional support?",
    "text": "Are certain subreddits more likely to express interpersonal conflict or emotional support?\nPurpose: This analysis aims to identify which subreddits are more likely to feature discussions about interpersonal conflict or emotional support, highlighting the role of community dynamics in health-related experiences. By examining the frequency of keywords associated with conflict—such as negative emotions, struggles, and relational challenges—and support keywords that signify kindness, empathy, and encouragement, we can determine which communities focus more on sharing challenges versus offering support.\nOutcome: The analysis of conflict and support mentions across healthcare subreddits reveals varying dynamics in community interactions. Subreddits like Autoimmune, CrohnsDisease, Fuckcancer, and HealthInsurance exhibit high levels of both conflict and support, indicating that members frequently discuss frustrations as well as offer mutual encouragement. In contrast, communities such as Cancersurvivors, Lymphedema, and PreCervicalCancer have high support but little conflict, suggesting a stronger focus on encouragement and empathy rather than interpersonal challenges. Other subreddits like BRCA, Hashimotos, and Microbiome display high conflict with minimal support, potentially reflecting specific challenges or frustrations unique to those conditions.\n\n\nCode\nconflict_keywords = [\n    \"frustration\", \"disappointment\", \"anger\", \"resentment\", \"annoyance\", \"conflict\", \n    \"tension\", \"irritation\", \"betrayal\", \"arguments\", \"struggle\", \"stress\", \n    \"misunderstanding\", \"criticism\", \"blame\", \"guilt\", \"hostility\", \"resent\", \n    \"unhappy\", \"sadness\", \"upset\", \"hurt\", \"fighting\", \"argument\", \"tough\", \n    \"challenging\", \"difficulty\", \"pressure\", \"discouragement\", \"despair\", \n    \"fear\", \"helplessness\", \"hopelessness\", \"loss\", \"disapproval\", \"rejection\", \n    \"grief\", \"pain\", \"suffering\", \"alienation\", \"abandonment\", \"desperation\", \n    \"friction\", \"dispute\", \"distrust\", \"confusion\", \"embarrassment\", \"injustice\", \n    \"unfair\", \"isolation\", \"loneliness\", \"agitation\", \"disgust\", \"irritability\", \n    \"exclusion\", \"condemnation\", \"bitterness\", \"exasperation\", \"discontent\", \n    \"jealousy\", \"envy\", \"regret\", \"doubt\", \"turmoil\", \"burden\", \"anxiety\", \n    \"pressure\", \"insecurity\", \"worry\", \"distress\", \"disillusionment\", \"strife\", \n    \"unresolved\", \"torment\", \"persecution\", \"victimization\", \"abrasiveness\", \n    \"defiance\", \"rebellion\", \"protest\", \"contempt\", \"accusation\", \"attack\", \n    \"judgment\", \"exclusion\", \"neglect\", \"withdrawal\", \"abuse\", \"mockery\", \"clash\"\n] # added more words to balance the viz\n\nsupport_keywords = [\n    \"support\", \"help\", \"care\", \"encourage\", \"comfort\", \"kindness\", \"understanding\", \n    \"compassion\", \"empathy\", \"assistance\", \"guidance\", \"backing\", \"aid\", \"uplifting\", \n    \"positive\", \"friendship\", \"community\", \"reassurance\", \"sympathy\", \"solidarity\", \n    \"together\", \"listening\", \"patience\", \"trust\", \"love\", \"appreciation\", \"encouragement\", \n    \"advice\", \"warmth\", \"respect\", \"hope\", \"motivating\", \"strength\", \"connection\"\n]\n\ndf['conflict_mention'] = df['body'].apply(lambda x: any(keyword in x for keyword in conflict_keywords))\ndf['support_mention'] = df['body'].apply(lambda x: any(keyword in x for keyword in support_keywords))\n\n# Aggregate counts\nconflict_counts = df.groupby('subreddit')['conflict_mention'].sum().sort_values(ascending = False)\nsupport_counts = df.groupby('subreddit')['support_mention'].sum().sort_values(ascending = False)\n\n# Select top 50 reddit\ntop_conflict_subreddit = conflict_counts.head(40).index\ntop_support_subreddit = support_counts.head(40).index\n\n# combine two sets \ntop_subreddit = set(top_conflict_subreddit).union(set(top_support_subreddit))\n\n# Filter Df for these subreddit \nfiltered_df = df[df['subreddit'].isin(top_subreddit)]\nfiltered_conflict_counts = filtered_df.groupby('subreddit')['conflict_mention'].sum()\nfiltered_support_counts = filtered_df.groupby('subreddit')['support_mention'].sum()\n\nconflict_values = filtered_conflict_counts.values\nsupport_values = filtered_support_counts.values\nsubreddit_labels = filtered_conflict_counts.index\n\n# Add theme \nhex_colors = {\n    'Conflict Mentions' : \"#ff4500\", \n    'Support Mentions': \"#5f0922\"\n}\n\n# Prepare data for dodge plot\ndodge_data = pd.DataFrame({\n    'subreddit': filtered_conflict_counts.index.tolist() + filtered_support_counts.index.tolist(),\n    'mention_type': ['Conflict Mentions'] * len(filtered_conflict_counts) + ['Support Mentions']  * len(filtered_support_counts),\n    'count': filtered_conflict_counts.tolist() + filtered_support_counts.tolist()\n})\n\n# Plot using Seaborn\nplt.figure(figsize=(15, 5))\nsns.barplot(data = dodge_data, x='subreddit', y= 'count', hue = 'mention_type', palette=  hex_colors)\nplt.title('Frequency of Conflict and Support Mentions by Subreddit')\nplt.xlabel('Subreddit')\nplt.ylabel('Mentions Count')\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 64, 63, Finished, Available, Finished)"
  },
  {
    "objectID": "eda-nlp-reddit.html#how-does-sentiment-vary-over-time-within-different-healthcare-communities",
    "href": "eda-nlp-reddit.html#how-does-sentiment-vary-over-time-within-different-healthcare-communities",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "How does sentiment vary over time within different healthcare communities?",
    "text": "How does sentiment vary over time within different healthcare communities?\nPurpose: The goal of this analysis is to examine how sentiment fluctuates over time within different healthcare communities. By identifying temporal trends in sentiment, we can uncover patterns that may reflect the impact of external events or stages in personal health journeys. For instance, certain healthcare subreddits might experience shifts in sentiment during awareness months, treatment advancements, or policy changes, while others may show personal emotional cycles as members navigate ongoing health challenges.\nOutcome: As there is no pull of data across time, we are leaving this to work on it later\n\n\nCode\ndf['date'] = df['created_utc'].dt.date\n#print(df['date'].unique()) # so didnt work so can we expand the data? \n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 64, 13, Finished, Available, Finished)"
  },
  {
    "objectID": "eda-nlp-reddit.html#what-is-the-average-sentiment-score",
    "href": "eda-nlp-reddit.html#what-is-the-average-sentiment-score",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What is the average sentiment score?",
    "text": "What is the average sentiment score?\nPurpose: The purpose of calculating the average sentiment score is to understand the general emotional tone across different subreddits. By examining the average sentiment for each subreddit, we can gain insights into whether discussions within these communities tend to be positive, negative, or neutral overall. This analysis helps identify which healthcare communities are more optimistic or challenged in their outlook, offering a broader view of the emotional landscape within each subreddit.\nOutcome: The sentiment analysis for healthcare subreddits on July 1, 2024, reveals a range of emotional tones across communities. Subreddits like CancerCaregivers (0.737608) and predental (0.9921) have positive sentiment scores, suggesting a supportive or encouraging atmosphere among members. On the other hand, communities like BRCA (-0.9525), Cancersurvivors (-0.9826), and CrohnsDisease (-0.569) show more negative sentiment, potentially indicating challenges, frustrations, or concerns commonly shared by these users. Several subreddits, such as BladderCancer (0.031667) and AskDocs (-0.26722), exhibit scores closer to neutral, implying a balanced mix of positive and negative experiences. This on an overall scale of sentiment scores highlights the diversity of emotional climates within these healthcare communities, reflecting the unique support needs and challenges faced by each group.\n\n\nCode\n# Convert data to a date time just in case \ndf['date'] = pd.to_datetime(df['date'])\n\n# Add new column for month \ndf['month'] = df['date'].dt.to_period('M')\n\n# Aggregate sentiment by month and subreddit\nmonthly_sentiment = df.groupby(['month', 'subreddit'])['sentiment_score'].mean().unstack()\n\n# Filter top 20 subreddit \ntop_subreddits = df['subreddit'].value_counts().head(20).index\nmonthly_sentiment = monthly_sentiment[top_subreddits] \n\n# Add theme \nhex_colors = [\n    \"#3d6469\", \"#ffa205\", \"#ff4500\", \"#08030a\", \"#feeece\", \"#d40637\", \"#5f0922\" \n]\n\nplt.figure(figsize=(15, 8))\nmonthly_sentiment.T.plot(kind='bar', figsize=(15, 8), color=hex_colors[:len(top_subreddits)])\nplt.title('Average Sentiment Score by Subreddit')\nplt.xlabel('Subreddit')\nplt.ylabel('Average Sentiment Score')\nplt.xticks(rotation=90)\nplt.legend(title=\"Subreddit\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 64, 62, Finished, Available, Finished)\n\n\n&lt;Figure size 1500x800 with 0 Axes&gt;"
  },
  {
    "objectID": "eda-nlp-reddit.html#how-is-the-sentiment-score-distribution",
    "href": "eda-nlp-reddit.html#how-is-the-sentiment-score-distribution",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "How is the sentiment score distribution?",
    "text": "How is the sentiment score distribution?\nPurpose: To analyze the distribution of sentiment scores across healthcare subreddits, providing insight into the range of emotions expressed by community members. This analysis helps identify whether discussions tend to lean more positive, negative, or neutral overall and reveals the diversity of emotional experiences within each subreddit. By examining the sentiment score distribution, we gain a clearer picture of the general mood and the emotional dynamics present in these healthcare communities.\nOutcome: The sentiment score distribution across healthcare subreddits reveals significant variability in the emotional tone of discussions. Subreddits like CancerCaregivers and ProstateCancer have a wide range of sentiment scores, extending from highly negative to highly positive, indicating mixed experiences and emotions. Some communities, such as braincancer and predental, show consistently positive scores, suggesting a more supportive or optimistic atmosphere. Conversely, subreddits like BRCA, Cancersurvivors, and UlcerativeColitis have consistently low sentiment scores, reflecting a generally negative emotional tone. Other subreddits, such as Autoimmune and WomensHealth, display a balanced distribution, with both positive and negative sentiment expressed.\n\n\nCode\n#sns.boxplot(data=df, x='subreddit', y='sentiment_score')\n#plt.xticks(rotation=90)\n#plt.title('Sentiment Score Distribution by Subreddit')\n#plt.show()"
  },
  {
    "objectID": "eda-nlp-reddit.html#is-there-any-association-between-the-comment-length-and-the-sentiment",
    "href": "eda-nlp-reddit.html#is-there-any-association-between-the-comment-length-and-the-sentiment",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "Is there any association between the comment length and the sentiment?",
    "text": "Is there any association between the comment length and the sentiment?\nPurpose: To explore the relationship between comment length and sentiment, as longer comments may reflect more detailed personal experiences or intense emotions, while shorter comments might be more factual or express brief support or frustration. By examining this association, we can gain insights into how the depth of engagement in comments correlates with emotional expression across healthcare communities. This analysis may reveal whether individuals expressing strong sentiments tend to elaborate more on their experiences compared to those with neutral or less intense sentiments.\nOutcome:\n\n\nThe average comment length varies widely across healthcare subreddits, reflecting different levels of engagement. Subreddits like HealthInsurance, UlcerativeColitis, and predental have longer comments, suggesting more detailed discussions. In contrast, communities like skincancer, pancreaticcancer, and Health have shorter comments, likely indicating brief or factual exchanges. These variations highlight differences in the depth of interaction within each community.\n\n\nIn terms of the association between commenth length and sentiment score, though it is a visual scatterplot, there is not much association betweent the both. The length of 100 for instance, has sentiment scores between -1 and +1, and also for longer comment lengths. But as the length of the comment increased, the sentiment scores are at it’s extreme or rather more pronounced.\n\n\n\n\nCode\n# lengh of comments\n#df['comment_length'] = df['body'].apply(lambda x: len(x.split()))\n\n# avg comment length by subreddit\n#avg_length_per_subreddit = df.groupby('subreddit')['comment_length'].mean()\n\n\n#plt.figure(figsize=(12, 8))\n#avg_length_per_subreddit.plot(kind='bar', color='skyblue')\n#plt.title('Average Comment Length by Subreddit')\n#plt.xlabel('Subreddit')\n#plt.ylabel('Average Comment Length (Words)')\n#plt.xticks(rotation=90)\n#plt.show()\n\n# comment length vs. sentiment score\n#plt.figure(figsize=(10, 6))\n#sns.scatterplot(data=df, x='comment_length', y='sentiment_score', alpha=0.6)\n#plt.title('Comment Length vs. Sentiment Score')\n#plt.xlabel('Comment Length (Words)')\n#plt.ylabel('Sentiment Score')\n#plt.show()"
  },
  {
    "objectID": "eda-nlp-reddit.html#how-does-the-engagement-differ-across-different-subreddits",
    "href": "eda-nlp-reddit.html#how-does-the-engagement-differ-across-different-subreddits",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "How does the engagement differ across different subreddits?",
    "text": "How does the engagement differ across different subreddits?\nPurpose: To understand how engagement varies across different subreddits, potentially highlighting topics that strongly resonate with the community or address relevant, relatable issues. Here, we analyze engagement using metrics like ‘controversiality’ and ‘mentions’ to uncover patterns in community response.\nOutcomes: - Using Controversiality: The data reveals that most subreddits exhibit low to zero levels of “controversiality,” indicating minimal engagement or conflict in community interactions for most topics. However, a few subreddits, such as lymphoma and nursing, show slightly higher values, suggesting that discussions in these communities may spark more diverse opinions or higher engagement. - Using “Conflict” and “Support Mentions”: This shows significant variation in community dynamics. Subreddits like Autoimmune, CrohnsDisease, Fuckcancer, HealthInsurance, and braincancer have high percentages for both conflict and support mentions, suggesting these communities frequently discuss both interpersonal challenges and offer mutual support. On the other hand, subreddits like Cancersurvivors and CancerCaregivers reflect a high level of support mentions but little conflict, possibly indicating these communities focus more on encouragement and less on contentious discussions. Subreddits such as thyroidcancer and testicularcancer have moderate levels of both conflict and support, showing a balanced mix of challenges and supportive exchanges. Conversely, topics like Lymphedema, WomensHealth, and skincancer show lower engagement in conflict or support discussions, which might indicate less emphasis on community interaction for these specific conditions.\n\n\nCode\n# avg controversiality per subreddit\n#avg_controversiality_per_subreddit = df.groupby('subreddit')['controversiality'].mean()\n#plt.figure(figsize=(12, 8))\n#avg_controversiality_per_subreddit.plot(kind='bar', color='lightcoral')\n#plt.title('Average Controversiality by Subreddit')\n#plt.xlabel('Subreddit')\n#plt.ylabel('Average Controversiality')\n#plt.xticks(rotation=90)\n#plt.show()"
  },
  {
    "objectID": "eda-nlp-reddit.html#what-are-the-most-common-words-or-phrases-for-positive-versus-negative-comments",
    "href": "eda-nlp-reddit.html#what-are-the-most-common-words-or-phrases-for-positive-versus-negative-comments",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What are the most common words or phrases for positive versus negative comments?",
    "text": "What are the most common words or phrases for positive versus negative comments?\nPurpose: Identifying the most common words in positive versus negative comments provides insight into the experiences and attitudes within the community. This analysis helps uncover topics, concerns, and emotions associated with each sentiment, offering a more nuanced understanding of the community’s overall mood and focus.\nOutcome: The most common words within positive and negative comments, we observe frequent use of personal pronouns like “I,” “my,” and “you,” suggesting that these comments are often personal narratives or responses directed toward others. Words like “cancer,” “treatment,” and “was” appear across both sentiment categories, highlighting the centrality of health experiences in these discussions. In positive comments, expressions such as “to,” “and,” and “but” indicate explanatory language, often associated with sharing advice or encouragement. Conversely, negative comments include words like “not” and “so,” hinting at expressions of dissatisfaction, frustration, or challenges.\n\n\nCode\nfrom wordcloud import STOPWORDS\nimport random\n# Add stopwords\nextra_stopwords = ['reddit', 'post', 'will', 'one', \"comment\", \"thread\", \"subreddit\", \n    \"question\", \"bot\", \"removed\", \"edit\", 'day', 'now', 'youre', 'year', 'much', 'say',\n    \"contact\", \"message\", \"please\", \"thank\", \"moderator\", \"compose\", \"post\", 'didnt',\n    \"https\", \"com\", \"automatically\", \"performed\", \"just\", \"like\", \"really\", 'time', 'ive',\n    \"know\", \"think\", \"don\", \"does\", \"want\", \"make\", \"take\", \"said\", \"got\", 'go',\n    \"going\", \"see\", \"look\", \"people\", \"thing\", \"way\", \"good\", \"help\", \"you\", 'thats',\n    \"i'm\", \"they\", \"we\", \"he\", \"she\", \"it\", \"my\", \"our\", \"their\", \"that\", 'need',\n    \"this\", \"these\", \"those\", \"and\", \"but\", \"or\", \"if\", \"im\", \"dont\", \"moderators\", \"even\", \"still\"]\ncustom_stopwords = set(STOPWORDS)\ncustom_stopwords.update(extra_stopwords)\n\n# positive vs negative\npositive_comments = ' '.join(df[df['sentiment_score'] &gt; 0]['body'])\nnegative_comments = ' '.join(df[df['sentiment_score'] &lt; 0]['body'])\n\n# Add theme \nhex_colors = [\n    \"#3d6469\", \"#ffa205\", \"#ff4500\", \"#08030a\", \"#d40637\", \"#5f0922\"\n]\n\n# Define custom color function \ndef custom_color(word, font_size, position, orientation, random_state = None, **kwargs):\n    return random.choice(hex_colors)\n\nplt.figure(figsize=(15, 7))\n\n# positive\nplt.subplot(1, 2, 1)\nwordcloud = WordCloud(background_color='white', stopwords= custom_stopwords, color_func= custom_color).generate(positive_comments)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Positive Comments')\n\n# negative\nplt.subplot(1, 2, 2)\nwordcloud = WordCloud(background_color='white', stopwords= custom_stopwords, color_func= custom_color).generate(negative_comments)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Negative Comments')\nplt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 64, 66, Finished, Available, Finished)"
  },
  {
    "objectID": "eda-nlp-reddit.html#are-there-specific-times-of-day-or-days-of-the-week-when-users-discuss-certain-emotions-more-frequently",
    "href": "eda-nlp-reddit.html#are-there-specific-times-of-day-or-days-of-the-week-when-users-discuss-certain-emotions-more-frequently",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "Are there specific times of day or days of the week when users discuss certain emotions more frequently?",
    "text": "Are there specific times of day or days of the week when users discuss certain emotions more frequently?\nPurpose: To identify patterns in the timing of emotional discussions, revealing when users are most active or in need of support. This can help understand community dynamics and determine whether certain emotions are more prevalent at specific times or days, providing insights into user engagement and support needs.\nOutcome: As it is only for one day, we can only do it across the hour. The graph presents sentiment scores across different hours on Monday, indicating variations in sentiment throughout the day. For example, positive sentiment peaks around 8 am and 12 pm, as reflected by scores of 0.377 and 0.253 respectively, while negative sentiment is more prominent at 6 am, 9 am, and 5 pm, with values like -0.664, -0.504, and -0.953. Though it would be better to do it across time.\n\n\nCode\n# day of the week and hour extraction\ndf['day_of_week'] = df['created_utc'].dt.day_name()\ndf['hour'] = df['created_utc'].dt.hour\n\n#  verage sentiment score by day of week and hour\nemotion_by_time = df.groupby(['day_of_week', 'hour'])['sentiment_score'].mean().unstack()\n\n# heatmap\nplt.figure(figsize=(15, 8))\nsns.heatmap(emotion_by_time, cmap='coolwarm', annot=True, fmt=\".2f\")\nplt.title('Average Sentiment Score by Day and Hour')\nplt.xlabel('Hour of the Day')\nplt.ylabel('Day of the Week')\nplt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 61, 35, Finished, Available, Finished)"
  },
  {
    "objectID": "eda-nlp-reddit.html#is-there-a-difference-in-sentiment-or-emotion-between-users-who-post-frequently-versus-those-who-post-infrequently",
    "href": "eda-nlp-reddit.html#is-there-a-difference-in-sentiment-or-emotion-between-users-who-post-frequently-versus-those-who-post-infrequently",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "Is there a difference in sentiment or emotion between users who post frequently versus those who post infrequently?",
    "text": "Is there a difference in sentiment or emotion between users who post frequently versus those who post infrequently?\nPurpose: To explore whether frequent posters differ in sentiment or emotional expression compared to infrequent posters, potentially indicating deeper engagement or ongoing health concerns. Since unique user IDs aren’t available, comment length will be used as a proxy for engagement, with longer comments representing more involved or frequent contributors.\nOutcome: The sentiment scores suggest that frequent posters have a generally neutral sentiment with a slight positive inclination (-0.011), whereas infrequent posters lean more negative with a sentiment score of -0.407. This indicates that users who post more often may express more balanced or slightly positive emotions, while those who post infrequently tend to express more negativity in their comments. This could reflect a higher level of involvement or support within the community for frequent posters, while infrequent posters may only engage during times of distress or negative experiences.\n\n\nCode\n# comment length in terms of word count\ndf['comment_length'] = df['body'].apply(lambda x: len(x.split()))\n\n# comment length threshold (e.g., more than 50 words as 'detailed' but can edit this\ndf['engagement_type'] = df['comment_length'].apply(lambda x: 'Frequent' if x &gt; 50 else 'Infrequent')\n\n# average sentiment score by engagement type\navg_sentiment_engagement = df.groupby('engagement_type')['sentiment_score'].mean()\nplt.figure(figsize=(8, 6))\navg_sentiment_engagement.plot(kind='bar', color=['skyblue', 'salmon'])\nplt.title('Average Sentiment Score by Engagement Type (Proxy)')\nplt.xlabel('Engagement Type (Proxy)')\nplt.ylabel('Average Sentiment Score')\nplt.xticks(rotation=0)\nplt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 61, 34, Finished, Available, Finished)"
  },
  {
    "objectID": "eda-nlp-reddit.html#do-specific-healthcare-related-terms-or-topics-e.g.-treatment-diagnosis-insurance-correlate-with-particular-emotions",
    "href": "eda-nlp-reddit.html#do-specific-healthcare-related-terms-or-topics-e.g.-treatment-diagnosis-insurance-correlate-with-particular-emotions",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "Do specific healthcare-related terms or topics (e.g., “treatment,” “diagnosis,” “insurance”) correlate with particular emotions?",
    "text": "Do specific healthcare-related terms or topics (e.g., “treatment,” “diagnosis,” “insurance”) correlate with particular emotions?\nPurpose: To explore whether specific healthcare-related terms or topics, like “treatment,” “diagnosis,” or “insurance,” are associated with distinct emotions. For example, discussions around insurance may evoke frustration, while conversations about successful treatments could elicit feelings of relief. Understanding these correlations can provide insight into the emotional landscape surrounding different aspects of healthcare within the community.\nOutcome: The sentiment scores across various healthcare subreddits show notable variation. Subreddits like CancerCaregivers and WomensHealth display high positive sentiment, potentially reflecting supportive environments or success stories within these communities. Conversely, subreddits such as UlcerativeColitis and Cancersurvivors show significantly negative sentiment scores, possibly indicating the challenges or frustrations often shared in these groups. Topics like HealthInsurance and thyroidcancer have more moderate positive sentiments, while pancreaticcancer and publichealth lean negative, suggesting complex or distressing issues discussed in these areas. This variety in sentiment scores highlights the diverse emotional landscapes within different healthcare communities.\n\n\nCode\n# filtweing comments with healthcare terms \nhealthcare_terms = [\"treatment\", \"diagnosis\", \"insurance\"] # can edit this\ndf['mentions_healthcare'] = df['body'].apply(lambda x: any(term in x.lower() for term in healthcare_terms))\n\n# average sentiment score for comments mentioning healthcare terms\navg_emotion_healthcare = df[df['mentions_healthcare']].groupby('subreddit')['sentiment_score'].mean()\nplt.figure(figsize=(12, 8))\navg_emotion_healthcare.plot(kind='bar', color='purple')\nplt.title('Average Sentiment for Healthcare-Related Comments by Subreddit')\nplt.xlabel('Subreddit')\nplt.ylabel('Average Sentiment Score')\nplt.xticks(rotation=90)\nplt.show()"
  },
  {
    "objectID": "eda-nlp-reddit.html#what-are-the-most-common-expressions-of-gratitude-frustration-or-hope",
    "href": "eda-nlp-reddit.html#what-are-the-most-common-expressions-of-gratitude-frustration-or-hope",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What are the most common expressions of gratitude, frustration, or hope?",
    "text": "What are the most common expressions of gratitude, frustration, or hope?\nPurpose: Identifying the most common expressions of gratitude, frustration, and hope offers valuable insight into the language used for support, shared challenges, and encouragement within healthcare communities. This analysis helps uncover how individuals convey their emotions, revealing the words and phrases that resonate most when expressing appreciation, challenges, or optimism.\nNote: The words are editable\nOutcome: The data reveals that “hope” is the most frequently expressed emotion among the three categories, with 62 mentions. This suggests that individuals within these healthcare communities often express optimism or a desire for positive outcomes. Expressions of gratitude, with 22 mentions, indicate an appreciation for support or shared experiences, reflecting a sense of community and thankfulness. In contrast, “frustration” is mentioned less frequently, with 6 mentions, signaling that while there are challenges, they may be less dominant in the discourse compared to hopeful or thankful sentiments. This analysis, using keywords like “thank you,” “frustrated,” and “hope,” helps highlight the tone of discussions within these communities.\n\n\nCode\n# keywords for each emotion category\n\n# can edit these words to anything else\ngratitude_keywords = [\"thank you\", \"thanks\", \"appreciate\"]\nhope_keywords = [\"hope\", \"optimistic\", \"wish\"]\nfrustration_keywords = [\"frustrated\", \"annoyed\", \"disappointed\"]\n\ndf['gratitude'] = df['body'].apply(lambda x: any(word in x.lower() for word in gratitude_keywords))\ndf['frustration'] = df['body'].apply(lambda x: any(word in x.lower() for word in frustration_keywords))\ndf['hope'] = df['body'].apply(lambda x: any(word in x.lower() for word in hope_keywords))\nemotion_counts = df[['gratitude', 'hope', 'frustration']].sum()\n\n# Add theming \nhex_colors = [\"#3d6469\", \"#ffa205\", \"#5f0922\"] \n\nemotion_counts.plot(kind='bar', color= hex_colors)\nplt.title('Frequency of Expressions of Gratitude, Frustration, and Hope')\nplt.xlabel('Emotion Category')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 64, 46, Finished, Available, Finished)"
  },
  {
    "objectID": "eda-nlp-reddit.html#are-there-differences-in-the-types-of-issues-discussed-by-new-users-versus-long-time-users",
    "href": "eda-nlp-reddit.html#are-there-differences-in-the-types-of-issues-discussed-by-new-users-versus-long-time-users",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "Are there differences in the types of issues discussed by new users versus long-time users?",
    "text": "Are there differences in the types of issues discussed by new users versus long-time users?\nLong-time users may focus on sharing ongoing challenges or offering guidance, whereas new users are likely to seek initial support or information. UserID data would be ideal for accurately identifying new versus long-time users, as it would enable a more precise classification based on user history and posting frequency."
  },
  {
    "objectID": "eda-nlp-reddit.html#what-are-the-patterns-of-conversation-in-highly-controversial-versus-non-controversial-comments",
    "href": "eda-nlp-reddit.html#what-are-the-patterns-of-conversation-in-highly-controversial-versus-non-controversial-comments",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What are the patterns of conversation in highly controversial versus non-controversial comments?",
    "text": "What are the patterns of conversation in highly controversial versus non-controversial comments?\nPurpose: To identify topics that spark strong reactions or disagreements within the community by examining highly controversial comments, providing insight into issues that may lead to divided opinions or heightened sensitivity.\nOutcome: The average sentiment score for highly controversial comments is positive at approximately 0.45, while non-controversial comments tend to have a slightly negative sentiment, averaging around -0.06. This suggests that discussions in controversial comments might evoke a more intense range of emotions, potentially reflecting the engagement or importance of the issues raised.\n\n\nCode\n# avg sentiment for controversial vs. non-controversial comments\navg_sentiment_controversial = df.groupby('controversiality')['sentiment_score'].mean()\nplt.figure(figsize=(8, 6))\navg_sentiment_controversial.plot(kind='bar', color=['gray', 'purple'])\nplt.title('Average Sentiment Score for Controversial vs. Non-Controversial Comments')\nplt.xlabel('Controversiality')\nplt.ylabel('Average Sentiment Score')\nplt.xticks(rotation=0)\nplt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 61, 32, Finished, Available, Finished)"
  },
  {
    "objectID": "eda-nlp-reddit.html#how-do-emotional-responses-change-when-users-discuss-their-own-health-experiences-versus-others-experiences",
    "href": "eda-nlp-reddit.html#how-do-emotional-responses-change-when-users-discuss-their-own-health-experiences-versus-others-experiences",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "How do emotional responses change when users discuss their own health experiences versus others’ experiences?",
    "text": "How do emotional responses change when users discuss their own health experiences versus others’ experiences?\nPurpose To understand if users show distinct emotional responses when sharing their own health experiences versus supporting others, providing insight into how personal versus supportive perspectives influence the type and intensity of emotions expressed. This can help identify patterns of empathy, vulnerability, or encouragement within the community.\nOutcome: The sentiment analysis shows varying emotional responses based on whether users are discussing their own health experiences or others’ experiences. When discussing their own health challenges alone, users express more negative sentiment (average sentiment score of -0.398). In contrast, users who mention both personal and others’ experiences exhibit a slight positive sentiment (0.072). Users focusing solely on others’ experiences show a less negative sentiment (-0.095) compared to personal discussions. This suggests that users may exhibit more vulnerability or frustration when discussing personal challenges, while discussions about others’ experiences might invoke more neutral or positive support-oriented responses.\n\n\nCode\n# using \"my\" and \"your\" personal and other-focused comments\ndf['personal_experience'] = df['body'].apply(lambda x: \"i \" in x.lower() or \"my \" in x.lower())\ndf['other_experience'] = df['body'].apply(lambda x: \"you \" in x.lower() or \"your \" in x.lower())\n\n# avg sentiment for personal vs. other experiences\navg_sentiment_experience = df.groupby(['personal_experience', 'other_experience'])['sentiment_score'].mean()\nplt.figure(figsize=(10, 6))\navg_sentiment_experience.plot(kind='bar', color=['lightcoral', 'lightblue'])\nplt.title('Average Sentiment Score for Personal vs. Other Experiences')\nplt.xlabel('Experience Type')\nplt.ylabel('Average Sentiment Score')\nplt.xticks(rotation=0)\nplt.show()\n\n\nStatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 61, 33, Finished, Available, Finished)"
  },
  {
    "objectID": "nlp.html#tf-idf",
    "href": "nlp.html#tf-idf",
    "title": "NLP",
    "section": "",
    "text": "Given the results of the sentiment analysis, the frequency of popular words in the cancer subreddit were assess using term frequency-inverse document frequency (TF-IDF). This method was used to identify unique or prominent keywords.\n\nDATA PREPARATION\n\n\nTwo separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits.\nThe datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer).\nText content was preprocessed into a column for TF-IDF analysis.\n\n\nTF-IDF VECTORIZATION\n\n\nThe TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets.\nCommon stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents.\n\n\nAGGREGATION & RANKING\n\n\nWords were ranked in descending order of their total TF-IDF scores for the cancer dataset.\nRankings allowed for the identification of keywords in the cancer dataset.\n\n\n\n\nWord\nTF-IDF\n\n\n\n\ntime\n604.333288\n\n\nremoved\n510.072719\n\n\nthank\n508.752837\n\n\ninformation\n491.421062\n\n\ndoctor\n480.772518\n\n\npeople\n467.593201\n\n\nwork\n443.772165\n\n\npatient\n387.074392\n\n\ncancer\n365.678933\n\n\nbest\n340.635342\n\n\n\n\n\n\nTF-IDF\n\n\nIn this word cloud we see words such as time, removed, thank, and information. Some of these words may be used in conversations to talk about cancer care and possibly gratitude from support groups in the Reddit forum."
  },
  {
    "objectID": "nlp.html#sparknlp",
    "href": "nlp.html#sparknlp",
    "title": "NLP",
    "section": "",
    "text": "Two different types of sentiment analysis was performed on both the cancer and non-cancer subreddits. In this method, a pretrained SparNLP pipeline in Azure ML was used to classify the comments has positive, negative, or neutral.\n\n\n\nsentiment_label\ncount_cancer\ncount_non_cancer\n\n\n\n\npositive\n5929\n5715\n\n\nnegative\n4071\n4285\n\n\n\n\nData Loading Comments from cancer-related and non-cancer subreddits were loaded as Parquet files saved in an Azure Blob container.\nSentiment Analysis using SparkNLP Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral.\nWeighted Scores A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment.\nSentiment Labeling Comments were labeled based on their weighted scores (positive, negative, or neutral).\nGrouping and Aggregation The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits.\nStatistical Testing A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories showed a difference in positive and negative sentiment in the cancer and non-cancer subreddits.\n\nThe source code for the sentiment analysis using SparkNLP is here:\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\nResults of Chi-square Test\nChi-square statistic: 9.325853191514204\nP-value: 0.0022594309243996907\nDegrees of freedom: 1\nExpected frequencies: [[5822. 5822.]\n[4178. 4178.]]\nTo assess the difference between positive and negative sentiment in the cancer and non-cancer subreddits the odds ratio (OR) is calculated as:\n\\[Odds Ratio (OR) = \\frac{(\\text{Positive Cancer} / \\text{Positive Non-Cancer})}{(\\text{Negative Cancer} / \\text{Negative Non-Cancer})}\\]\n\\[OR = \\frac{(5929/5715)}{(4071/4285)}\\]\n\\[OR = \\frac{(5929 \\times 4285)}{(5715 \\times 4071)}\\]\n\\[OR = 1.065\\]\n\\[\\beta = ln(OR)\\]\n\\[\\beta = ln(1.065) = 0.063\\]\nThe coefficient indicates the log odds of cancer for the “positive” sentiment label relative to the “negative” sentiment label. A positive value of 0.063 suggests that there is a slight increase in “positive” sentiment in the cancer subreddits compared to the non-cancer subreddits.\nThe p-value of 0.002 confirms a statistical significance. However, with the odds ratio, the positive sentiment is only slightly increased.\nPositive sentiment was higher in the cancer subreddit indicating the these groups may provide support and resiliency in the communities who use these subreddits. In comparison to the rest of Reddit (the non-cancer subreddits), users may look for look and discuss for more information about experience with treatment, logistics, and navigating a cancer diagnosis, fostering a space for encouragement and information."
  },
  {
    "objectID": "nlp.html#nrc-lex",
    "href": "nlp.html#nrc-lex",
    "title": "NLP",
    "section": "",
    "text": "Sentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubreddit\npositive\nnegative\nneutral\nanger\nfear\ndisgust\nsadness\njoy\nsurprise\ntrust\n\n\n\n\nCancer\n0.202806\n0.119079\n0.0\n0.037681\n0.067383\n0.033798\n0.058226\n0.044399\n0.028933\n0.119079\n\n\nNon-Cancer\n0.170739\n0.096125\n0.0\n0.034273\n0.041384\n0.023316\n0.034463\n0.044963\n0.024675\n0.082917"
  },
  {
    "objectID": "ml.html#section",
    "href": "ml.html#section",
    "title": "ML",
    "section": "",
    "text": "Test Performance\nNaive Bayes\nLogistic Regression\n\n\n\n\nAccuracy\n0.775\n0.707\n\n\nPrecision\n0.777\n0.707\n\n\nRecall\n0.775\n0.707\n\n\nF1-Score\n0.775\n0.707\n\n\nAUC-ROC\n0.776\n0.707"
  },
  {
    "objectID": "reddit_ml.html",
    "href": "reddit_ml.html",
    "title": "Sheeba to update",
    "section": "",
    "text": "Sheeba to update"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Our team set out to understand how people navigate and trust health information. Our goal of examining public trust, frustrations, and sentiment about healthcare and cancer information was met - below are our questions and conclusions based on the analysis from previous sections.\n\n\nUsing our two data sources, Subreddits that mention cancer in comments and the Health Information National Trends Survey (HINTS), we were able to find a linkage between sentiment and many of the HINTS responses.\n\n\n\nTrust in scientists, religious institutions, the government, etc. might help us understand where individuals go and how they might feel when they are looking for medical advice or information related to cancer. After our analysis trying to predict Trust In Healthcare, we found that the XGBoost (Tuned) model was the best choice to predict Trust in Healthcare based on its highest accuracy (74%) and performance for High Trust in Healthcare. It had a strong recall of 90%. While this model still struggled with Low Trust in Healthcare, the improved overall performance makes it the most reliable model for the task at hand.\n\n\n\nIn order to investigate the sentiments and emotions in the Reddit data, the comments from cancer-related and non-cancer subreddits were loaded as Parquet files saved in an Azure Blob container. Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral. A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment. Comments were labeled based on their weighted scores (positive, negative, or neutral). The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits. A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories were statistically significant. Bar plots were created using Seaborn and Matplotlib to visualize sentiment distributions for cancer and non-cancer subreddit comments. Our biggest finding on this question was that cancer-related comments have a higher proportion of positive sentiment.\n\n\n\nThis question was target to identify unique or prominent keywords and identify thematic differences in the text-based data. Two separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits. The datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer). Text content was preprocessed into a column for TF-IDF analysis. Then, the TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets. Standard procedures were followed (common stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents). Words were ranked in descending order of their total TF-IDF scores for the cancer dataset. Rankings allowed for the identification of keywords in the cancer dataset. A word cloud was created to display the rank of keywords, with the top two or three were “time” “removed” and “thank”.\n\n\n\nIn this question the Machine Learning concept was classifying cancer-related and non-cancer subreddits using machine learning (Naive Bayes and Logistic Regression). For our approach, we loaded cancer-related and non-cancer subreddit comments and added source column to label as cancer and non. Datasets were combined into a single frame. Text was tokenized and stop words removed, and tokens were converted into raw features using CountVectorizer. The TF-IDF scores were computed using the IDF transformer. Using PySpark ML, created a Naive Bayes and Logistic Regression pipeline with the preprocessing steps. Data was split (80/20) for train and test.\nNaive Bayes outperforms Logistic Regression in text classification\nNaive Bayes:\nMultinomial Naive Bayes achieved test accuracy of 78%. * Precision: 78% * Recall: 78% * F1-Score: 78% * AUC-ROC: 78%\nLogistic Regression: Achieved test accuracy of 71%. * Precision: 71% * Recall: 71% * F1-Score: 71% * AUC-ROC: 71%\nAs this was text data, it was a little unsurprising that Naive Bayes outperformed logistic regression. Though 78% is not the highest test accuracy, it is still a healthy setting above 50% (blind guess)."
  },
  {
    "objectID": "summary.html#takeaways",
    "href": "summary.html#takeaways",
    "title": "Summary",
    "section": "",
    "text": "Subreddits related to cancer revealed a range of emotions, including positivity, negativity, trust, fear, and sadness. These factors suggest that commenters experience complex and multifaceted emotions in their cancer journey.\nReddit is used as a forum to discuss and look for cancer information with keywords, such as doctor, information, and contact.\nHealthcare professionals could use online forum formats to reach the sentiments of patients while researchers could use the data for patient-centered research."
  },
  {
    "objectID": "summary.html#project-approach",
    "href": "summary.html#project-approach",
    "title": "Summary",
    "section": "",
    "text": "Using our two data sources, Subreddits that mention cancer in comments and the Health Information National Trends Survey (HINTS), we were able to find a linkage between sentiment and many of the HINTS responses."
  },
  {
    "objectID": "summary.html#question-1-is-trust-in-the-healtchare-system-influenced-by-other-factors-of-trust",
    "href": "summary.html#question-1-is-trust-in-the-healtchare-system-influenced-by-other-factors-of-trust",
    "title": "Summary",
    "section": "",
    "text": "Trust in scientists, religious institutions, the government, etc. might help us understand where individuals go and how they might feel when they are looking for medical advice or information related to cancer. After our analysis trying to predict Trust In Healthcare, we found that the XGBoost (Tuned) model was the best choice to predict Trust in Healthcare based on its highest accuracy (74%) and performance for High Trust in Healthcare. It had a strong recall of 90%. While this model still struggled with Low Trust in Healthcare, the improved overall performance makes it the most reliable model for the task at hand."
  },
  {
    "objectID": "summary.html#question-2-doe-sentiment-vary-between-cancer-and-non-cancer-subreddits",
    "href": "summary.html#question-2-doe-sentiment-vary-between-cancer-and-non-cancer-subreddits",
    "title": "Summary",
    "section": "",
    "text": "In order to investigate the sentiments and emotions in the Reddit data, the comments from cancer-related and non-cancer subreddits were loaded as Parquet files saved in an Azure Blob container. Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral. A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment. Comments were labeled based on their weighted scores (positive, negative, or neutral). The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits. A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories were statistically significant. Bar plots were created using Seaborn and Matplotlib to visualize sentiment distributions for cancer and non-cancer subreddit comments. Our biggest finding on this question was that cancer-related comments have a higher proportion of positive sentiment."
  },
  {
    "objectID": "summary.html#question-3what-are-the-most-frequently-used-words-in-cancer-subreddits",
    "href": "summary.html#question-3what-are-the-most-frequently-used-words-in-cancer-subreddits",
    "title": "Summary",
    "section": "",
    "text": "This question was target to identify unique or prominent keywords and identify thematic differences in the text-based data. Two separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits. The datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer). Text content was preprocessed into a column for TF-IDF analysis. Then, the TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets. Standard procedures were followed (common stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents). Words were ranked in descending order of their total TF-IDF scores for the cancer dataset. Rankings allowed for the identification of keywords in the cancer dataset. A word cloud was created to display the rank of keywords, with the top two or three were “time” “removed” and “thank”."
  },
  {
    "objectID": "summary.html#question-4-can-we-predict-if-a-comment-will-be-cancer-related",
    "href": "summary.html#question-4-can-we-predict-if-a-comment-will-be-cancer-related",
    "title": "Summary",
    "section": "",
    "text": "In this question the Machine Learning concept was classifying cancer-related and non-cancer subreddits using machine learning (Naive Bayes and Logistic Regression). For our approach, we loaded cancer-related and non-cancer subreddit comments and added source column to label as cancer and non. Datasets were combined into a single frame. Text was tokenized and stop words removed, and tokens were converted into raw features using CountVectorizer. The TF-IDF scores were computed using the IDF transformer. Using PySpark ML, created a Naive Bayes and Logistic Regression pipeline with the preprocessing steps. Data was split (80/20) for train and test.\nNaive Bayes outperforms Logistic Regression in text classification\nNaive Bayes:\nMultinomial Naive Bayes achieved test accuracy of 78%. * Precision: 78% * Recall: 78% * F1-Score: 78% * AUC-ROC: 78%\nLogistic Regression: Achieved test accuracy of 71%. * Precision: 71% * Recall: 71% * F1-Score: 71% * AUC-ROC: 71%\nAs this was text data, it was a little unsurprising that Naive Bayes outperformed logistic regression. Though 78% is not the highest test accuracy, it is still a healthy setting above 50% (blind guess)."
  },
  {
    "objectID": "nlp.html#sources",
    "href": "nlp.html#sources",
    "title": "NLP",
    "section": "",
    "text": "Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\nEmotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon, Saif Mohammad and Peter Turney, In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California."
  },
  {
    "objectID": "data_cleaning.html#introduction",
    "href": "data_cleaning.html#introduction",
    "title": "About the Data and Data Cleaning",
    "section": "",
    "text": "To explore the use of online sources in health communication, two publicly available datasets were used, HINTS and Reddit Pushshift. The Health Information National Trends Survey (HINTS) collects information about the Amercian’s use of cancer-related information. In this project, eleven questions from the HINTS survey were collected for review. These questions focus on gathering information about individuals’ behaviors, trust, and perceptions related to cancer information and health communication. These questions collectively provide insights into the sources and levels of trust Americans place in health and cancer information, their experiences in searching for such information, and their perceptions of the reliability of online and social media health content. HINTS is used to better understand how the American public looks for health information for themselves or their loved ones.\nThe Reddit Pushshift dataset includes free text data from the Reddit online forum where users can post and look for information. It also includes subreddits, which branch off into various subreddits—dedicated communities centered around specific topics. By examining this dataset, this project complements insights from HINTS, offering a unique perspective on how individuals seek, share, and discuss cancer-related health information online. The analysis of Reddit data allows researchers to explore the dynamic and informal exchanges that occur in digital forums to better our understanding of how cancer information is communicated."
  },
  {
    "objectID": "data_cleaning.html#reddit-preparation-of-the-data",
    "href": "data_cleaning.html#reddit-preparation-of-the-data",
    "title": "About the Data and Data Cleaning",
    "section": "Reddit Preparation of the Data",
    "text": "Reddit Preparation of the Data\nThe data was queried from the Reddit Pushshift dataset. Following the themes captured in the HINTs dataset, we performanced an intial eight queries searching for comments that included keywords in each of the questions in the HINTs dataset. The initial query was performed in AWS on a sample of the data. After reviewing some of the comments, all the unique subreddits were found. Searching through these subreddits, we made a list of subreddits that actually included comments about cancer and filtered out any of the subreddits that were not relevant to health at all.\nList of Cancer Subreddits that discussed cancer in the comments.\nsubreddit_list = ['CrohnsDisease', 'thyroidcancer', 'AskDocs', 'UlcerativeColitis', 'Autoimmune', \n              'BladderCancer', 'breastcancer', 'CancerFamilySupport', 'doihavebreastcancer', \n              'WomensHealth', 'ProstateCancer', 'cll', 'Microbiome', 'predental', 'endometrialcancer', \n              'cancer', 'Hashimotos', 'coloncancer', 'PreCervicalCancer', 'lymphoma', 'Lymphedema', \n              'CancerCaregivers', 'braincancer', 'lynchsyndrome', 'nursing', 'testicularcancer', 'leukemia', \n              'publichealth', 'Health', 'Fuckcancer', 'HealthInsurance', 'BRCA', 'Cancersurvivors', \n              'pancreaticcancer', 'skincancer', 'stomachcancer']\nThese subreddits were compared to a random sample from the full Reddit dataset excluding the list of cancer subreddits above. The subreddits were queried in Azure ML using a Spark. The data was copied from the instructor’s Azure blob container.\n\n\nCode\n# Path to the Azure ML Blob Container\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\ncomments_path = \"cancer/comments\"\nsubmissions_path = \"cancer/submissions\"\n\n\nPySpark was used to clean the data by removing leading and trailing whitespaces, removing punctuation (using regex), removing underscores, and converting to lowercase. Both subsets of data were limited to 10,000 rows in order to allow a reasonable compute time for each job. After the data was cleaned it was saved into a two parquet files in an Azure ML blob container to use for the rest of the project. The combined cancer subreddits and non-cancer subreddit totaled in 20,000 rows.\n\n\nCode\n# Cancer subset of Reddit Data saved to an Azure ML Blob Container\noutput_path = f\"{workspace_wasbs_base_url}cancer_subreddit.parquet\"\n\n# Non-cancer subset of Reddit Data saved to an Azure ML Blob Container\noutput_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n\n\nThe source code for the cleaning the Reddit data is in GitHub: fall-2024-project-team-35/code/spark-job-sample-data"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Using Reddit Pushshift to Enrich Understanding of Cancer Communication",
    "section": "",
    "text": "Project Authors: Tiana Le, Sheeba Moghal, Ishaan Babbar, Liz Kovalchuk\nIn the digital age, where information is readily available at our fingertips, understanding how people navigate and trust health information is critical. With countless sources such as healthcare professionals, government agencies, social media, and online forums, it is essential to explore what the United States population believes about the reliability and accessibility of health information. To better understand this phenomenon, data was gathered from an online forum called Reddit. The Reddit dataset comprises textual data that offers valuable insights into the information shared online about health, particularly topics related to cancer. Using NLP methods, we can explore the most common topics and also the sentiment toward these topics. In addition to the textual data, we will also use the Health Information National Trends Survey (HINTS), which is administered by National Cancer Institute. In this survey, people were asked directly about what they think about health information. Using both the Reddit data and HINTS, we seek to understand trust, frustrations, and overall sentiment about healthcare and more specifically, cancer information.\nOur project explores the Pushshift Reddit Dataset to examine trust in healthcare, likely narrowing specifically to r/cancer forums. While there are some assumptions that need to be made to pair our corroborating dataset, the Health Information National Trends Survey (HINTS) survey, with the Reddit dataset, we believe that we compare and use both of these datasets to investigate the following questions:\n\nIs public sentiment toward healthcare and cancer generally positive, or negative, in an online social platform?\nIs there evidence of trust (or lack thereof) when the public discusses healthcare?\nEvidence of other interpersonal conflict in healthcare discussions and emotion / sentiment analysis (e.g. sadness, disgust, surprise, joy, anger)."
  },
  {
    "objectID": "index.html#data-sources",
    "href": "index.html#data-sources",
    "title": "Using Reddit Pushshift to Enrich Understanding of Cancer Communication",
    "section": "",
    "text": "The Health Information National Trends Survey (HINTS) is a significant research initiative launched by the National Cancer Institute (NCI) in 2001. It aims to assess how adults in the United States access and utilize health information with a particular focus on cancer-related topics. HINTS provides a rich source of data that helps researchers understand public interactions with health information, including trends in health communication, knowledge, attitudes, and behaviors regarding cancer prevention and control.\nIt is an excellent starting place for structured questionnaires that would be helfpul to enhance in a “more accessible” format of Reddit data.\nHINTS employs a cross-sectional, nationally representative survey methodology targeting non-institutionalized adults aged 18 years and older. This design allows for the collection of data that reflects the diverse experiences and perspectives of the U.S. population13. Annual Administration: Initially conducted biennially, HINTS has transitioned to an annual survey format. This change enhances the ability to track evolving trends in health information access and utilization over time.\nThe survey seeks information on:\n\nHealth information-seeking behaviors\nCancer prevention and screening practices\nKnowledge and perceptions related to cancer risks\nAccess to healthcare services\nUtilization of technology for health information37.\nPublicly Available Data: HINTS data is freely accessible through its website, allowing researchers, policymakers, and public health professionals to utilize this information for various applications, including the design and evaluation of health communication programs9.\n\nThough HINTS was initially conducted biennially, HINTS has transitioned to an annual survey format. This change enhances the ability to track evolving trends in health information access and utilization over time.\n\n\n\nThe Pushshift Reddit dataset is a comprehensive collection of submissions and comments from Reddit, designed to facilitate social media research by providing easy access to historical data. Launched in 2015, Pushshift has become a crucial resource for researchers interested in analyzing user-generated content on one of the largest social media platforms.\nThe data leveraged from the Reddit dataset was from the provided repository of pre-processed data:\n\nSpans the June-2023 to July-2024 in the 202306-202407 directory\nSpans Jan-2021 to March-2023 in the 202101-202303 directory\n\nThe data queried is of both submissions and comments from the PushShift Dataset."
  },
  {
    "objectID": "index.html#sources",
    "href": "index.html#sources",
    "title": "Using Reddit Pushshift to Enrich Understanding of Cancer Communication",
    "section": "",
    "text": "Finney Rutten, L. J., Arora, N. K., Bakken, S., & Hesse, B. W. (2022). Expanding the Health Information National Trends Survey research: A comparison of health information seeking behaviors across countries. Health Communication, 37(1), 1-10. https://doi.org/10.1080/10810730.2022.2134522\nNational Cancer Institute. (2021). Health Information National Trends Survey (HINTS). Retrieved from https://hints.cancer.gov\nKreps, G. L., & Neuhauser, L. (2020). The health information national trends survey: Research and implications for health communication. PubMed. https://pubmed.ncbi.nlm.nih.gov/33970822/\nZannettou, S., Caulfield, T., & De Cristofaro, E. (2020). The Pushshift Reddit dataset. arXiv. https://doi.org/10.48550/arXiv.2001.08435\nPushshift. (n.d.). Pushshift Reddit dataset. In Papers with Code. Retrieved from https://paperswithcode.com/dataset/pushshift-reddit"
  },
  {
    "objectID": "reddit_ml.html#methodology",
    "href": "reddit_ml.html#methodology",
    "title": "EDA HINTS DATA",
    "section": "Methodology",
    "text": "Methodology\nThe dataset preparation and preprocessing were crucial to ensuring the models had the appropriate features for learning. The raw data from the PushShift Reddit Dataset included text data and category labels for cancer-related and non-cancer comments.\nFirst, the data was combined into a single DataFrame, with an additional column named source labeling the comments as “Cancer” or “Non-Cancer.” The comments were then preprocessed to ensure they were in a suitable format for the models.\nText tokenization was applied to split the comments into individual words, enabling feature extraction. Tokenization was followed by the removal of stop words, such as “and,” “the,” and “is,” which are frequent but do not carry significant information for classification. The tokenized words were then converted into numerical vectors using CountVectorizer, which captures word frequencies. To enhance the importance of rare but meaningful words, TF-IDF (Term Frequency-Inverse Document Frequency) scores were computed. This step ensured that commonly occurring but uninformative words received less weight, while rare and distinctive words were emphasized.\nFinally, the labels (“Cancer” and “Non-Cancer”) were encoded numerically as 0 and 1, respectively, using StringIndexer. These preprocessing steps were implemented as a PySpark pipeline to ensure consistency and scalability.\nThe processed dataset was then split into 80% training and 20% testing subsets. Two machine learning models, Naive Bayes and Logistic Regression, were trained on the training data using PySpark’s built-in implementations. The evaluation metrics, such as accuracy, precision, recall, F1-score, and AUC-ROC, were computed on the test set. Additionally, confusion matrices were generated to visualize the distribution of correct and incorrect predictions for both models.\nLogistic Regression and Naive Bayes were chosen for their simplicity, efficiency, and effectiveness in text classification. Naive Bayes handles high-dimensional data well with its independence assumption, while Logistic Regression provides robust, interpretable probability predictions. These models serve as strong benchmarks for binary classification tasks."
  },
  {
    "objectID": "reddit_ml.html#naive-bayes",
    "href": "reddit_ml.html#naive-bayes",
    "title": "EDA HINTS DATA",
    "section": "Naive Bayes",
    "text": "Naive Bayes"
  }
]