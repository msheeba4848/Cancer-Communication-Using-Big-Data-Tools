[
  {
    "objectID": "reddit_ml.html#methodology",
    "href": "reddit_ml.html#methodology",
    "title": "Machine Learning - Reddit Data",
    "section": "Methodology",
    "text": "Methodology\nThe dataset preparation and preprocessing were crucial to ensuring the models had the appropriate features for learning. The raw data from the PushShift Reddit Dataset included text data and category labels for cancer-related and non-cancer comments.\nFirst, the data was combined into a single DataFrame, with an additional column named source labeling the comments as “Cancer” or “Non-Cancer.” The comments were then preprocessed to ensure they were in a suitable format for the models.\nText tokenization was applied to split the comments into individual words, enabling feature extraction. Tokenization was followed by the removal of stop words, such as “and,” “the,” and “is,” which are frequent but do not carry significant information for classification. The tokenized words were then converted into numerical vectors using CountVectorizer, which captures word frequencies. To enhance the importance of rare but meaningful words, TF-IDF (Term Frequency-Inverse Document Frequency) scores were computed. This step ensured that commonly occurring but uninformative words received less weight, while rare and distinctive words were emphasized.\nFinally, the labels (“Cancer” and “Non-Cancer”) were encoded numerically as 0 and 1, respectively, using StringIndexer. These preprocessing steps were implemented as a PySpark pipeline to ensure consistency and scalability.\nThe processed dataset was then split into 80% training and 20% testing subsets. Two machine learning models, Naive Bayes and Logistic Regression, were trained on the training data using PySpark’s built-in implementations. The evaluation metrics, such as accuracy, precision, recall, F1-score, and AUC-ROC, were computed on the test set. Additionally, confusion matrices were generated to visualize the distribution of correct and incorrect predictions for both models.\nLogistic Regression and Naive Bayes were chosen for their simplicity, efficiency, and effectiveness in text classification. Naive Bayes handles high-dimensional data well with its independence assumption, while Logistic Regression provides robust, interpretable probability predictions. These models serve as strong benchmarks for binary classification tasks."
  },
  {
    "objectID": "reddit_ml.html#naive-bayes",
    "href": "reddit_ml.html#naive-bayes",
    "title": "Machine Learning - Reddit Data",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nRationale\nNaive Bayes is a probabilistic model based on Bayes’ theorem, with the assumption that features are conditionally independent given the label. This model is particularly suitable for text classification tasks, as it performs well when features (words) contribute independently to the prediction.\nIn this task, Naive Bayes classified comments as cancer-related or non-cancer based on word frequencies and their association with the labels. By assuming independence between words, the model simplifies the computation, making it highly efficient.\n\n\nModel Performance\nThe model performance, though wasn’t that great, still created a strong baseline for further modeling analysis. Using the table below\nTable 1 : Performance Metrics for Naive Bayes\n\n\n\nMetric\nValue\n\n\n\n\nTest Precision\n0.777\n\n\nTest Recall\n0.775\n\n\nTest F1-Score\n0.775\n\n\nTest AUC-ROC\n0.776\n\n\n\nThe Naive Bayes model achieved an accuracy of 77.5%, indicating that it correctly classified over three-fourths of the test data. Its precision and recall were similarly high, both at approximately 77.5%, reflecting a good balance between identifying cancer-related comments (true positives) and minimizing false positives (non-cancer comments wrongly classified as cancer). The F1-score, a harmonic mean of precision and recall, was also 77.5%, confirming the model’s consistent performance. The AUC-ROC score of 77.6% highlights the model’s ability to distinguish between the two classes effectively.\n\n\n\nNaive Bayes Classification Confusion Matrix\n\n\nThe confusion matrix provided deeper insights into the model’s strengths and weaknesses. Of the cancer-related comments, 1564 were correctly classified, while 391 were misclassified as non-cancer. Conversely, 1517 non-cancer comments were correctly classified, but 501 were incorrectly predicted as cancer.\nThese false positives suggest that the model occasionally overpredicts the “Cancer” class, possibly due to words or phrases common in both categories. Nevertheless, the relatively low number of false negatives (391) demonstrates that the model is effective at capturing cancer-related content, aligning well with the task’s objective.\nHence, Naive Bayes’s bias toward predicting the “Cancer” class (higher false positives) can be acceptable in scenarios where prioritizing cancer-related content is more important than avoiding false positives."
  },
  {
    "objectID": "reddit_ml.html#logistic-regressio",
    "href": "reddit_ml.html#logistic-regressio",
    "title": "Machine Learning - Reddit Data",
    "section": "Logistic Regressio",
    "text": "Logistic Regressio\n\nRationale\nLogistic Regression is a linear model that predicts the probability of an instance belonging to a particular class by fitting a logistic function (sigmoid curve) to the data. Unlike Naive Bayes, Logistic Regression does not assume independence among features. Instead, it learns the weights of features (words) directly from the data, making it more flexible but also more prone to overfitting in high-dimensional spaces without proper regularization.\n\n\nModel Performance\nThough Logistic Regression underperformed in comparison to Naive Bayes, it was still interesting to look into it’s performance, based on the table below\n\n\n\nMetric\nValue\n\n\n\n\nTest Precision\n0.707\n\n\nTest Recall\n0.707\n\n\nTest F1-Score\n0.707\n\n\nTest AUC-ROC\n0.707\n\n\n\nFor this question, Logistic Regression performed moderately well, achieving an accuracy of 70.7%. Its precision, recall, and F1-score were all consistent at 70.7%, reflecting a balanced performance but slightly lower than that of Naive Bayes. The AUC-ROC score was also 70.7%, indicating that the model struggled more than Naive Bayes in distinguishing cancer-related from non-cancer comments.\n\n\n\nNaive Bayes Classification Confusion Matrix\n\n\nThe confusion matrix highlighted specific challenges faced by Logistic Regression. While it correctly identified 1374 cancer-related comments, it misclassified 581 as non-cancer, a relatively high number of false negatives. Similarly, 1434 non-cancer comments were correctly classified, but 584 were predicted as cancer, resulting in a comparable number of false positives. These results suggest that the model had difficulty capturing the subtle nuances in the text that differentiate the two categories.\nThe lower recall for cancer-related comments (more false negatives) is a significant limitation of Logistic Regression in this context. Missing cancer-related comments is more critical than overclassifying non-cancer content as cancer.\nThis limitation makes Logistic Regression less suitable for the given task, where the focus is on identifying cancer-related content with high confidence. Logistic Regression’s moderate performance can be attributed to its reliance on linear decision boundaries, which may not fully capture the complexities of the text data. While it is a robust and interpretable model, its linear nature and sensitivity to high-dimensional data might have limited its effectiveness in this problem."
  },
  {
    "objectID": "reddit_ml.html#reflection",
    "href": "reddit_ml.html#reflection",
    "title": "Machine Learning - Reddit Data",
    "section": "Reflection",
    "text": "Reflection\nUnderstanding whether a comment is cancer-related is crucial for enhancing online support systems and health communication. Cancer-related subreddits often serve as platforms for individuals seeking emotional support, sharing personal experiences, or discussing treatment options. Automatically identifying such comments can help moderators curate relevant content, ensure timely responses to critical queries, and provide researchers with valuable insights into public sentiment, challenges, and trends related to cancer. This understanding is particularly important for designing interventions, improving healthcare accessibility, and fostering a supportive community for those affected by cancer."
  },
  {
    "objectID": "reddit_ml.html#logistic-regression",
    "href": "reddit_ml.html#logistic-regression",
    "title": "Machine Learning - Reddit Data",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nRationale\nLogistic Regression is a linear model that predicts the probability of an instance belonging to a particular class by fitting a logistic function (sigmoid curve) to the data. Unlike Naive Bayes, Logistic Regression does not assume independence among features. Instead, it learns the weights of features (words) directly from the data, making it more flexible but also more prone to overfitting in high-dimensional spaces without proper regularization.\n\n\nModel Performance\nThough Logistic Regression underperformed in comparison to Naive Bayes, it was still interesting to look into it’s performance, based on the table below\n\n\n\nMetric\nValue\n\n\n\n\nTest Precision\n0.707\n\n\nTest Recall\n0.707\n\n\nTest F1-Score\n0.707\n\n\nTest AUC-ROC\n0.707\n\n\n\nFor this question, Logistic Regression performed moderately well, achieving an accuracy of 70.7%. Its precision, recall, and F1-score were all consistent at 70.7%, reflecting a balanced performance but slightly lower than that of Naive Bayes. The AUC-ROC score was also 70.7%, indicating that the model struggled more than Naive Bayes in distinguishing cancer-related from non-cancer comments.\n\n\n\nNaive Bayes Classification Confusion Matrix\n\n\nThe confusion matrix highlighted specific challenges faced by Logistic Regression. While it correctly identified 1374 cancer-related comments, it misclassified 581 as non-cancer, a relatively high number of false negatives. Similarly, 1434 non-cancer comments were correctly classified, but 584 were predicted as cancer, resulting in a comparable number of false positives. These results suggest that the model had difficulty capturing the subtle nuances in the text that differentiate the two categories.\nThe lower recall for cancer-related comments (more false negatives) is a significant limitation of Logistic Regression in this context. Missing cancer-related comments is more critical than overclassifying non-cancer content as cancer.\nThis limitation makes Logistic Regression less suitable for the given task, where the focus is on identifying cancer-related content with high confidence. Logistic Regression’s moderate performance can be attributed to its reliance on linear decision boundaries, which may not fully capture the complexities of the text data. While it is a robust and interpretable model, its linear nature and sensitivity to high-dimensional data might have limited its effectiveness in this problem."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "About the Data and Data Cleaning",
    "section": "",
    "text": "To explore the use of online sources in health communication, two publicly available datasets were used, HINTS and Reddit Pushshift. The Health Information National Trends Survey (HINTS) collects information about the Amercian’s use of cancer-related information. In this project, eleven questions from the HINTS survey were collected for review. These questions focus on gathering information about individuals’ behaviors, trust, and perceptions related to cancer information and health communication. These questions collectively provide insights into the sources and levels of trust Americans place in health and cancer information, their experiences in searching for such information, and their perceptions of the reliability of online and social media health content. HINTS is used to better understand how the American public looks for health information for themselves or their loved ones.\nThe Reddit Pushshift dataset includes free text data from the Reddit online forum where users can post and look for information. It also includes subreddits, which branch off into various subreddits—dedicated communities centered around specific topics. By examining this dataset, this project complements insights from HINTS, offering a unique perspective on how individuals seek, share, and discuss cancer-related health information online. The analysis of Reddit data allows researchers to explore the dynamic and informal exchanges that occur in digital forums to better our understanding of how cancer information is communicated."
  },
  {
    "objectID": "data_cleaning.html#introduction",
    "href": "data_cleaning.html#introduction",
    "title": "About the Data and Data Cleaning",
    "section": "",
    "text": "To explore the use of online sources in health communication, two publicly available datasets were used, HINTS and Reddit Pushshift. The Health Information National Trends Survey (HINTS) collects information about the Amercian’s use of cancer-related information. In this project, eleven questions from the HINTS survey were collected for review. These questions focus on gathering information about individuals’ behaviors, trust, and perceptions related to cancer information and health communication. These questions collectively provide insights into the sources and levels of trust Americans place in health and cancer information, their experiences in searching for such information, and their perceptions of the reliability of online and social media health content. HINTS is used to better understand how the American public looks for health information for themselves or their loved ones.\nThe Reddit Pushshift dataset includes free text data from the Reddit online forum where users can post and look for information. It also includes subreddits, which branch off into various subreddits—dedicated communities centered around specific topics. By examining this dataset, this project complements insights from HINTS, offering a unique perspective on how individuals seek, share, and discuss cancer-related health information online. The analysis of Reddit data allows researchers to explore the dynamic and informal exchanges that occur in digital forums to better our understanding of how cancer information is communicated."
  },
  {
    "objectID": "data_cleaning.html#hints-preparation-of-data",
    "href": "data_cleaning.html#hints-preparation-of-data",
    "title": "About the Data and Data Cleaning",
    "section": "HINTS Preparation of Data",
    "text": "HINTS Preparation of Data\n\n\nCode\nimport pyreadr\nimport pandas as pd\nfrom IPython.display import display\n\n# Load the .rda file\nresult = pyreadr.read_r('/Users/elizabethkovalchuk/Documents/DSAN6000/Project/fall-2024-project-team-35/data/HINTS6_R_20240524/hints6_public.rda')\n\n# Extract the DataFrame from the loaded data\nhints = result['public']  # Assuming 'public' is the name of the R object in the file\n\n# Specify the columns to select\ncolumns = [\n    \"HHID\", \"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\",\n    \"CancerTrustFamily\", \"CancerTrustGov\", \"CancerTrustCharities\",\n    \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\", \"Electronic2_HealthInfo\",\n    \"MisleadingHealthInfo\", \"TrustHCSystem\"\n]\n\n# Select the relevant columns\nhints_select = hints[columns]\n\n# # Convert the 'updatedate' column if required (commented for now)\n# hints_select['updatedate'] = pd.to_datetime(hints_select['updatedate'] / 1000, unit='s')\n\n# Preview the first few rows\nprint(\"Sample data from the HINTS dataset:\")\ndisplay(hints_select.head())\nprint(f\"Shape of the original dataset: {hints_select.shape}\")\n\n\nSample data from the HINTS dataset:\nShape of the original dataset: (6252, 12)\n\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n21000006\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nQuestion answered in error (Commission Error)\nI do not use social media\nVery\n\n\n1\n21000009\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nSome\nA lot\nSome\nSome\nA lot\nYes\nI do not use social media\nVery\n\n\n2\n21000020\nYes\nSomewhat disagree\nA lot\nSome\nSome\nA little\nNot at all\nA lot\nYes\nSome\nSomewhat\n\n\n3\n21000022\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nInapplicable, coded 2 in UseInternet\nI do not use social media\nSomewhat\n\n\n4\n21000039\nNo\nInapplicable, coded 2 in SeekCancerInfo\nSome\nSome\nSome\nNot at all\nNot at all\nSome\nYes\nA lot\nSomewhat\n\n\n\n\n\n\n\n\n\n\nCode\n# Count missing values in each column\nmissing_values = hints_select.isna().sum()\n\n# Display the count of missing values\nprint(\"Missing values per column:\")\ndisplay(missing_values)\n\n\nMissing values per column:\n\n\nHHID                        0\nSeekCancerInfo              0\nCancerFrustrated            0\nCancerTrustDoctor           0\nCancerTrustFamily           0\nCancerTrustGov              0\nCancerTrustCharities        0\nCancerTrustReligiousOrgs    0\nCancerTrustScientists       0\nElectronic2_HealthInfo      0\nMisleadingHealthInfo        0\nTrustHCSystem               0\ndtype: int64\n\n\n\n\nCode\n# List of ordinal columns\nordinal_columns = [\n    \"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\",\n    \"CancerTrustFamily\", \"CancerTrustGov\", \"CancerTrustCharities\",\n    \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\", \"Electronic2_HealthInfo\",\n    \"MisleadingHealthInfo\", \"TrustHCSystem\"\n]\n\n# Display unique values for each ordinal column\nprint(\"Unique values for ordinal columns:\")\nfor column in ordinal_columns:\n    unique_values = hints_select[column].unique()\n    print(f\"\\nColumn: {column}\")\n    print(f\"Unique Values: {unique_values}\")\n\n\nUnique values for ordinal columns:\n\nColumn: SeekCancerInfo\nUnique Values: ['No', 'Yes', 'Missing data (Not Ascertained)']\nCategories (3, object): ['Missing data (Not Ascertained)', 'No', 'Yes']\n\nColumn: CancerFrustrated\nUnique Values: ['Inapplicable, coded 2 in SeekCancerInfo', 'Somewhat disagree', 'Strongly disagree', 'Somewhat agree', 'Strongly agree', 'Question answered in error (Commission Error)', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (9, object): ['Inapplicable, coded 2 in SeekCancerInfo', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', ..., 'Somewhat agree', 'Somewhat disagree', 'Strongly agree', 'Strongly disagree']\n\nColumn: CancerTrustDoctor\nUnique Values: ['A lot', 'Some', 'Not at all', 'A little', 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustFamily\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'A little', 'Not at all', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustGov\nUnique Values: ['Missing data (Not Ascertained)', 'A lot', 'Some', 'A little', 'Not at all', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustCharities\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'A little', 'Not at all', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustReligiousOrgs\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'Not at all', 'A little', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustScientists\nUnique Values: ['Missing data (Not Ascertained)', 'A lot', 'Some', 'A little', 'Not at all', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: Electronic2_HealthInfo\nUnique Values: ['Question answered in error (Commission Error)', 'Yes', 'Inapplicable, coded 2 in UseInternet', 'No', 'Missing data (Not Ascertained)', 'Missing data (Filter Missing)']\nCategories (6, object): ['Inapplicable, coded 2 in UseInternet', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'No', 'Question answered in error (Commission Error)', 'Yes']\n\nColumn: MisleadingHealthInfo\nUnique Values: ['I do not use social media', 'Some', 'A lot', 'A little', 'None', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se...]\nCategories (7, object): ['A little', 'A lot', 'I do not use social media', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se..., 'None', 'Some']\n\nColumn: TrustHCSystem\nUnique Values: ['Very', 'Somewhat', 'A little', 'Not at all', 'Missing data (Web partial - Question Never Se..., 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (7, object): ['A little', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se..., 'Multiple responses selected in error', 'Not at all', 'Somewhat', 'Very']\n\n\n\n\nCode\n# Define the valid scales for each column\nvalid_scales = {\n    \"CancerFrustrated\": ['Somewhat disagree', 'Strongly disagree', 'Somewhat agree', 'Strongly agree'],\n    \"CancerTrustDoctor\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustFamily\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustGov\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustCharities\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustReligiousOrgs\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustScientists\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"TrustHCSystem\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"Electronic2_HealthInfo\": ['Yes', 'No'], \n    \"MisleadingHealthInfo\": ['I do not use social media', 'None', 'A little', 'Some', 'A lot']  \n}\n\n# Create a copy of the original DataFrame\nhints_cleaned = hints_select.copy()\n\n# Filter the DataFrame\nfor column, scale in valid_scales.items():\n    hints_cleaned = hints_cleaned[hints_cleaned[column].isin(scale)]\n\n# Display the cleaned dataset and its shape\nprint(\"Data after filtering invalid values:\")\ndisplay(hints_cleaned.head())\nprint(f\"Shape of the cleaned dataset: {hints_cleaned.shape}\")\n\n\nData after filtering invalid values:\nShape of the cleaned dataset: (323, 12)\n\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n51\n21000330\nYes\nSomewhat disagree\nSome\nNot at all\nSome\nSome\nNot at all\nA lot\nYes\nA lot\nA little\n\n\n112\n21000976\nYes\nSomewhat agree\nA lot\nSome\nSome\nSome\nSome\nA lot\nYes\nSome\nA little\n\n\n136\n21001112\nYes\nSomewhat disagree\nA little\nA little\nNot at all\nNot at all\nNot at all\nA little\nNo\nA lot\nNot at all\n\n\n157\n21001283\nYes\nSomewhat disagree\nA lot\nSome\nNot at all\nA little\nSome\nNot at all\nNo\nI do not use social media\nNot at all\n\n\n181\n21001548\nYes\nStrongly agree\nA lot\nSome\nNot at all\nSome\nA lot\nA little\nYes\nSome\nA little\n\n\n\n\n\n\n\n\n\n\nCode\n# Count the number of NA or NaN values in each column\nna_count = hints_cleaned.isna().sum()\n#print(\"NA values count per column:\")\n#print(na_count)\n#print(hints_cleaned.shape)\n# Count unique values in the 'SeekCancerInfo' column\nvalue_counts = hints_cleaned['SeekCancerInfo'].value_counts()\n#print(\"Unique value counts in 'SeekCancerInfo':\")\n#print(value_counts)\n\n# Save the cleaned dataset to an Excel file\noutput_file = \"../data/csv/hints_cleaned_forML_spearman.csv\"\nhints_cleaned.to_csv(output_file, index=False)\n\n#print(f\"Cleaned dataset saved as {output_file}\")\n\n\nUnique value counts in 'SeekCancerInfo':\nSeekCancerInfo\nYes                               323\nMissing data (Not Ascertained)      0\nNo                                  0\nName: count, dtype: int64\nCleaned dataset saved as ../data/csv/hints_cleaned_forML_spearman.csv"
  },
  {
    "objectID": "data_cleaning.html#reddit-preparation-of-the-data",
    "href": "data_cleaning.html#reddit-preparation-of-the-data",
    "title": "About the Data and Data Cleaning",
    "section": "Reddit Preparation of the Data",
    "text": "Reddit Preparation of the Data\nThe data was queried from the Reddit Pushshift dataset. Following the themes captured in the HINTs dataset, we performanced an intial eight queries searching for comments that included keywords in each of the questions in the HINTs dataset. The initial query was performed in AWS on a sample of the data. After reviewing some of the comments, all the unique subreddits were found. Searching through these subreddits, we made a list of subreddits that actually included comments about cancer and filtered out any of the subreddits that were not relevant to health at all.\nList of Cancer Subreddits that discussed cancer in the comments.\nsubreddit_list = ['CrohnsDisease', 'thyroidcancer', 'AskDocs', 'UlcerativeColitis', 'Autoimmune', \n              'BladderCancer', 'breastcancer', 'CancerFamilySupport', 'doihavebreastcancer', \n              'WomensHealth', 'ProstateCancer', 'cll', 'Microbiome', 'predental', 'endometrialcancer', \n              'cancer', 'Hashimotos', 'coloncancer', 'PreCervicalCancer', 'lymphoma', 'Lymphedema', \n              'CancerCaregivers', 'braincancer', 'lynchsyndrome', 'nursing', 'testicularcancer', 'leukemia', \n              'publichealth', 'Health', 'Fuckcancer', 'HealthInsurance', 'BRCA', 'Cancersurvivors', \n              'pancreaticcancer', 'skincancer', 'stomachcancer']\nThese subreddits were compared to a random sample from the full Reddit dataset excluding the list of cancer subreddits above.\nQueries were conducted in Azure ML using Spark, with the data sourced from the instructor’s Azure Blob container. Comments from both cancer-related and non-cancer subreddits were processed using an Azure ML job and saved as Parquet files in an Azure Blob container. The job applied a filter to separate cancer subreddits into one Parquet file and non-cancer subreddits into another. For the non-cancer subreddits, the data was randomized before filtering out the cancer-related subreddits.\n\n\nCode\n# Path to the Azure ML Blob Container\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\ncomments_path = \"cancer/comments\"\nsubmissions_path = \"cancer/submissions\"\n\n\nPySpark was used to clean the data by removing leading and trailing whitespaces, removing punctuation (using regex), removing underscores, and converting to lowercase. Both subsets of data were limited to 10,000 rows in order to allow a reasonable compute time for each job. After the data was cleaned it was saved into a two parquet files in an Azure ML blob container to use for the rest of the project. The combined cancer subreddits and non-cancer subreddit totaled in 20,000 rows.\n\n\nCode\n# Cancer subset of Reddit Data saved to an Azure ML Blob Container\noutput_path = f\"{workspace_wasbs_base_url}cancer_subreddit.parquet\"\n\n# Non-cancer subset of Reddit Data saved to an Azure ML Blob Container\noutput_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n\n\nThe source code for the cleaning the Reddit data is in GitHub: fall-2024-project-team-35/code/spark-job-sample-data"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "NLP",
    "section": "",
    "text": "Two different types of sentiment analysis were performed on both the cancer and non-cancer subreddits. In this method, a pretrained SparkNLP pipeline in Azure ML was used to classify the comments as positive, negative, or neutral.\n\n\n\nsentiment_label\ncount_cancer\ncount_non_cancer\n\n\n\n\npositive\n5929\n5715\n\n\nnegative\n4071\n4285\n\n\n\n\nData Processing Comments from cancer-related and non-cancer subreddits were queried using a job in Azure ML and saved as Parquet files in an Azure Blob container. In the job, a filter was used to include cancer subreddits in a Parquet file and exclude cancer subreddits in another Parquet file. In the non-cancer subreddit, the data was randomized before excluding the cancer subreddits.\nSentiment Analysis using SparkNLP Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral.\nWeighted Scores A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment.\nSentiment Labeling Comments were labeled based on their weighted scores (positive, negative, or neutral).\nGrouping and Aggregation The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits.\nStatistical Testing A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories showed a difference in positive and negative sentiment in the cancer and non-cancer subreddits.\n\nThe source code for the sentiment analysis using SparkNLP is here: https://github.com/gu-dsan6000/fall-2024-project-team-35/tree/main/code/nlp/spark-job-with-sparknlp-sentiment\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\nResults of Chi-square Test\nChi-square statistic: 9.325853191514204\nP-value: 0.0022594309243996907\nDegrees of freedom: 1\nExpected frequencies: [[5822. 5822.]\n[4178. 4178.]]\nTo assess the difference between positive and negative sentiment in the cancer and non-cancer subreddits the odds ratio (OR) is calculated as:\n\\[Odds Ratio (OR) = \\frac{(\\text{Positive Cancer} / \\text{Positive Non-Cancer})}{(\\text{Negative Cancer} / \\text{Negative Non-Cancer})}\\]\n\\[OR = \\frac{(5929/5715)}{(4071/4285)}\\]\n\\[OR = \\frac{(5929 \\times 4285)}{(5715 \\times 4071)}\\]\n\\[OR = 1.065\\]\n\\[\\beta = ln(OR)\\]\n\\[\\beta = ln(1.065) = 0.063\\]\nThe coefficient indicates the log odds of cancer for the “positive” sentiment label relative to the “negative” sentiment label. A positive value of 0.063 suggests that there is a slight increase in “positive” sentiment in the cancer subreddits compared to the non-cancer subreddits.\nThe p-value of 0.002 confirms a statistical significance. However, with the odds ratio, the positive sentiment is only slightly increased.\nPositive sentiment was higher in the cancer subreddit indicating the these groups may provide support and resiliency in the communities who use these subreddits. In comparison to the rest of Reddit (the non-cancer subreddits), users may look for look and discuss for more information about experience with treatment, logistics, and navigating a cancer diagnosis, fostering a space for encouragement and information.\n\n\n\n\n\n\nTo analyze the emotions in the Reddit Pushshift dataset, the NRC Emotion Lexicon was used. The NRC Emotion Lexicon is a type of sentiment analysis that calculates the negative, positive, and eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust). The annotations to determine emotions were manually done through crowdsourcing according to Saif Mohammad and Peter Turney.\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubreddit\npositive\nnegative\nneutral\nanger\nfear\ndisgust\nsadness\njoy\nsurprise\ntrust\n\n\n\n\nCancer\n0.202806\n0.119079\n0.0\n0.037681\n0.067383\n0.033798\n0.058226\n0.044399\n0.028933\n0.119079\n\n\nNon-Cancer\n0.170739\n0.096125\n0.0\n0.034273\n0.041384\n0.023316\n0.034463\n0.044963\n0.024675\n0.082917\n\n\n\nIn the NRC Lexicon Sentiment Analysis, positive and negative sentiment were both higher in the cancer subreddits compared to the non-cancer subreddits. These findings may indicate that users in the cancer subreddits have complicated sentiment toward cancer discussions. The analysis also showed higher trust, fear, sadness, and disgust. Trust is a particularly noteworthy finding, as it aligns with HINTS’ broader goal of deepening our understanding of trust in cancer-related communication.\nThe source code for the sentiment analysis using NRC-Lex is here: https://github.com/gu-dsan6000/fall-2024-project-team-35/blob/main/code/nlp/spark-job-with-sparknlp-sentiment/nrc_lex_averages_plot.ipynb\n\n\n\n\n\n\nGiven the results of the sentiment analysis, the frequency of popular words in the cancer subreddit were assess using term frequency-inverse document frequency (TF-IDF). This method was used to identify unique or prominent keywords.\n\nData preparation\n\n\nTwo separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits.\nThe datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer).\nText content was preprocessed into a column for TF-IDF analysis.\n\n\nTF-IDF VECTORIZATION\n\n\nThe TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets.\nCommon stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents.\n\n\nAGGREGATION & RANKING\n\n\nWords were ranked in descending order of their total TF-IDF scores for the cancer dataset.\nRankings allowed for the identification of keywords in the cancer dataset.\n\n\n\n\nWord\nTF-IDF\n\n\n\n\ntime\n604.333288\n\n\nremoved\n510.072719\n\n\nthank\n508.752837\n\n\ninformation\n491.421062\n\n\ndoctor\n480.772518\n\n\npeople\n467.593201\n\n\nwork\n443.772165\n\n\npatient\n387.074392\n\n\ncancer\n365.678933\n\n\nbest\n340.635342\n\n\n\n\n\n\nTF-IDF\n\n\nIn this word cloud we see words such as time, removed, thank, and information. Some of these words may be used in conversations to talk about cancer care and possibly gratitude from support groups in the Reddit forum.\nThe source code for the TF-IDF is in the same notebook as nrc_lex.ipynb: https://github.com/gu-dsan6000/fall-2024-project-team-35/blob/main/code/nlp/spark-job-with-sparknlp-sentiment/nrc_lex.ipynb\n\n\n\n\n\nCrowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\nEmotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon, Saif Mohammad and Peter Turney, In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California."
  },
  {
    "objectID": "nlp.html#sparknlp",
    "href": "nlp.html#sparknlp",
    "title": "NLP",
    "section": "",
    "text": "Two different types of sentiment analysis were performed on both the cancer and non-cancer subreddits. In this method, a pretrained SparkNLP pipeline in Azure ML was used to classify the comments as positive, negative, or neutral.\n\n\n\nsentiment_label\ncount_cancer\ncount_non_cancer\n\n\n\n\npositive\n5929\n5715\n\n\nnegative\n4071\n4285\n\n\n\n\nData Processing Comments from cancer-related and non-cancer subreddits were queried using a job in Azure ML and saved as Parquet files in an Azure Blob container. In the job, a filter was used to include cancer subreddits in a Parquet file and exclude cancer subreddits in another Parquet file. In the non-cancer subreddit, the data was randomized before excluding the cancer subreddits.\nSentiment Analysis using SparkNLP Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral.\nWeighted Scores A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment.\nSentiment Labeling Comments were labeled based on their weighted scores (positive, negative, or neutral).\nGrouping and Aggregation The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits.\nStatistical Testing A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories showed a difference in positive and negative sentiment in the cancer and non-cancer subreddits.\n\nThe source code for the sentiment analysis using SparkNLP is here: https://github.com/gu-dsan6000/fall-2024-project-team-35/tree/main/code/nlp/spark-job-with-sparknlp-sentiment\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\nResults of Chi-square Test\nChi-square statistic: 9.325853191514204\nP-value: 0.0022594309243996907\nDegrees of freedom: 1\nExpected frequencies: [[5822. 5822.]\n[4178. 4178.]]\nTo assess the difference between positive and negative sentiment in the cancer and non-cancer subreddits the odds ratio (OR) is calculated as:\n\\[Odds Ratio (OR) = \\frac{(\\text{Positive Cancer} / \\text{Positive Non-Cancer})}{(\\text{Negative Cancer} / \\text{Negative Non-Cancer})}\\]\n\\[OR = \\frac{(5929/5715)}{(4071/4285)}\\]\n\\[OR = \\frac{(5929 \\times 4285)}{(5715 \\times 4071)}\\]\n\\[OR = 1.065\\]\n\\[\\beta = ln(OR)\\]\n\\[\\beta = ln(1.065) = 0.063\\]\nThe coefficient indicates the log odds of cancer for the “positive” sentiment label relative to the “negative” sentiment label. A positive value of 0.063 suggests that there is a slight increase in “positive” sentiment in the cancer subreddits compared to the non-cancer subreddits.\nThe p-value of 0.002 confirms a statistical significance. However, with the odds ratio, the positive sentiment is only slightly increased.\nPositive sentiment was higher in the cancer subreddit indicating the these groups may provide support and resiliency in the communities who use these subreddits. In comparison to the rest of Reddit (the non-cancer subreddits), users may look for look and discuss for more information about experience with treatment, logistics, and navigating a cancer diagnosis, fostering a space for encouragement and information."
  },
  {
    "objectID": "nlp.html#nrc-lex",
    "href": "nlp.html#nrc-lex",
    "title": "NLP",
    "section": "",
    "text": "To analyze the emotions in the Reddit Pushshift dataset, the NRC Emotion Lexicon was used. The NRC Emotion Lexicon is a type of sentiment analysis that calculates the negative, positive, and eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust). The annotations to determine emotions were manually done through crowdsourcing according to Saif Mohammad and Peter Turney.\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubreddit\npositive\nnegative\nneutral\nanger\nfear\ndisgust\nsadness\njoy\nsurprise\ntrust\n\n\n\n\nCancer\n0.202806\n0.119079\n0.0\n0.037681\n0.067383\n0.033798\n0.058226\n0.044399\n0.028933\n0.119079\n\n\nNon-Cancer\n0.170739\n0.096125\n0.0\n0.034273\n0.041384\n0.023316\n0.034463\n0.044963\n0.024675\n0.082917\n\n\n\nIn the NRC Lexicon Sentiment Analysis, positive and negative sentiment were both higher in the cancer subreddits compared to the non-cancer subreddits. These findings may indicate that users in the cancer subreddits have complicated sentiment toward cancer discussions. The analysis also showed higher trust, fear, sadness, and disgust. Trust is a particularly noteworthy finding, as it aligns with HINTS’ broader goal of deepening our understanding of trust in cancer-related communication.\nThe source code for the sentiment analysis using NRC-Lex is here: https://github.com/gu-dsan6000/fall-2024-project-team-35/blob/main/code/nlp/spark-job-with-sparknlp-sentiment/nrc_lex_averages_plot.ipynb"
  },
  {
    "objectID": "nlp.html#tf-idf",
    "href": "nlp.html#tf-idf",
    "title": "NLP",
    "section": "",
    "text": "Given the results of the sentiment analysis, the frequency of popular words in the cancer subreddit were assess using term frequency-inverse document frequency (TF-IDF). This method was used to identify unique or prominent keywords.\n\nData preparation\n\n\nTwo separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits.\nThe datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer).\nText content was preprocessed into a column for TF-IDF analysis.\n\n\nTF-IDF VECTORIZATION\n\n\nThe TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets.\nCommon stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents.\n\n\nAGGREGATION & RANKING\n\n\nWords were ranked in descending order of their total TF-IDF scores for the cancer dataset.\nRankings allowed for the identification of keywords in the cancer dataset.\n\n\n\n\nWord\nTF-IDF\n\n\n\n\ntime\n604.333288\n\n\nremoved\n510.072719\n\n\nthank\n508.752837\n\n\ninformation\n491.421062\n\n\ndoctor\n480.772518\n\n\npeople\n467.593201\n\n\nwork\n443.772165\n\n\npatient\n387.074392\n\n\ncancer\n365.678933\n\n\nbest\n340.635342\n\n\n\n\n\n\nTF-IDF\n\n\nIn this word cloud we see words such as time, removed, thank, and information. Some of these words may be used in conversations to talk about cancer care and possibly gratitude from support groups in the Reddit forum.\nThe source code for the TF-IDF is in the same notebook as nrc_lex.ipynb: https://github.com/gu-dsan6000/fall-2024-project-team-35/blob/main/code/nlp/spark-job-with-sparknlp-sentiment/nrc_lex.ipynb"
  },
  {
    "objectID": "nlp.html#sources",
    "href": "nlp.html#sources",
    "title": "NLP",
    "section": "",
    "text": "Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\nEmotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon, Saif Mohammad and Peter Turney, In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California."
  }
]